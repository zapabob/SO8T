# SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆ
# GGUFå¤‰æ›ã¨llama.cppæ¨è«–æ¤œè¨¼ã‚’å®Ÿæ–½

param(
    [string]$ModelPath = "./outputs",
    [string]$OutputDir = "./gguf_test_results",
    [string]$ModelName = "so8t-qwen2vl-2b"
)

Write-Host "ğŸ”„ SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆé–‹å§‹..." -ForegroundColor Green

# ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
Write-Host "ğŸ”§ ä»®æƒ³ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆä¸­..." -ForegroundColor Yellow
.\.venv\Scripts\Activate.ps1

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
Write-Host "ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆä¸­..." -ForegroundColor Yellow
New-Item -ItemType Directory -Path $OutputDir -Force | Out-Null

# GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
Write-Host "ğŸ¯ GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­..." -ForegroundColor Yellow

$ggufTestScript = @"
import sys
import os
import json
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
import subprocess
import shutil

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append('src')

from training.trainer_with_pet import SO8TIntegratedTrainer
from modules.qwen2vl_wrapper import create_so8t_qwen2vl_model

def check_llama_cpp_availability():
    """llama.cppã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ” llama.cppã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    results = []
    
    # 1. llama-cpp-pythonã®ç¢ºèª
    try:
        import llama_cpp
        print(f"  âœ… llama-cpp-python: {llama_cpp.__version__}")
        results.append({
            "component": "llama_cpp_python",
            "available": True,
            "version": llama_cpp.__version__
        })
    except ImportError:
        print("  âŒ llama-cpp-python: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        results.append({
            "component": "llama_cpp_python",
            "available": False,
            "error": "Not installed"
        })
    
    # 2. llama.cppãƒã‚¤ãƒŠãƒªã®ç¢ºèª
    llama_cpp_paths = [
        "llama.cpp/convert.py",
        "llama.cpp/main",
        "llama-cpp-python",
        "llama-cpp-python.exe"
    ]
    
    for path in llama_cpp_paths:
        if shutil.which(path):
            print(f"  âœ… llama.cppãƒã‚¤ãƒŠãƒª: {path}")
            results.append({
                "component": "llama_cpp_binary",
                "available": True,
                "path": path
            })
            break
    else:
        print("  âš ï¸ llama.cppãƒã‚¤ãƒŠãƒª: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        results.append({
            "component": "llama_cpp_binary",
            "available": False,
            "error": "Binary not found"
        })
    
    return results

def test_model_conversion(model_path, output_dir, model_name):
    """ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    
    results = []
    
    try:
        # 1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        print("  ğŸ“¦ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        trainer = SO8TIntegratedTrainer(
            model_path=model_path,
            config_path=os.path.join(model_path, "config.json"),
            output_dir=output_dir
        )
        trainer.setup_components()
        
        print(f"  âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        print(f"     å›è»¢ã‚²ãƒ¼ãƒˆ: {'æœ‰åŠ¹' if trainer.rotation_gate is not None else 'ç„¡åŠ¹'}")
        print(f"     PETæå¤±: {'æœ‰åŠ¹' if trainer.pet_loss is not None else 'ç„¡åŠ¹'}")
        
        results.append({
            "test": "model_loading",
            "success": True,
            "rotation_gate": trainer.rotation_gate is not None,
            "pet_loss": trainer.pet_loss is not None
        })
        
    except Exception as e:
        print(f"  âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        results.append({
            "test": "model_loading",
            "success": False,
            "error": str(e)
        })
        return results
    
    try:
        # 2. Hugging Faceå½¢å¼ã§ä¿å­˜
        print("  ğŸ’¾ Hugging Faceå½¢å¼ã§ä¿å­˜ä¸­...")
        hf_path = os.path.join(output_dir, f"{model_name}_hf")
        trainer.model.save_pretrained(hf_path)
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚³ãƒ”ãƒ¼
        config_files = ["config.json", "tokenizer.json", "tokenizer_config.json"]
        for config_file in config_files:
            src_path = os.path.join(model_path, config_file)
            if os.path.exists(src_path):
                shutil.copy2(src_path, hf_path)
        
        print(f"  âœ… Hugging Faceå½¢å¼ã§ä¿å­˜å®Œäº†: {hf_path}")
        
        results.append({
            "test": "hf_saving",
            "success": True,
            "hf_path": hf_path
        })
        
    except Exception as e:
        print(f"  âŒ Hugging Faceä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        results.append({
            "test": "hf_saving",
            "success": False,
            "error": str(e)
        })
        return results
    
    try:
        # 3. GGUFå¤‰æ›è¨­å®šã‚’ä½œæˆ
        print("  âš™ï¸ GGUFå¤‰æ›è¨­å®šã‚’ä½œæˆä¸­...")
        gguf_config = {
            "model_name": model_name,
            "model_path": hf_path,
            "output_path": os.path.join(output_dir, f"{model_name}.gguf"),
            "quantization": "Q8_0",
            "description": "SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM (ç„¼ãè¾¼ã¿æ¸ˆã¿)",
            "conversion_script": "llama.cpp/convert.py",
            "conversion_command": f"python llama.cpp/convert.py {hf_path} --outfile {os.path.join(output_dir, f'{model_name}.gguf')} --outtype q8_0"
        }
        
        config_path = os.path.join(output_dir, "gguf_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(gguf_config, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ… å¤‰æ›è¨­å®šã‚’ä½œæˆ: {config_path}")
        
        results.append({
            "test": "config_creation",
            "success": True,
            "config_path": config_path,
            "gguf_config": gguf_config
        })
        
    except Exception as e:
        print(f"  âŒ è¨­å®šä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        results.append({
            "test": "config_creation",
            "success": False,
            "error": str(e)
        })
    
    try:
        # 4. å®Ÿéš›ã®GGUFå¤‰æ›ã‚’è©¦è¡Œ
        print("  ğŸ”„ GGUFå¤‰æ›ã‚’è©¦è¡Œä¸­...")
        
        # llama.cppã®convert.pyã‚’æ¢ã™
        convert_script = None
        for path in ["llama.cpp/convert.py", "convert.py", "llama-cpp-python"]:
            if os.path.exists(path) or shutil.which(path):
                convert_script = path
                break
        
        if convert_script:
            print(f"  ğŸ“ å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨: {convert_script}")
            
            # å¤‰æ›ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
            gguf_path = os.path.join(output_dir, f"{model_name}.gguf")
            cmd = [
                "python", convert_script,
                hf_path,
                "--outfile", gguf_path,
                "--outtype", "q8_0"
            ]
            
            print(f"  ğŸš€ å¤‰æ›ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}")
            
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    print(f"  âœ… GGUFå¤‰æ›æˆåŠŸ: {gguf_path}")
                    results.append({
                        "test": "gguf_conversion",
                        "success": True,
                        "gguf_path": gguf_path,
                        "command": cmd,
                        "stdout": result.stdout,
                        "stderr": result.stderr
                    })
                else:
                    print(f"  âŒ GGUFå¤‰æ›å¤±æ•—: {result.stderr}")
                    results.append({
                        "test": "gguf_conversion",
                        "success": False,
                        "error": result.stderr,
                        "command": cmd
                    })
            
            except subprocess.TimeoutExpired:
                print("  â° GGUFå¤‰æ›ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                results.append({
                    "test": "gguf_conversion",
                    "success": False,
                    "error": "Timeout"
                })
            
            except Exception as e:
                print(f"  âŒ å¤‰æ›å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
                results.append({
                    "test": "gguf_conversion",
                    "success": False,
                    "error": str(e)
                })
        
        else:
            print("  âš ï¸ å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ‰‹å‹•å¤‰æ›ãŒå¿…è¦ã§ã™ã€‚")
            results.append({
                "test": "gguf_conversion",
                "success": False,
                "error": "Conversion script not found",
                "manual_conversion_required": True
            })
    
    except Exception as e:
        print(f"  âŒ GGUFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        results.append({
            "test": "gguf_conversion",
            "success": False,
            "error": str(e)
        })
    
    return results

def test_ollama_integration(output_dir, model_name):
    """Ollamaçµ±åˆã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ¦™ Ollamaçµ±åˆã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    
    results = []
    
    try:
        # 1. Modelfileã‚’ä½œæˆ
        print("  ğŸ“ Modelfileã‚’ä½œæˆä¸­...")
        modelfile_content = f"""FROM {os.path.join(output_dir, f"{model_name}.gguf")}

TEMPLATE \"\"\"{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}\"\"\"

# SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM Model Card
# SO(8)ç¾¤å›è»¢ã‚²ãƒ¼ãƒˆ + PETæ­£å‰‡åŒ– + OCRè¦ç´„ + SQLiteç›£æŸ»

SYSTEM \"\"\"You are SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM, an advanced multimodal language model with SO(8) group structure and enhanced safety features.

Key Features:
- SO(8) Group Structure: 8-dimensional rotation gates for enhanced reasoning
- PET Regularization: Second-order difference penalty for smooth outputs
- OCR Summary: Local image processing with privacy protection
- SQLite Audit: Complete decision logging and policy tracking

Capabilities:
- Multimodal understanding (text + images)
- Safe and responsible AI responses
- Local OCR processing (no external data sharing)
- Comprehensive audit logging

Safety Guidelines:
- Always prioritize user safety and privacy
- Process images locally without external sharing
- Log all decisions for transparency
- Escalate complex ethical decisions when needed

You provide helpful, accurate, and safe responses while maintaining complete privacy and auditability.\"\"\"

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 32768
PARAMETER num_predict 2048
"""
        
        modelfile_path = os.path.join(output_dir, f"{model_name}.Modelfile")
        with open(modelfile_path, 'w', encoding='utf-8') as f:
            f.write(modelfile_content)
        
        print(f"  âœ… Modelfileã‚’ä½œæˆ: {modelfile_path}")
        
        results.append({
            "test": "modelfile_creation",
            "success": True,
            "modelfile_path": modelfile_path
        })
        
    except Exception as e:
        print(f"  âŒ Modelfileä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        results.append({
            "test": "modelfile_creation",
            "success": False,
            "error": str(e)
        })
    
    try:
        # 2. Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’è©¦è¡Œ
        print("  ğŸ¦™ Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’è©¦è¡Œä¸­...")
        
        # ollamaã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèª
        if shutil.which("ollama"):
            print("  âœ… ollamaã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚³ãƒãƒ³ãƒ‰
            create_cmd = ["ollama", "create", model_name, "-f", modelfile_path]
            print(f"  ğŸš€ ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚³ãƒãƒ³ãƒ‰: {' '.join(create_cmd)}")
            
            try:
                result = subprocess.run(create_cmd, capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    print(f"  âœ… Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {model_name}")
                    results.append({
                        "test": "ollama_model_creation",
                        "success": True,
                        "model_name": model_name,
                        "command": create_cmd,
                        "stdout": result.stdout
                    })
                    
                    # 3. ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
                    print("  ğŸ§ª ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œãƒ†ã‚¹ãƒˆä¸­...")
                    run_cmd = ["ollama", "run", model_name, "ç”»åƒã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"]
                    
                    try:
                        run_result = subprocess.run(run_cmd, capture_output=True, text=True, timeout=30)
                        
                        if run_result.returncode == 0:
                            print(f"  âœ… ãƒ¢ãƒ‡ãƒ«å®Ÿè¡ŒæˆåŠŸ")
                            print(f"     å¿œç­”: {run_result.stdout[:100]}...")
                            results.append({
                                "test": "ollama_model_run",
                                "success": True,
                                "response": run_result.stdout,
                                "command": run_cmd
                            })
                        else:
                            print(f"  âŒ ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œå¤±æ•—: {run_result.stderr}")
                            results.append({
                                "test": "ollama_model_run",
                                "success": False,
                                "error": run_result.stderr
                            })
                    
                    except subprocess.TimeoutExpired:
                        print("  â° ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                        results.append({
                            "test": "ollama_model_run",
                            "success": False,
                            "error": "Timeout"
                        })
                    
                    except Exception as e:
                        print(f"  âŒ ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
                        results.append({
                            "test": "ollama_model_run",
                            "success": False,
                            "error": str(e)
                        })
                
                else:
                    print(f"  âŒ Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—: {result.stderr}")
                    results.append({
                        "test": "ollama_model_creation",
                        "success": False,
                        "error": result.stderr,
                        "command": create_cmd
                    })
            
            except subprocess.TimeoutExpired:
                print("  â° ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                results.append({
                    "test": "ollama_model_creation",
                    "success": False,
                    "error": "Timeout"
                })
            
            except Exception as e:
                print(f"  âŒ ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                results.append({
                    "test": "ollama_model_creation",
                    "success": False,
                    "error": str(e)
                })
        
        else:
            print("  âš ï¸ ollamaã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            results.append({
                "test": "ollama_model_creation",
                "success": False,
                "error": "ollama command not found"
            })
    
    except Exception as e:
        print(f"  âŒ Ollamaçµ±åˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        results.append({
            "test": "ollama_integration",
            "success": False,
            "error": str(e)
        })
    
    return results

def analyze_results(all_results):
    """çµæœã‚’åˆ†æ"""
    print("\\nğŸ“Š GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆçµæœåˆ†æ")
    print("=" * 50)
    
    # å„ãƒ†ã‚¹ãƒˆã®æˆåŠŸç‡ã‚’è¨ˆç®—
    test_success = {}
    for result in all_results:
        test_name = result.get('test', 'unknown')
        if test_name not in test_success:
            test_success[test_name] = {'success': 0, 'total': 0}
        
        test_success[test_name]['total'] += 1
        if result.get('success', False):
            test_success[test_name]['success'] += 1
    
    print("ğŸ“ˆ ãƒ†ã‚¹ãƒˆçµæœ:")
    for test_name, stats in test_success.items():
        success_rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0.0
        print(f"  {test_name}: {stats['success']}/{stats['total']} ({success_rate:.3f})")
    
    # ç·åˆæˆåŠŸç‡
    total_success = sum(stats['success'] for stats in test_success.values())
    total_tests = sum(stats['total'] for stats in test_success.values())
    overall_success_rate = total_success / total_tests if total_tests > 0 else 0.0
    
    print(f"\\nğŸ“Š ç·åˆæˆåŠŸç‡: {overall_success_rate:.3f}")
    
    # æ¨å¥¨äº‹é …
    print("\\nğŸ’¡ æ¨å¥¨äº‹é …:")
    if not test_success.get('gguf_conversion', {}).get('success', 0):
        print("  - llama.cppã®convert.pyã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦GGUFå¤‰æ›ã‚’å®Œäº†ã—ã¦ãã ã•ã„")
    if not test_success.get('ollama_model_creation', {}).get('success', 0):
        print("  - Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œã‚’ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„")
    
    return {
        "test_success": test_success,
        "overall_success_rate": overall_success_rate
    }

def main():
    print("ğŸ”„ SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    print("\\nğŸ¯ llama.cppåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
    availability_results = check_llama_cpp_availability()
    
    print("\\nğŸ¯ ãƒ¢ãƒ‡ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    conversion_results = test_model_conversion('$ModelPath', '$OutputDir', '$ModelName')
    
    print("\\nğŸ¯ Ollamaçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹...")
    ollama_results = test_ollama_integration('$OutputDir', '$ModelName')
    
    # å…¨çµæœã‚’çµ±åˆ
    all_results = availability_results + conversion_results + ollama_results
    
    # çµæœã‚’åˆ†æ
    analysis = analyze_results(all_results)
    
    # çµæœã‚’ä¿å­˜
    results = {
        "timestamp": datetime.now().isoformat(),
        "model_path": "$ModelPath",
        "output_dir": "$OutputDir",
        "model_name": "$ModelName",
        "availability_results": availability_results,
        "conversion_results": conversion_results,
        "ollama_results": ollama_results,
        "analysis": analysis
    }
    
    results_file = "$OutputDir/gguf_test_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\\nğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
    print(f"ğŸ“Š ç·åˆæˆåŠŸç‡: {analysis['overall_success_rate']:.3f}")
    
    print("\\nâœ… GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")

if __name__ == "__main__":
    main()
"@

# GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
$ggufTestScript | py -3

if ($LASTEXITCODE -eq 0) {
    Write-Host "âœ… GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆå®Œäº†ï¼" -ForegroundColor Green
    Write-Host "ğŸ“ çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: $OutputDir" -ForegroundColor Cyan
    Write-Host "ğŸ“Š çµæœãƒ•ã‚¡ã‚¤ãƒ«: $OutputDir/gguf_test_results.json" -ForegroundColor Cyan
    Write-Host "ğŸ”„ GGUFãƒ•ã‚¡ã‚¤ãƒ«: $OutputDir/$ModelName.gguf" -ForegroundColor Cyan
    Write-Host "ğŸ¦™ Modelfile: $OutputDir/$ModelName.Modelfile" -ForegroundColor Cyan
} else {
    Write-Error "âŒ GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    exit 1
}
