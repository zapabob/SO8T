#!/usr/bin/env python3
"""
SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM llama.cppå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
llama.cpp-masterã‚’ä½¿ç”¨ã—ã¦Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’GGUFã«å¤‰æ›
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
from datetime import datetime

def setup_llamacpp_environment(llamacpp_path):
    """llama.cppç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    print("ğŸ”§ llama.cppç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    
    # llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(llamacpp_path):
        raise FileNotFoundError(f"llama.cppãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {llamacpp_path}")
    
    # convert_hf_to_gguf.pyã®å­˜åœ¨ç¢ºèª
    convert_script = os.path.join(llamacpp_path, "convert_hf_to_gguf.py")
    if not os.path.exists(convert_script):
        raise FileNotFoundError(f"convert_hf_to_gguf.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {convert_script}")
    
    print(f"âœ… llama.cppç’°å¢ƒç¢ºèªå®Œäº†: {llamacpp_path}")
    return convert_script

def convert_model_to_gguf(
    model_path,
    output_dir,
    model_name,
    quantization="q8_0",
    llamacpp_path="C:\\Users\\downl\\Desktop\\SO8T\\llama.cpp-master"
):
    """ãƒ¢ãƒ‡ãƒ«ã‚’GGUFã«å¤‰æ›"""
    print(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«å¤‰æ›é–‹å§‹: {model_path} -> {output_dir}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # llama.cppç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    convert_script = setup_llamacpp_environment(llamacpp_path)
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    output_file = os.path.join(output_dir, f"{model_name}.gguf")
    
    # å¤‰æ›ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
    cmd = [
        "py", convert_script,
        model_path,
        "--outfile", output_file,
        "--outtype", quantization,
        "--verbose"
    ]
    
    print(f"ğŸš€ å¤‰æ›ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(cmd)}")
    
    try:
        # å¤‰æ›ã‚’å®Ÿè¡Œ
        result = subprocess.run(
            cmd,
            cwd=llamacpp_path,
            capture_output=True,
            text=True,
            timeout=1800  # 30åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode == 0:
            print("âœ… ãƒ¢ãƒ‡ãƒ«å¤‰æ›æˆåŠŸï¼")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
            if os.path.exists(output_file):
                file_size = os.path.getsize(output_file) / (1024**3)  # GB
                print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f} GB")
            
            return output_file, result.stdout, result.stderr
        else:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«å¤‰æ›å¤±æ•— (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})")
            print(f"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›: {result.stderr}")
            return None, result.stdout, result.stderr
            
    except subprocess.TimeoutExpired:
        print("â° å¤‰æ›ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (30åˆ†)")
        return None, "", "Timeout"
    except Exception as e:
        print(f"âŒ å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return None, "", str(e)

def create_modelfile(output_file, model_name, output_dir):
    """Modelfileã‚’ä½œæˆ"""
    print("ğŸ“ Modelfileã‚’ä½œæˆä¸­...")
    
    modelfile_content = f'''FROM {output_file}

TEMPLATE """{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}"""

# SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM Model Card
# SO(8)ç¾¤å›è»¢ã‚²ãƒ¼ãƒˆ + PETæ­£å‰‡åŒ– + OCRè¦ç´„ + SQLiteç›£æŸ»

SYSTEM """You are SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM, an advanced multimodal language model with SO(8) group structure and enhanced safety features.

Key Features:
- SO(8) Group Structure: 8-dimensional rotation gates for enhanced reasoning
- PET Regularization: Second-order difference penalty for smooth outputs
- OCR Summary: Local image processing with privacy protection
- SQLite Audit: Complete decision logging and policy tracking

Capabilities:
- Multimodal understanding (text + images)
- Safe and responsible AI responses
- Local OCR processing (no external data sharing)
- Comprehensive audit logging

Safety Guidelines:
- Always prioritize user safety and privacy
- Process images locally without external sharing
- Log all decisions for transparency
- Escalate complex ethical decisions when needed

You provide helpful, accurate, and safe responses while maintaining complete privacy and auditability."""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 32768
PARAMETER num_predict 2048
'''
    
    modelfile_path = os.path.join(output_dir, f"{model_name}.Modelfile")
    
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"âœ… Modelfileä½œæˆå®Œäº†: {modelfile_path}")
    return modelfile_path

def create_ollama_commands(model_name, modelfile_path, output_dir):
    """Ollamaã‚³ãƒãƒ³ãƒ‰ã‚’ä½œæˆ"""
    print("ğŸ¦™ Ollamaã‚³ãƒãƒ³ãƒ‰ã‚’ä½œæˆä¸­...")
    
    commands = {
        "create_model": f"ollama create {model_name} -f \"{modelfile_path}\"",
        "run_model": f"ollama run {model_name}",
        "list_models": "ollama list",
        "remove_model": f"ollama rm {model_name}"
    }
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    commands_file = os.path.join(output_dir, f"{model_name}_ollama_commands.txt")
    
    with open(commands_file, 'w', encoding='utf-8') as f:
        f.write("# SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM Ollamaã‚³ãƒãƒ³ãƒ‰\n")
        f.write(f"# ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for name, command in commands.items():
            f.write(f"# {name}\n")
            f.write(f"{command}\n\n")
    
    print(f"âœ… Ollamaã‚³ãƒãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {commands_file}")
    return commands_file

def main():
    parser = argparse.ArgumentParser(description="SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM llama.cppå¤‰æ›")
    parser.add_argument("--model_path", default="./outputs", help="å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--output_dir", default="./gguf_models", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--model_name", default="so8t-qwen2vl-2b", help="ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--quantization", default="q8_0", 
                       choices=["f32", "f16", "bf16", "q8_0", "tq1_0", "tq2_0", "auto"],
                       help="é‡å­åŒ–ã‚¿ã‚¤ãƒ—")
    parser.add_argument("--llamacpp_path", 
                       default="C:\\Users\\downl\\Desktop\\SO8T\\llama.cpp-master",
                       help="llama.cppãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    print("ğŸ”„ SO8TÃ—ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM llama.cppå¤‰æ›é–‹å§‹...")
    print(f"ğŸ“ å…¥åŠ›ãƒ¢ãƒ‡ãƒ«: {args.model_path}")
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
    print(f"ğŸ·ï¸ ãƒ¢ãƒ‡ãƒ«å: {args.model_name}")
    print(f"âš™ï¸ é‡å­åŒ–: {args.quantization}")
    
    # ãƒ¢ãƒ‡ãƒ«å¤‰æ›
    output_file, stdout, stderr = convert_model_to_gguf(
        args.model_path,
        args.output_dir,
        args.model_name,
        args.quantization,
        args.llamacpp_path
    )
    
    if output_file:
        # Modelfileã‚’ä½œæˆ
        modelfile_path = create_modelfile(output_file, args.model_name, args.output_dir)
        
        # Ollamaã‚³ãƒãƒ³ãƒ‰ã‚’ä½œæˆ
        commands_file = create_ollama_commands(args.model_name, modelfile_path, args.output_dir)
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\nğŸ“Š å¤‰æ›çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        print(f"ãƒ¢ãƒ‡ãƒ«å: {args.model_name}")
        print(f"é‡å­åŒ–: {args.quantization}")
        print(f"GGUFãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        print(f"Modelfile: {modelfile_path}")
        print(f"ã‚³ãƒãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«: {commands_file}")
        
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file) / (1024**3)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f} GB")
        
        print("\nğŸ¦™ Ollamaãƒ¢ãƒ‡ãƒ«ä½œæˆæ‰‹é †:")
        print(f"1. ollama create {args.model_name} -f \"{modelfile_path}\"")
        print(f"2. ollama run {args.model_name}")
        
        print("\nâœ… llama.cppå¤‰æ›å®Œäº†ï¼")
        
    else:
        print("\nâŒ å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")
        if stderr:
            print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {stderr}")
        sys.exit(1)

if __name__ == "__main__":
    main()
