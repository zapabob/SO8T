#!/usr/bin/env python3
"""
SO8Tç¾¤Transformer GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

SO8T GGUFå¤‰æ›æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦å¤‰æ›ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import torch
import numpy as np
import tempfile
import shutil
from pathlib import Path
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from scripts.convert_so8t_to_gguf_colab import SO8TGGUFConverter


def create_dummy_so8t_model(output_dir: str) -> str:
    """ãƒ€ãƒŸãƒ¼ã®SO8Tãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print("ğŸ”§ ãƒ€ãƒŸãƒ¼SO8Tãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ãƒ€ãƒŸãƒ¼ã®SO8Tãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹è¾æ›¸ã‚’ä½œæˆ
    state_dict = {}
    
    # SO8Tç¾¤é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    state_dict['so8t.rotation_params'] = torch.randn(8, 8) * 0.01
    state_dict['so8t.rotation_angles'] = torch.randn(8) * 0.1
    state_dict['so8t.group_structure.weight'] = torch.randn(64, 64) * 0.02
    
    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤
    state_dict['attention.q_proj.weight'] = torch.randn(4096, 4096) * 0.02
    state_dict['attention.k_proj.weight'] = torch.randn(4096, 4096) * 0.02
    state_dict['attention.v_proj.weight'] = torch.randn(4096, 4096) * 0.02
    state_dict['attention.o_proj.weight'] = torch.randn(4096, 4096) * 0.02
    
    # FFNå±¤
    state_dict['mlp.gate_proj.weight'] = torch.randn(11008, 4096) * 0.02
    state_dict['mlp.up_proj.weight'] = torch.randn(11008, 4096) * 0.02
    state_dict['mlp.down_proj.weight'] = torch.randn(4096, 11008) * 0.02
    
    # Triality reasoning heads
    state_dict['task_head.weight'] = torch.randn(151936, 4096) * 0.02
    state_dict['safety_head.weight'] = torch.randn(2, 4096) * 0.02
    state_dict['safety_head.bias'] = torch.zeros(2)
    state_dict['authority_head.weight'] = torch.randn(2, 4096) * 0.02
    state_dict['authority_head.bias'] = torch.zeros(2)
    
    # åŸ‹ã‚è¾¼ã¿å±¤
    state_dict['embed_tokens.weight'] = torch.randn(151936, 4096) * 0.02
    state_dict['norm.weight'] = torch.ones(4096)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    model_path = output_path / "pytorch_model.bin"
    torch.save(state_dict, model_path)
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆ
    config = {
        "model_type": "so8t_transformer",
        "hidden_size": 4096,
        "num_attention_heads": 32,
        "num_hidden_layers": 32,
        "vocab_size": 151936,
        "so8t_layers": 32,
        "triality_heads": 3
    }
    
    import json
    config_path = output_path / "config.json"
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… ãƒ€ãƒŸãƒ¼SO8Tãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {model_path}")
    print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in state_dict.values()):,}")
    print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {sum(p.numel() * p.element_size() for p in state_dict.values()) / (1024**2):.1f} MB")
    
    return str(output_path)


def test_gguf_conversion():
    """GGUFå¤‰æ›ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ SO8T GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼")
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        try:
            # 1. ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            print("\nğŸ“¥ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ€ãƒŸãƒ¼SO8Tãƒ¢ãƒ‡ãƒ«ä½œæˆ")
            model_path = create_dummy_so8t_model(str(temp_path / "dummy_model"))
            
            # 2. å¤‰æ›å™¨ä½œæˆ
            print("\nğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—2: GGUFå¤‰æ›å™¨ä½œæˆ")
            converter = SO8TGGUFConverter(
                model_path=model_path,
                output_dir=str(temp_path / "gguf_output"),
                quantization_type="Q8_0",
                max_memory_gb=2.0
            )
            
            # 3. å¤‰æ›å®Ÿè¡Œ
            print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—3: GGUFå¤‰æ›å®Ÿè¡Œ")
            output_path = converter.convert()
            
            # 4. çµæœç¢ºèª
            print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—4: çµæœç¢ºèª")
            output_dir = Path(output_path).parent
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            expected_files = [
                "dummy_model_so8t_Q8_0.json",  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                "dummy_model_so8t_Q8_0.npz",   # ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿
                "dummy_model_so8t_Q8_0.quant.json",  # é‡å­åŒ–æƒ…å ±
                "README.md"  # ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰
            ]
            
            for file_name in expected_files:
                file_path = output_dir / file_name
                if file_path.exists():
                    file_size = file_path.stat().st_size
                    print(f"  âœ… {file_name}: {file_size / 1024:.1f} KB")
                else:
                    print(f"  âŒ {file_name}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            metadata_path = output_dir / "dummy_model_so8t_Q8_0.json"
            if metadata_path.exists():
                import json
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                print(f"\nğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª:")
                print(f"  - ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {metadata.get('model_type', 'N/A')}")
                print(f"  - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {metadata.get('architecture', 'N/A')}")
                print(f"  - é‡å­åŒ–ã‚¿ã‚¤ãƒ—: {metadata.get('quantization_type', 'N/A')}")
                print(f"  - SO8Tç¾¤ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {metadata.get('so8t_layers', 'N/A')}")
                print(f"  - SO8å›è»¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {metadata.get('so8_rotation_params', 'N/A')}")
                print(f"  - Triality headsæ•°: {metadata.get('triality_heads', 'N/A')}")
            
            # ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            tensor_path = output_dir / "dummy_model_so8t_Q8_0.npz"
            if tensor_path.exists():
                tensor_data = np.load(tensor_path)
                print(f"\nğŸ“¦ ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª:")
                print(f"  - ãƒ†ãƒ³ã‚½ãƒ«æ•°: {len(tensor_data.files)}")
                print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {tensor_path.stat().st_size / (1024**2):.1f} MB")
                
                # é‡å­åŒ–ã®ç¢ºèª
                quantized_count = 0
                for key in tensor_data.files:
                    if tensor_data[key].dtype == np.int8:
                        quantized_count += 1
                
                print(f"  - é‡å­åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«æ•°: {quantized_count}")
                print(f"  - é‡å­åŒ–ç‡: {quantized_count / len(tensor_data.files) * 100:.1f}%")
            
            print(f"\nğŸ‰ SO8T GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
            print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


def test_different_quantization_types():
    """ç•°ãªã‚‹é‡å­åŒ–ã‚¿ã‚¤ãƒ—ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ç•°ãªã‚‹é‡å­åŒ–ã‚¿ã‚¤ãƒ—ã®ãƒ†ã‚¹ãƒˆ")
    
    quantization_types = ["Q8_0", "Q4_K_M", "none"]
    
    for quant_type in quantization_types:
        print(f"\nğŸ”§ é‡å­åŒ–ã‚¿ã‚¤ãƒ—: {quant_type}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            try:
                # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ä½œæˆ
                model_path = create_dummy_so8t_model(str(temp_path / "test_model"))
                
                # å¤‰æ›å™¨ä½œæˆ
                converter = SO8TGGUFConverter(
                    model_path=model_path,
                    output_dir=str(temp_path / "output"),
                    quantization_type=quant_type,
                    max_memory_gb=1.0
                )
                
                # å¤‰æ›å®Ÿè¡Œ
                output_path = converter.convert()
                
                # çµæœç¢ºèª
                output_dir = Path(output_path).parent
                tensor_file = list(output_dir.glob("*.npz"))[0]
                tensor_data = np.load(tensor_file)
                
                # é‡å­åŒ–çµ±è¨ˆ
                total_tensors = len(tensor_data.files)
                quantized_tensors = sum(1 for key in tensor_data.files if tensor_data[key].dtype == np.int8)
                quantized_ratio = quantized_tensors / total_tensors * 100
                
                print(f"  âœ… å¤‰æ›æˆåŠŸ")
                print(f"  ğŸ“Š ãƒ†ãƒ³ã‚½ãƒ«æ•°: {total_tensors}")
                print(f"  ğŸ”¢ é‡å­åŒ–ãƒ†ãƒ³ã‚½ãƒ«æ•°: {quantized_tensors}")
                print(f"  ğŸ“ˆ é‡å­åŒ–ç‡: {quantized_ratio:.1f}%")
                print(f"  ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {tensor_file.stat().st_size / (1024**2):.1f} MB")
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ SO8Tç¾¤Transformer GGUFå¤‰æ›ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼")
    print("=" * 60)
    
    # åŸºæœ¬å¤‰æ›ãƒ†ã‚¹ãƒˆ
    success = test_gguf_conversion()
    
    if success:
        print("\n" + "=" * 60)
        # ç•°ãªã‚‹é‡å­åŒ–ã‚¿ã‚¤ãƒ—ã®ãƒ†ã‚¹ãƒˆ
        test_different_quantization_types()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("âœ… SO8Tç¾¤Transformer GGUFå¤‰æ›æ©Ÿèƒ½ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    else:
        print("\n" + "=" * 60)
        print("âŒ ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("ğŸ’¡ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
