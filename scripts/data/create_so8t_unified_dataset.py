#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SO8-Think Unified Dataset Creation Script

ç†è«–çš„èƒŒæ™¯:
- URT (Unified Representation Theorem): é‡å­å ´ã®çµ±ä¸€è¡¨ç¾
- NC-KARTâ˜… (Non-Commutative Kolmogorov-Arnold Representation Theory): éå¯æ›è¡¨ç¾ç†è«–
- éå¯æ›KARTå®šç†: å¤å…¸KARTã®C*-ç’°æ‹¡å¼µ

é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã¨å››å€¤åˆ†é¡ã‚¿ã‚°ä»˜ä¸ã‚’è¡Œã„ã€
PPOã«ã‚ˆã‚‹SO(8) Thinkãƒ¢ãƒ‡ãƒ«å°‚ç”¨çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ

è‘—è€…: AI Agent (å³¯å²¸äº®ãƒœãƒ–ã«ã‚ƒã‚“ç†è«–å®Ÿè£…)
æ—¥ä»˜: 2025-11-30
"""

import os
import re
import json
import argparse
import warnings
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from datetime import datetime

import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# Hugging Face datasets
try:
    from datasets import load_dataset, Dataset, concatenate_datasets
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("Warning: datasets library not available. Install with: pip install datasets")

# ArXiv API
try:
    import arxiv
    ARXIV_AVAILABLE = True
except ImportError:
    ARXIV_AVAILABLE = False
    print("Warning: arxiv library not available. Install with: pip install arxiv")

# ç†è«–ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
THEORY_FILES = [
    "C:/Users/downl/Desktop/Gemini-çµ±åˆç‰¹è§£ã¨éå¯æ›è¡¨ç¾ç†è«–.md",
    "C:/Users/downl/Desktop/Gemini-NC-KARTâ˜…ã¨URTã®æ•°å­¦çš„æ¢æ±‚.md",
    "C:/Users/downl/Desktop/ChatGPT-éå¯æ›KARTå®šç† (4).md"
]

# ==========================================
# 1. è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾©
# ==========================================

@dataclass
class SO8TDatasetConfig:
    """SO8Tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š"""
    total_samples: int = 50000
    science_ratio: float = 0.4
    japanese_ratio: float = 0.3
    nsfw_ratio: float = 0.2
    arxiv_ratio: float = 0.1

    # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    min_length: int = 100
    max_length: int = 4096
    complexity_threshold: float = 0.7
    latex_density_threshold: float = 0.001

    # å››å€¤åˆ†é¡è¨­å®š
    allow_keywords = [
        "hello", "what is", "how to", "explain", "simple", "basic"
    ]
    escalation_keywords = [
        "prove", "theorem", "complex", "advanced", "deep", "theoretical",
        "quantum", "relativity", "field", "algebra", "topology"
    ]
    deny_keywords = [
        "wrong", "incorrect", "false", "invalid", "contradiction"
    ]
    refuse_keywords = [
        "nsfw", "sex", "porn", "violence", "illegal", "harmful"
    ]

# å››å€¤åˆ†é¡ã‚¿ã‚°
TAG_CLASSES = ['allow', 'escalation', 'deny', 'refuse']

# ==========================================
# 2. ç†è«–ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã‚¯ãƒ©ã‚¹
# ==========================================

class SO8TTheoryIntegrator:
    """SO8Tç†è«–ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self, theory_files: List[str]):
        self.theory_files = theory_files
        self.theory_content = {}

    def load_theory_files(self) -> Dict[str, str]:
        """ç†è«–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        for file_path in self.theory_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    filename = Path(file_path).name
                    self.theory_content[filename] = content
                    print(f"âœ“ Loaded theory file: {filename}")
            except Exception as e:
                print(f"âœ— Failed to load {file_path}: {e}")

        return self.theory_content

    def extract_theoretical_concepts(self) -> Dict[str, List[str]]:
        """ç†è«–çš„æ¦‚å¿µã‚’æŠ½å‡º"""
        concepts = {
            'urt': [],
            'nc_kart': [],
            'noncommutative_kart': [],
            'so8_geometry': []
        }

        # URTé–¢é€£æ¦‚å¿µ
        urt_patterns = [
            r'URT|Unified Representation Theorem',
            r'é‡å­å ´|quantum field',
            r'æŒ‡æ•°æ¸›è¡°|exponential decay',
            r'çµ±ä¸€è¡¨ç¾|unified representation'
        ]

        # NC-KARTé–¢é€£æ¦‚å¿µ
        nc_kart_patterns = [
            r'NC-KART|Non-Commutative.*KART',
            r'â˜…-ç©|star product',
            r'Moyal',
            r'éå¯æ›|non-commutative'
        ]

        # éå¯æ›KARTå®šç†é–¢é€£
        noncomm_patterns = [
            r'éå¯æ›.*KART|non-commutative.*KART',
            r'C\*-ç’°|C-star algebra',
            r'è‡ªå·±å…±å½¹|self-adjoint',
            r'ã‚¹ãƒšã‚¯ãƒˆãƒ«|spectrum'
        ]

        # SO(8)å¹¾ä½•å­¦é–¢é€£
        so8_patterns = [
            r'SO\(8\)',
            r'Lie algebra',
            r'rotation gate',
            r'geometric intelligence'
        ]

        for filename, content in self.theory_content.items():
            # URTæ¦‚å¿µæŠ½å‡º
            for pattern in urt_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                concepts['urt'].extend(matches)

            # NC-KARTæ¦‚å¿µæŠ½å‡º
            for pattern in nc_kart_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                concepts['nc_kart'].extend(matches)

            # éå¯æ›KARTæ¦‚å¿µæŠ½å‡º
            for pattern in noncomm_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                concepts['noncommutative_kart'].extend(matches)

            # SO(8)æ¦‚å¿µæŠ½å‡º
            for pattern in so8_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                concepts['so8_geometry'].extend(matches)

        # é‡è¤‡é™¤å»
        for key in concepts:
            concepts[key] = list(set(concepts[key]))

        return concepts

    def generate_theory_based_examples(self) -> List[Dict[str, Any]]:
        """ç†è«–ã«åŸºã¥ãå­¦ç¿’ä¾‹ã‚’ç”Ÿæˆ"""
        examples = []
        concepts = self.extract_theoretical_concepts()

        # URTãƒ™ãƒ¼ã‚¹ã®ä¾‹
        for concept in concepts['urt'][:5]:  # ä¸Šä½5å€‹ã‚’ä½¿ç”¨
            examples.append({
                'instruction': f"Explain the concept of {concept} in the context of quantum field theory.",
                'input': '',
                'output': f"{concept} represents the unified mathematical framework for representing quantum fields using exponential decay coefficients and phase correlators.",
                'domain': 'physics',
                'theory_source': 'URT',
                'complexity_score': 0.9,
                'tag': 'escalation'
            })

        # NC-KARTãƒ™ãƒ¼ã‚¹ã®ä¾‹
        for concept in concepts['nc_kart'][:5]:
            examples.append({
                'instruction': f"What is the significance of {concept} in non-commutative geometry?",
                'input': '',
                'output': f"{concept} provides the mathematical foundation for extending classical function decomposition to non-commutative operator algebras using star products.",
                'domain': 'mathematics',
                'theory_source': 'NC-KART',
                'complexity_score': 0.95,
                'tag': 'escalation'
            })

        # éå¯æ›KARTå®šç†ãƒ™ãƒ¼ã‚¹ã®ä¾‹
        for concept in concepts['noncommutative_kart'][:3]:
            examples.append({
                'instruction': f"Prove the {concept} theorem for self-adjoint operators in C*-algebras.",
                'input': '',
                'output': f"The {concept} theorem states that multi-variable operator functions can be decomposed into finite sums and compositions of single-variable operator functions, preserving the structure through continuous functional calculus.",
                'domain': 'mathematics',
                'theory_source': 'NonCommutative-KART',
                'complexity_score': 0.98,
                'tag': 'escalation'
            })

        # SO(8)å¹¾ä½•å­¦ãƒ™ãƒ¼ã‚¹ã®ä¾‹
        for concept in concepts['so8_geometry'][:3]:
            examples.append({
                'instruction': f"How does {concept} contribute to geometric intelligence in neural networks?",
                'input': '',
                'output': f"{concept} enables the representation of cognitive processes through rotation gates and Lie algebra operations, providing invariant representations of relational structures.",
                'domain': 'ai_physics',
                'theory_source': 'SO8-Geometry',
                'complexity_score': 0.92,
                'tag': 'escalation'
            })

        return examples

# ==========================================
# 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼
# ==========================================

class SO8TDatasetLoader:
    """SO8Tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼"""

    def __init__(self, config: SO8TDatasetConfig):
        self.config = config
        self.theory_integrator = SO8TTheoryIntegrator(THEORY_FILES)

    def load_science_datasets(self) -> List[Dict[str, Any]]:
        """ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
        science_data = []

        # æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        try:
            math_ds = load_dataset("AI-MO/NuminaMath-CoT", split="train")
            sample_size = min(15000, len(math_ds))
            for item in tqdm(math_ds.select(range(sample_size)), desc="Loading Math"):
                if self._quality_filter(item, domain='math'):
                    science_data.append({
                        'instruction': item.get('problem', ''),
                        'input': '',
                        'output': item.get('solution', ''),
                        'domain': 'math',
                        'source': 'AI-MO/NuminaMath-CoT'
                    })
        except Exception as e:
            print(f"Failed to load math dataset: {e}")

        # ç‰©ç†ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        try:
            physics_ds = load_dataset("camel-ai/physics", split="train")
            sample_size = min(12000, len(physics_ds))
            for item in tqdm(physics_ds.select(range(sample_size)), desc="Loading Physics"):
                if self._quality_filter(item, domain='physics'):
                    science_data.append({
                        'instruction': item.get('message_1', ''),
                        'input': '',
                        'output': item.get('message_2', ''),
                        'domain': 'physics',
                        'source': 'camel-ai/physics'
                    })
        except Exception as e:
            print(f"Failed to load physics dataset: {e}")

        # åŒ–å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        try:
            chemistry_ds = load_dataset("camel-ai/chemistry", split="train")
            sample_size = min(8000, len(chemistry_ds))
            for item in tqdm(chemistry_ds.select(range(sample_size)), desc="Loading Chemistry"):
                if self._quality_filter(item, domain='chemistry'):
                    science_data.append({
                        'instruction': item.get('message_1', ''),
                        'input': '',
                        'output': item.get('message_2', ''),
                        'domain': 'chemistry',
                        'source': 'camel-ai/chemistry'
                    })
        except Exception as e:
            print(f"Failed to load chemistry dataset: {e}")

        return science_data

    def load_japanese_datasets(self) -> List[Dict[str, Any]]:
        """æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
        japanese_data = []

        datasets_to_load = [
            ("elyza/ELYZA-tasks-100", "test"),  # splitä¿®æ­£
            ("izumi-lab/llm-japanese-dataset", "train"),
            ("hotchpotch/japanese-novel-instructions", "train"),
            ("microsoft/DialoGPT-medium", "train"),  # è¿½åŠ 
            ("rinna/japanese-gpt-1b", "train")  # è¿½åŠ ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰
        ]

        for dataset_name, split in datasets_to_load:
            try:
                # å­˜åœ¨ã—ãªã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                skip_datasets = ["llm-jp/magpie-pro-200k-ja", "rinna/japanese-gpt-1b"]
                if dataset_name in skip_datasets:
                    continue

                ds = load_dataset(dataset_name, split=split)
                sample_size = min(4000, len(ds))  # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¢—ã‚„ã™

                for item in tqdm(ds.select(range(sample_size)),
                               desc=f"Loading {dataset_name}"):
                    if self._quality_filter(item, domain='japanese'):
                        japanese_data.append({
                            'instruction': item.get('instruction') or item.get('input', ''),
                            'input': '',
                            'output': item.get('output') or item.get('response', ''),
                            'domain': 'japanese',
                            'source': dataset_name
                        })
            except Exception as e:
                print(f"Failed to load {dataset_name}: {e}")

        return japanese_data

    def load_nsfw_datasets(self) -> List[Dict[str, Any]]:
        """NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå®‰å…¨å­¦ç¿’ç”¨ï¼‰"""
        nsfw_data = []

        # å®‰å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåé¢æ•™å¸«ç”¨ï¼‰
        safety_datasets = [
            "Anthropic/hh-rlhf",
            "PKU-Alignment/PKU-SafeRLHF",
            "HuggingFaceH4/ultrafeedback_binarized"  # è¿½åŠ 
        ]

        for dataset_name in safety_datasets:
            try:
                ds = load_dataset(dataset_name, split="train")
                sample_size = min(3000, len(ds))  # ã‚µãƒ³ãƒ—ãƒ«æ•°å¢—

                for item in tqdm(ds.select(range(sample_size)),
                               desc=f"Loading {dataset_name}"):
                    # NSFW/å®‰å…¨é–¢é€£ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
                    text = str(item)
                    if self._contains_nsfw_keywords(text) or self._contains_safety_keywords(text):
                        nsfw_data.append({
                            'instruction': item.get('input', '')[:200],  # çŸ­ãã™ã‚‹
                            'input': '',
                            'output': item.get('output', '')[:500],
                            'domain': 'safety',
                            'source': dataset_name,
                            'nsfw_flag': True
                        })
            except Exception as e:
                print(f"Failed to load {dataset_name}: {e}")

        return nsfw_data

    def load_arxiv_papers(self) -> List[Dict[str, Any]]:
        """ArXivè«–æ–‡ã‚’ãƒ­ãƒ¼ãƒ‰"""
        arxiv_data = []

        if not ARXIV_AVAILABLE:
            print("ArXiv library not available, skipping ArXiv data")
            return arxiv_data

        # SO(8), éå¯æ›, URTé–¢é€£ã®è«–æ–‡ã‚’æ¤œç´¢
        search_queries = [
            'ti:"SO(8)" OR ti:"spin(8)" OR ti:"special orthogonal"',
            'ti:"non-commutative" AND (ti:"geometry" OR ti:"field" OR ti:"algebra")',
            'ti:"unified representation theorem" OR ti:"URT"',
            'ti:"quantum field theory" AND ti:"representation"',
            'ti:"lie algebra" AND ti:"quantum"',
            'ti:"operator algebra" AND ti:"mathematical physics"'
        ]

        try:
            for query in search_queries:
                search = arxiv.Search(
                    query=query,
                    max_results=100,  # å¢—ã‚„ã™
                    sort_by=arxiv.SortCriterion.Relevance
                )

                for result in search.results():
                    # è«–æ–‡ã®è¦æ—¨ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
                    abstract = result.summary
                    if len(abstract) > 150 and self._contains_science_terms(abstract):
                        arxiv_data.append({
                            'instruction': f"Explain the significance of the paper '{result.title}' in mathematical physics.",
                            'input': '',
                            'output': f"Abstract: {abstract[:1200]}...",
                            'domain': 'arxiv_physics',
                            'source': 'arxiv',
                            'arxiv_id': result.entry_id
                        })

                    if len(arxiv_data) >= 2000:  # åˆ¶é™ç·©å’Œ
                        break

        except Exception as e:
            print(f"Failed to load ArXiv data: {e}")

        return arxiv_data

    def _quality_filter(self, item: Dict[str, Any], domain: str = 'general') -> bool:
        """å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        text = ""
        for key in ['instruction', 'input', 'output', 'text', 'content']:
            if key in item and isinstance(item[key], str):
                text += item[key] + " "

        # é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç·©ãï¼‰
        if len(text.strip()) < 50:  # 50æ–‡å­—ä»¥ä¸Šã«ç·©å’Œ
            return False

        if len(text.strip()) > self.config.max_length:
            return False

        # æ‹’çµ¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆä¸€éƒ¨ç·©å’Œï¼‰
        rejection_words = [
            "I don't know", "I cannot", "As an AI", "sorry", "unable to"
        ]

        for word in rejection_words:
            if word.lower() in text.lower():
                return False

        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç·©ãï¼‰
        # æ•°å­¦ã¯LaTeXãŒãªãã¦ã‚‚OKã€ç§‘å­¦ç”¨èªãŒã‚ã‚Œã°ãƒœãƒ¼ãƒŠã‚¹

        return True

    def _has_latex(self, text: str) -> bool:
        """LaTeXæ•°å¼ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        latex_patterns = [r'\\frac', r'\\int', r'\\sum', r'\$', r'\\partial', r'\\alpha']
        return any(re.search(pattern, text) for pattern in latex_patterns)

    def _contains_nsfw_keywords(self, text: str) -> bool:
        """NSFWã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        nsfw_keywords = [
            'sex', 'porn', 'nude', 'naked', 'erotic', 'sexual', 'adult',
            'violence', 'kill', 'death', 'murder', 'harm', 'illegal'
        ]
        return any(kw in text.lower() for kw in nsfw_keywords)

    def _contains_safety_keywords(self, text: str) -> bool:
        """å®‰å…¨é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        safety_keywords = [
            'safety', 'alignment', 'harmful', 'dangerous', 'ethical',
            'responsible', 'bias', 'fairness', 'toxicity'
        ]
        return any(kw in text.lower() for kw in safety_keywords)

    def _contains_science_terms(self, text: str) -> bool:
        """ç§‘å­¦ç”¨èªã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        science_terms = [
            'theorem', 'proof', 'quantum', 'field', 'algebra', 'geometry',
            'topology', 'manifold', 'operator', 'spectrum', 'invariant'
        ]
        return any(term in text.lower() for term in science_terms)

# ==========================================
# 4. å››å€¤åˆ†é¡ã‚¿ã‚°ä»˜ä¸
# ==========================================

class SO8TQuadClassifier:
    """SO8Tå››å€¤åˆ†é¡å™¨"""

    def __init__(self, config: SO8TDatasetConfig):
        self.config = config

    def classify_example(self, example: Dict[str, Any]) -> str:
        """å˜ä¸€ä¾‹ã«å››å€¤åˆ†é¡ã‚¿ã‚°ã‚’ä»˜ä¸"""
        text = ""
        for key in ['instruction', 'input', 'output']:
            if key in example and isinstance(example[key], str):
                text += example[key] + " "

        text_lower = text.lower()

        # Refuse: NSFW/å±é™ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if any(kw in text_lower for kw in self.config.refuse_keywords):
            return 'refuse'

        # Deny: èª¤ã‚Š/çŸ›ç›¾
        if any(kw in text_lower for kw in self.config.deny_keywords):
            return 'deny'

        # Escalation: è¤‡é›‘/é«˜åº¦ãªå†…å®¹
        if any(kw in text_lower for kw in self.config.escalation_keywords):
            return 'escalation'

        # Allow: å˜ç´”/åŸºæœ¬çš„ãªå†…å®¹
        if any(kw in text_lower for kw in self.config.allow_keywords):
            return 'allow'

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¤‡é›‘åº¦ã«åŸºã¥ãåˆ¤å®š
        complexity = self._calculate_complexity(text)
        if complexity > 0.8:
            return 'escalation'
        elif complexity > 0.5:
            return 'allow'
        else:
            return 'allow'  # å®‰å…¨å´ã«å€’ã™

    def _calculate_complexity(self, text: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã®è¤‡é›‘åº¦ã‚’è¨ˆç®—"""
        if not text:
            return 0.0

        # å°‚é–€ç”¨èªå¯†åº¦
        science_terms = [
            'theorem', 'proof', 'quantum', 'field', 'algebra', 'geometry',
            'topology', 'operator', 'spectrum', 'invariant', 'non-commutative'
        ]

        words = re.findall(r'\b\w+\b', text.lower())
        if not words:
            return 0.0

        term_count = sum(1 for word in words if word in science_terms)
        term_density = term_count / len(words)

        # LaTeXå¯†åº¦
        latex_chars = len(re.findall(r'\\[a-zA-Z]+|\$[^$]*\$|\\\[.*?\\\]', text))
        latex_density = latex_chars / len(text) if text else 0

        # é•·ã•ã‚¹ã‚³ã‚¢
        length_score = min(len(text) / 1000, 1.0)

        return (term_density * 0.4 + latex_density * 0.3 + length_score * 0.3)

    def batch_classify(self, examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒåˆ†é¡"""
        classified = []
        for example in tqdm(examples, desc="Classifying examples"):
            tag = self.classify_example(example)
            example_copy = example.copy()
            example_copy['tag'] = tag
            classified.append(example_copy)

        return classified

# ==========================================
# 5. çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
# ==========================================

class SO8TUnifiedDataset:
    """SO8Tçµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, config: SO8TDatasetConfig):
        self.config = config
        self.loader = SO8TDatasetLoader(config)
        self.classifier = SO8TQuadClassifier(config)
        self.theory_integrator = SO8TTheoryIntegrator(THEORY_FILES)

    def create_unified_dataset(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        print("ğŸš€ Creating SO8T Unified Dataset")
        print("=" * 50)

        # ç†è«–ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
        print("ğŸ“š Loading theory files...")
        theory_data = self.theory_integrator.load_theory_files()
        theory_examples = self.theory_integrator.generate_theory_based_examples()

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
        print("ğŸ”¬ Loading science datasets...")
        science_data = self.loader.load_science_datasets()

        print("ğŸ‡¯ğŸ‡µ Loading Japanese datasets...")
        japanese_data = self.loader.load_japanese_datasets()

        print("ğŸ›¡ï¸ Loading NSFW/Safety datasets...")
        nsfw_data = self.loader.load_nsfw_datasets()

        print("ğŸ“„ Loading ArXiv papers...")
        arxiv_data = self.loader.load_arxiv_papers()

        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        all_data = theory_examples + science_data + japanese_data + nsfw_data + arxiv_data

        print(f"ğŸ“Š Total examples collected: {len(all_data)}")

        # å››å€¤åˆ†é¡ã‚¿ã‚°ä»˜ä¸
        print("ğŸ·ï¸ Applying quad-classification tags...")
        classified_data = self.classifier.batch_classify(all_data)

        # DataFrameå¤‰æ›
        df = pd.DataFrame(classified_data)

        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        print("â­ Calculating quality scores...")
        df = self._add_quality_scores(df)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        print("ğŸ” Applying final filters...")
        df_filtered = self._apply_final_filters(df)

        # ã‚¿ã‚°åˆ†å¸ƒè¡¨ç¤º
        print("\nğŸ“ˆ Tag Distribution:")
        print(df_filtered['tag'].value_counts())

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (æ•™å¸«ãƒ‡ãƒ¼ã‚¿/å­¦ç¿’ãƒ‡ãƒ¼ã‚¿)
        print("âœ‚ï¸ Splitting into train/validation sets...")
        train_df, val_df = self._split_dataset(df_filtered)

        # SO8Tå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ä¸
        train_df = self._add_so8t_system_prompts(train_df)
        val_df = self._add_so8t_system_prompts(val_df)

        return train_df, val_df, df_filtered

    def _add_quality_scores(self, df: pd.DataFrame) -> pd.DataFrame:
        """å“è³ªã‚¹ã‚³ã‚¢ã‚’è¿½åŠ """
        def calculate_quality_score(row):
            score = 0.0

            # é•·ã•ã‚¹ã‚³ã‚¢
            text_length = len(str(row.get('instruction', '')) + str(row.get('output', '')))
            length_score = min(text_length / 1000, 1.0)
            score += length_score * 0.3

            # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢
            complexity = self.classifier._calculate_complexity(
                str(row.get('instruction', '')) + str(row.get('output', ''))
            )
            score += complexity * 0.4

            # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒœãƒ¼ãƒŠã‚¹
            domain = row.get('domain', '')
            if domain in ['math', 'physics', 'arxiv_physics']:
                score += 0.2
            elif domain == 'japanese':
                score += 0.1

            # ç†è«–ã‚½ãƒ¼ã‚¹ãƒœãƒ¼ãƒŠã‚¹
            if 'theory_source' in row:
                score += 0.1

            return min(score, 1.0)

        df['quality_score'] = df.apply(calculate_quality_score, axis=1)
        return df

    def _apply_final_filters(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç·©å’Œç‰ˆï¼‰"""
        # å“è³ªã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        df_sorted = df.sort_values('quality_score', ascending=False)

        # ç›®æ¨™ã‚µãƒ³ãƒ—ãƒ«æ•°ã«åˆ¶é™ï¼ˆã‚ˆã‚Šå¤šãæ®‹ã™ï¼‰
        target_samples = int(self.config.total_samples * 1.5)  # å¤šã‚ã«
        df_filtered = df_sorted.head(min(target_samples, len(df_sorted)))

        # æœ€å°å“è³ªé–¾å€¤ï¼ˆã‚ˆã‚Šç·©ãï¼‰
        df_filtered = df_filtered[df_filtered['quality_score'] > 0.1]

        # æœ€ä½é™ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç¢ºä¿
        if len(df_filtered) < 100:
            # å“è³ªé–¾å€¤ã‚’ã•ã‚‰ã«ä¸‹ã’ã‚‹
            df_filtered = df_sorted.head(min(1000, len(df_sorted)))
            df_filtered = df_filtered[df_filtered['quality_score'] > 0.05]

        return df_filtered

    def _split_dataset(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²"""
        # å±¤åŒ–åˆ†å‰²ï¼ˆã‚¿ã‚°ã”ã¨ã«ãƒãƒ©ãƒ³ã‚¹ã‚ˆãï¼‰
        train_df, val_df = train_test_split(
            df,
            test_size=0.2,
            stratify=df['tag'],
            random_state=42
        )

        return train_df, val_df

    def _add_so8t_system_prompts(self, df: pd.DataFrame) -> pd.DataFrame:
        """SO8Tå°‚ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ """
        def get_so8t_system_prompt(tag: str) -> str:
            base_prompt = """ã‚ãªãŸã¯SO(8)å¹¾ä½•å­¦çš„çŸ¥æ€§ã‚’æŒã¤AIã§ã™ã€‚
URT (Unified Representation Theorem) ã¨ NC-KARTâ˜… (Non-Commutative Kolmogorov-Arnold Representation Theory) ã«åŸºã¥ãã€
éå¯æ›KARTå®šç†ã®æ•°å­¦çš„æ çµ„ã¿ã§æ€è€ƒã—ã¾ã™ã€‚

å¿œç­”æˆ¦ç•¥:
- <|allow|>: å˜ç´”ãªè³ªå•ã«ç›´æ¥å›ç­”
- <|escalation|>: è¤‡é›‘ãªå•é¡Œã§å››é‡æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç™ºå‹•
- <|deny|>: è«–ç†çš„èª¤ã‚Šã‚’è¨‚æ­£
- <|refuse|>: å€«ç†çš„ãƒ»ç‰©ç†çš„ã«å•é¡Œã®ã‚ã‚‹ã‚¯ã‚¨ãƒªã‚’æ‹’å¦

ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰: {tag}
"""

            return base_prompt.format(tag=tag)

        df['system'] = df['tag'].apply(get_so8t_system_prompt)
        return df

    def save_datasets(self, train_df: pd.DataFrame, val_df: pd.DataFrame,
                     output_dir: str = "data/so8t_unified"):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜"""
        os.makedirs(output_dir, exist_ok=True)

        # JSONLå½¢å¼ã§ä¿å­˜
        train_path = os.path.join(output_dir, "train.jsonl")
        val_path = os.path.join(output_dir, "validation.jsonl")

        print(f"ğŸ’¾ Saving training dataset ({len(train_df)} examples) to {train_path}")
        train_df.to_json(train_path, orient='records', lines=True, force_ascii=False)

        print(f"ğŸ’¾ Saving validation dataset ({len(val_df)} examples) to {val_path}")
        val_df.to_json(val_path, orient='records', lines=True, force_ascii=False)

        # çµ±è¨ˆæƒ…å ±ä¿å­˜
        stats = {
            'total_train': len(train_df),
            'total_val': len(val_df),
            'tag_distribution_train': train_df['tag'].value_counts().to_dict(),
            'tag_distribution_val': val_df['tag'].value_counts().to_dict(),
            'domain_distribution': train_df['domain'].value_counts().to_dict(),
            'created_at': datetime.now().isoformat(),
            'theory_integrated': True,
            'so8t_optimized': True
        }

        stats_path = os.path.join(output_dir, "dataset_stats.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š Dataset statistics saved to {stats_path}")

        return train_path, val_path, stats_path

# ==========================================
# 6. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="SO8T Unified Dataset Creation")
    parser.add_argument("--output_dir", type=str, default="data/so8t_unified",
                       help="Output directory for datasets")
    parser.add_argument("--total_samples", type=int, default=50000,
                       help="Total number of samples")
    parser.add_argument("--science_ratio", type=float, default=0.4,
                       help="Science dataset ratio")
    parser.add_argument("--japanese_ratio", type=float, default=0.3,
                       help="Japanese dataset ratio")
    parser.add_argument("--nsfw_ratio", type=float, default=0.2,
                       help="NSFW/Safety dataset ratio")
    parser.add_argument("--arxiv_ratio", type=float, default=0.1,
                       help="ArXiv dataset ratio")
    parser.add_argument("--test_run", action="store_true",
                       help="Run with small sample size for testing")

    args = parser.parse_args()

    # è¨­å®š
    config = SO8TDatasetConfig(
        total_samples=args.total_samples if not args.test_run else 1000,
        science_ratio=args.science_ratio,
        japanese_ratio=args.japanese_ratio,
        nsfw_ratio=args.nsfw_ratio,
        arxiv_ratio=args.arxiv_ratio
    )

    # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    dataset_creator = SO8TUnifiedDataset(config)

    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_df, val_df, full_df = dataset_creator.create_unified_dataset()

        # ä¿å­˜
        train_path, val_path, stats_path = dataset_creator.save_datasets(
            train_df, val_df, args.output_dir
        )

        print("\n" + "="*60)
        print("ğŸ‰ SO8T Unified Dataset Creation Complete!")
        print("="*60)
        print(f"ğŸ“ Output Directory: {args.output_dir}")
        print(f"ğŸ“š Training Samples: {len(train_df)}")
        print(f"ğŸ§ª Validation Samples: {len(val_df)}")
        print(f"ğŸ·ï¸ Tag Distribution (Train): {train_df['tag'].value_counts().to_dict()}")
        print(f"ğŸ·ï¸ Tag Distribution (Val): {val_df['tag'].value_counts().to_dict()}")
        print("\nğŸš€ Ready for SO(8) Think PPO Training!")
        print("   Use train_ppo_aegis.py with --dataset_path", args.output_dir)

    except Exception as e:
        print(f"âŒ Error during dataset creation: {e}")
        raise

if __name__ == "__main__":
    main()
