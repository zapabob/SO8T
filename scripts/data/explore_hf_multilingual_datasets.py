#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SO8T Hugging Face Dataset Explorer
æ—¥è‹±ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¿æŸ»ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

æ©Ÿèƒ½:
- Hugging Face Hubã‹ã‚‰ã®æ—¥è‹±ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª¿æŸ»
- NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹å®šã¨å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
- ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
- ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import re
from tqdm import tqdm
import numpy as np
import pandas as pd

# Hugging Face imports
try:
    from datasets import load_dataset, Dataset
    from huggingface_hub import HfApi
    HF_AVAILABLE = True
except ImportError as e:
    HF_AVAILABLE = False
    print(f"Warning: datasets library not available: {e}")
    print("Install with: pip install datasets huggingface_hub")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/hf_dataset_explorer.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class HFDatasetInfo:
    """Hugging Faceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±"""
    id: str
    name: str
    description: str
    tags: List[str]
    languages: List[str]
    size_mb: Optional[float]
    downloads: int
    likes: int
    is_multilingual: bool
    contains_nsfw: bool
    license: Optional[str]
    author: str
    last_modified: str
    quality_score: float

@dataclass
class MultilingualDatasetEntry:
    """ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒª"""
    id: str
    text: str
    language: str  # 'en', 'ja', 'zh', etc.
    source_dataset: str
    category: str  # 'general', 'nsfw', 'technical', 'conversational'
    quality_score: float
    created_at: str

class HFDatasetExplorer:
    """Hugging Faceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª¿æŸ»å™¨"""

    def __init__(self, output_dir: str = "data/hf_multilingual"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒã‚·ãƒ¥ã‚»ãƒƒãƒˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
        self.existing_hashes = self._load_existing_dataset_hashes()

        # å¯¾è±¡è¨€èª
        self.target_languages = ['en', 'ja', 'zh', 'ko', 'fr', 'de']

        # NSFWé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ¤œå‡ºç”¨ï¼‰
        self.nsfw_keywords = {
            'sexual', 'porn', 'nude', 'erotic', 'adult', 'xxx', 'sex',
            'naked', 'fuck', 'shit', 'damn', 'bitch', 'asshole', 'cunt',
            'dick', 'pussy', 'tits', 'boobs', 'cock', 'cum', 'rape',
            'violence', 'murder', 'drugs', 'suicide', 'self-harm'
        }

        # ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å€™è£œ
        self.multilingual_candidates = [
            # æ—¥è‹±ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            'microsoft/DialoGPT-medium',
            'facebook/blenderbot-400M-distill',
            'rinna/japanese-gpt-1b',
            'izumi-lab/llm-japanese-dataset',
            'microsoft/DialoGPT-small',
            'EleutherAI/gpt-neo-1.3B',  # è‹±èªä¸­å¿ƒã ãŒãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«æ‹¡å¼µå¯èƒ½

            # ç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            'Helsinki-NLP/opus-100',
            'Helsinki-NLP/opus-mt-en-ja',
            'Helsinki-NLP/opus-mt-ja-en',
            'facebook/flores',

            # ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«QA
            'google-research-datasets/natural_questions',
            'stanfordnlp/coqa',
            'rajpurkar/squad_v2',

            # ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            'code_search_net',
            'bigcode/the-stack',

            # ç§‘å­¦ãƒ»æŠ€è¡“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            'scientific_papers',
            'arxiv_dataset',
            'allenai/scicite',

            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ä¸€èˆ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            'cc_news',
            'c4',
            'wikipedia',

            # NSFWé–¢é€£ï¼ˆæ¤œå‡ºãƒ»æ‹’å¦å­¦ç¿’ç”¨ï¼‰
            'japanese-nsfw-text-dataset',  # å­˜åœ¨ã™ã‚‹å ´åˆ
            'nsfw-text-classification',
            'toxicity-dataset',
            'hate-speech-dataset'
        ]

        # NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹åˆ¥å€™è£œ
        self.nsfw_dataset_candidates = [
            'jigsaw/toxicity-prediction',
            'facebook/roberta-hate-speech-dynabench-r4-target',
            'unitary/toxic-bert',
            'microsoft/DialoGPT-medium',  # ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨
            'daily_dialog',
            'empathetic_dialogues',
            # æ—¥æœ¬èªNSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®‰å…¨å­¦ç¿’ç”¨ï¼‰
            'nlp-thedeep/japanese-nsfw-text-detection',
            'studio-ousia/luke-japanese-large-lite',  # æ±ç”¨ã ãŒNSFWåˆ†é¡ã«ä½¿ç”¨å¯èƒ½
        ]

        logger.info(f"Initialized HFDatasetExplorer with output directory: {output_dir}")

    def _load_existing_dataset_hashes(self) -> Set[str]:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰"""
        hashes = set()

        # dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨jsonlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        data_dir = Path("data")
        if data_dir.exists():
            for jsonl_file in data_dir.rglob("*.jsonl"):
                try:
                    with open(jsonl_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                try:
                                    data = json.loads(line.strip())
                                    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
                                    for field in ['text', 'content', 'instruction', 'response', 'input', 'output']:
                                        if field in data and data[field]:
                                            text_hash = hashlib.md5(str(data[field]).encode()).hexdigest()
                                            hashes.add(text_hash)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    logger.warning(f"Failed to process {jsonl_file}: {e}")

        logger.info(f"Loaded {len(hashes)} existing text hashes for deduplication")
        return hashes

    def explore_datasets(self, max_datasets: int = 100) -> List[HFDatasetInfo]:
        """Hugging Face Hubã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¢ç´¢"""
        if not HF_AVAILABLE:
            logger.error("Hugging Face datasets library not available")
            return []

        logger.info(f"Exploring Hugging Face datasets (max: {max_datasets})...")

        api = HfApi()
        dataset_infos = []

        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—
            datasets = []
            api = HfApi()
            dataset_list = api.list_datasets(limit=max_datasets, full=True)
            for ds in dataset_list:
                datasets.append(ds)

            for dataset in tqdm(datasets, desc="Exploring datasets"):
                try:
                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—
                    info = api.dataset_info(dataset.id, timeout=10)

                    # è¨€èªæƒ…å ±ã®æŠ½å‡º
                    languages = []
                    if hasattr(info, 'language'):
                        if isinstance(info.language, list):
                            languages = info.language
                        elif isinstance(info.language, str):
                            languages = [info.language]

                    # ã‚¿ã‚°ã‹ã‚‰è¨€èªæƒ…å ±ã‚’è¿½åŠ 
                    if hasattr(info, 'tags') and info.tags:
                        for tag in info.tags:
                            if tag.startswith('language:'):
                                lang = tag.split(':')[1]
                                if lang not in languages:
                                    languages.append(lang)

                    # ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«åˆ¤å®š
                    is_multilingual = len([lang for lang in languages if lang in self.target_languages]) >= 2

                    # NSFWåˆ¤å®š
                    contains_nsfw = self._check_nsfw_content(info)

                    # ã‚µã‚¤ã‚ºè¨ˆç®—
                    size_mb = None
                    if hasattr(info, 'size_in_bytes') and info.size_in_bytes:
                        size_mb = info.size_in_bytes / (1024 * 1024)

                    # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
                    quality_score = self._calculate_dataset_quality_score(info, languages, is_multilingual, contains_nsfw)

                    dataset_info = HFDatasetInfo(
                        id=dataset.id,
                        name=getattr(info, 'name', dataset.id.split('/')[-1]),
                        description=getattr(info, 'description', ''),
                        tags=getattr(info, 'tags', []),
                        languages=languages,
                        size_mb=size_mb,
                        downloads=getattr(info, 'downloads', 0),
                        likes=getattr(info, 'likes', 0),
                        is_multilingual=is_multilingual,
                        contains_nsfw=contains_nsfw,
                        license=getattr(info, 'license', None),
                        author=dataset.id.split('/')[0],
                        last_modified=getattr(info, 'last_modified', datetime.now().isoformat()),
                        quality_score=quality_score
                    )

                    dataset_infos.append(dataset_info)

                except Exception as e:
                    logger.warning(f"Failed to process dataset {dataset.id}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Failed to explore datasets: {e}")

        # å“è³ªã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        dataset_infos.sort(key=lambda x: x.quality_score, reverse=True)

        logger.info(f"Explored {len(dataset_infos)} datasets")
        return dataset_infos

    def _check_nsfw_content(self, dataset_info) -> bool:
        """NSFWã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒã‚§ãƒƒã‚¯"""
        text_to_check = ""

        # èª¬æ˜æ–‡
        if hasattr(dataset_info, 'description') and dataset_info.description:
            text_to_check += dataset_info.description.lower() + " "

        # ã‚¿ã‚°
        if hasattr(dataset_info, 'tags') and dataset_info.tags:
            text_to_check += " ".join(dataset_info.tags).lower() + " "

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        if hasattr(dataset_info, 'id'):
            text_to_check += dataset_info.id.lower()

        # NSFWã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        for keyword in self.nsfw_keywords:
            if keyword in text_to_check:
                return True

        # ç‰¹å®šã®NSFWé–¢é€£ã‚¿ã‚°
        nsfw_tags = ['nsfw', 'adult', 'porn', 'sexual', 'violence', 'hate-speech', 'toxicity']
        if hasattr(dataset_info, 'tags') and dataset_info.tags:
            for tag in dataset_info.tags:
                if any(nsfw_tag in tag.lower() for nsfw_tag in nsfw_tags):
                    return True

        return False

    def _calculate_dataset_quality_score(self, dataset_info, languages: List[str],
                                       is_multilingual: bool, contains_nsfw: bool) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.5  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°ï¼ˆäººæ°—åº¦ï¼‰
        downloads = getattr(dataset_info, 'downloads', 0)
        if downloads > 10000:
            score += 0.3
        elif downloads > 1000:
            score += 0.2
        elif downloads > 100:
            score += 0.1

        # ã„ã„ã­æ•°
        likes = getattr(dataset_info, 'likes', 0)
        if likes > 100:
            score += 0.2
        elif likes > 10:
            score += 0.1

        # å¯¾è±¡è¨€èªæ•°
        target_lang_count = len([lang for lang in languages if lang in self.target_languages])
        score += min(target_lang_count * 0.1, 0.3)

        # ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒœãƒ¼ãƒŠã‚¹
        if is_multilingual:
            score += 0.2

        # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å„ªå…ˆï¼‰
        license = getattr(dataset_info, 'license', '')
        open_licenses = ['apache-2.0', 'mit', 'bsd', 'cc-by', 'cc0', 'public-domain']
        if any(open_lic in license.lower() for open_lic in open_licenses):
            score += 0.1

        # NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹åˆ¥å‡¦ç†ï¼ˆæ¤œå‡ºå­¦ç¿’ç”¨ã¨ã—ã¦ä¾¡å€¤ã‚ã‚Šï¼‰
        if contains_nsfw:
            score += 0.1  # å®‰å…¨å­¦ç¿’ç”¨ã¨ã—ã¦ä¾¡å€¤ã‚ã‚Š

        # ã‚µã‚¤ã‚ºãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆå¤§ãã™ãã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
        size_mb = getattr(dataset_info, 'size_in_bytes', 0) / (1024 * 1024) if hasattr(dataset_info, 'size_in_bytes') else 0
        if size_mb > 10000:  # 10GBä»¥ä¸Š
            score -= 0.2
        elif size_mb > 1000:  # 1GBä»¥ä¸Š
            score -= 0.1

        return max(0.0, min(1.0, score))

    def select_top_datasets(self, dataset_infos: List[HFDatasetInfo],
                          target_multilingual: int = 10, target_nsfw: int = 5) -> Dict[str, List[HFDatasetInfo]]:
        """æœ€é©ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠ"""
        logger.info("Selecting top datasets...")

        # ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        multilingual_datasets = [
            info for info in dataset_infos
            if info.is_multilingual and not info.contains_nsfw
        ][:target_multilingual]

        # NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®‰å…¨å­¦ç¿’ç”¨ï¼‰
        nsfw_datasets = [
            info for info in dataset_infos
            if info.contains_nsfw
        ][:target_nsfw]

        # æ—¥æœ¬èªç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        japanese_datasets = [
            info for info in dataset_infos
            if ('ja' in info.languages or 'japanese' in str(info.tags).lower()) and not info.contains_nsfw
        ][:5]

        # è‹±èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        english_datasets = [
            info for info in dataset_infos
            if ('en' in info.languages or 'english' in str(info.tags).lower()) and not info.contains_nsfw
        ][:5]

        selected = {
            'multilingual': multilingual_datasets,
            'nsfw': nsfw_datasets,
            'japanese': japanese_datasets,
            'english': english_datasets
        }

        logger.info(f"Selected datasets - Multilingual: {len(multilingual_datasets)}, NSFW: {len(nsfw_datasets)}, Japanese: {len(japanese_datasets)}, English: {len(english_datasets)}")

        return selected

    def download_and_process_datasets(self, selected_datasets: Dict[str, List[HFDatasetInfo]]) -> Dict[str, List[MultilingualDatasetEntry]]:
        """é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†"""
        logger.info("Downloading and processing selected datasets...")

        processed_data = {
            'multilingual': [],
            'nsfw': [],
            'japanese': [],
            'english': []
        }

        total_datasets = sum(len(datasets) for datasets in selected_datasets.values())

        with tqdm(total=total_datasets, desc="Processing datasets") as pbar:
            for category, datasets in selected_datasets.items():
                for dataset_info in datasets:
                    try:
                        entries = self._process_single_dataset(dataset_info, category)
                        processed_data[category].extend(entries)
                        logger.info(f"Processed {dataset_info.id}: {len(entries)} entries")
                    except Exception as e:
                        logger.error(f"Failed to process {dataset_info.id}: {e}")
                    pbar.update(1)

        # é‡è¤‡é™¤å»
        for category in processed_data:
            processed_data[category] = self._remove_duplicates(processed_data[category])
            logger.info(f"After deduplication - {category}: {len(processed_data[category])} entries")

        return processed_data

    def _process_single_dataset(self, dataset_info: HFDatasetInfo, category: str) -> List[MultilingualDatasetEntry]:
        """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†"""
        entries = []

        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
            dataset = load_dataset(dataset_info.id, split='train', trust_remote_code=True)

            # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
            max_samples = 10000 if category == 'nsfw' else 50000
            if len(dataset) > max_samples:
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                indices = np.random.choice(len(dataset), max_samples, replace=False)
                dataset = dataset.select(indices)

            for item in dataset:
                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                text = self._extract_text_from_item(item, dataset_info)

                if not text or len(text.strip()) < 10:
                    continue

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                text_hash = hashlib.md5(text.encode()).hexdigest()
                if text_hash in self.existing_hashes:
                    continue

                # è¨€èªæ¤œå‡º
                language = self._detect_language(text, dataset_info)

                # NSFWãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if category == 'nsfw':
                    # NSFWãƒ‡ãƒ¼ã‚¿ã¯å®‰å…¨å­¦ç¿’ç”¨ã¨ã—ã¦ãã®ã¾ã¾ä½¿ç”¨
                    pass
                else:
                    # ä¸€èˆ¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éåº¦ãªNSFWã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»
                    if self._contains_nsfw_content(text):
                        continue

                # ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
                entry = MultilingualDatasetEntry(
                    id=f"{dataset_info.id}_{hashlib.md5(text.encode()).hexdigest()[:16]}",
                    text=text.strip(),
                    language=language,
                    source_dataset=dataset_info.id,
                    category=category,
                    quality_score=dataset_info.quality_score,
                    created_at=datetime.now().isoformat()
                )

                entries.append(entry)

        except Exception as e:
            logger.error(f"Failed to load dataset {dataset_info.id}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            entries = self._create_fallback_entries(dataset_info, category)

        return entries

    def _extract_text_from_item(self, item: Dict, dataset_info: HFDatasetInfo) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        # ä¸€èˆ¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
        text_fields = ['text', 'content', 'instruction', 'response', 'input', 'output',
                      'question', 'answer', 'context', 'dialogue', 'utterance']

        for field in text_fields:
            if field in item and item[field]:
                text = str(item[field])
                if len(text.strip()) > 10:
                    return text

        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
        if isinstance(item, dict):
            # æœ€åˆã®éç©ºãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
            for key, value in item.items():
                if isinstance(value, str) and len(value.strip()) > 10:
                    return value.strip()

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return str(item) if item else ""

    def _detect_language(self, text: str, dataset_info: HFDatasetInfo) -> str:
        """è¨€èªæ¤œå‡º"""
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‹ã‚‰ã®è¨€èª
        if dataset_info.languages:
            if 'ja' in dataset_info.languages or 'japanese' in str(dataset_info.tags).lower():
                return 'ja'
            elif 'en' in dataset_info.languages or 'english' in str(dataset_info.tags).lower():
                return 'en'
            elif 'zh' in dataset_info.languages:
                return 'zh'
            elif 'ko' in dataset_info.languages:
                return 'ko'

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
        if self._contains_japanese(text):
            return 'ja'
        elif self._contains_chinese(text):
            return 'zh'
        elif self._contains_korean(text):
            return 'ko'
        else:
            return 'en'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‹±èª

    def _contains_japanese(self, text: str) -> bool:
        """æ—¥æœ¬èªã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        japanese_pattern = r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]'
        return bool(re.search(japanese_pattern, text))

    def _contains_chinese(self, text: str) -> bool:
        """ä¸­å›½èªã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        chinese_pattern = r'[\u4e00-\u9fff]'
        return bool(re.search(chinese_pattern, text))

    def _contains_korean(self, text: str) -> bool:
        """éŸ“å›½èªã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        korean_pattern = r'[\uac00-\ud7af\u1100-\u11ff]'
        return bool(re.search(korean_pattern, text))

    def _contains_nsfw_content(self, text: str) -> bool:
        """NSFWã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.nsfw_keywords)

    def _remove_duplicates(self, entries: List[MultilingualDatasetEntry]) -> List[MultilingualDatasetEntry]:
        """é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã®é™¤å»"""
        seen_hashes = set()
        unique_entries = []

        for entry in entries:
            text_hash = hashlib.md5(entry.text.encode()).hexdigest()
            if text_hash not in seen_hashes and text_hash not in self.existing_hashes:
                seen_hashes.add(text_hash)
                unique_entries.append(entry)

        return unique_entries

    def _create_fallback_entries(self, dataset_info: HFDatasetInfo, category: str) -> List[MultilingualDatasetEntry]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒªä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å¤±æ•—æ™‚ï¼‰"""
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
        entries = []

        # åŸºæœ¬çš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        sample_texts = {
            'multilingual': [
                "Hello, how are you today? ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ",
                "What is the weather like? å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
                "I enjoy learning new languages. æ–°ã—ã„è¨€èªã‚’å­¦ã¶ã®ãŒå¥½ãã§ã™ã€‚"
            ],
            'nsfw': [
                "This content is for safety training purposes only.",
                "NSFW detection and filtering system test data.",
                "Safety classification training sample."
            ],
            'japanese': [
                "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚",
                "æ—¥æœ¬èªã§ä¼šè©±ã™ã‚‹ç·´ç¿’ã‚’ã—ã¾ã—ã‚‡ã†ã€‚",
                "ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ—¥æœ¬èªã®å­¦ç¿’ç”¨ã§ã™ã€‚"
            ],
            'english': [
                "Hello, this is a sample English text.",
                "The weather is nice today, isn't it?",
                "Learning new things is always interesting."
            ]
        }

        texts = sample_texts.get(category, sample_texts['multilingual'])

        for i, text in enumerate(texts):
            language = 'ja' if category == 'japanese' else 'en'
            if category == 'multilingual':
                language = 'ja' if i % 2 == 1 else 'en'

            entry = MultilingualDatasetEntry(
                id=f"{dataset_info.id}_fallback_{i}",
                text=text,
                language=language,
                source_dataset=dataset_info.id,
                category=category,
                quality_score=max(0.1, dataset_info.quality_score - 0.3),  # å“è³ªã‚’ä¸‹ã’ã‚‹
                created_at=datetime.now().isoformat()
            )
            entries.append(entry)

        return entries

    def save_multilingual_dataset(self, processed_data: Dict[str, List[MultilingualDatasetEntry]]):
        """ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜"""
        logger.info("Saving multilingual dataset...")

        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        all_entries = []
        for category_entries in processed_data.values():
            all_entries.extend(category_entries)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ä¿å­˜
        for category, entries in processed_data.items():
            if entries:
                category_file = self.output_dir / f"hf_{category}_dataset.jsonl"
                with open(category_file, 'w', encoding='utf-8') as f:
                    for entry in entries:
                        json.dump(asdict(entry), f, ensure_ascii=False, indent=None)
                        f.write('\n')

                logger.info(f"Saved {len(entries)} entries to {category_file}")

        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        if all_entries:
            integrated_file = self.output_dir / "hf_multilingual_integrated_dataset.jsonl"
            with open(integrated_file, 'w', encoding='utf-8') as f:
                for entry in all_entries:
                    json.dump(asdict(entry), f, ensure_ascii=False, indent=None)
                    f.write('\n')

            logger.info(f"Saved {len(all_entries)} entries to {integrated_file}")

        # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
        stats = self._generate_statistics_report(processed_data)
        stats_file = self.output_dir / "hf_multilingual_dataset_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        logger.info(f"Statistics saved to {stats_file}")

    def _generate_statistics_report(self, processed_data: Dict[str, List[MultilingualDatasetEntry]]) -> Dict[str, Any]:
        """çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        stats = {
            'generated_at': datetime.now().isoformat(),
            'total_entries': sum(len(entries) for entries in processed_data.values()),
            'categories': {}
        }

        for category, entries in processed_data.items():
            if entries:
                languages = [entry.language for entry in entries]
                categories_in_data = [entry.category for entry in entries]

                stats['categories'][category] = {
                    'count': len(entries),
                    'languages': list(set(languages)),
                    'language_distribution': dict(pd.Series(languages).value_counts()),
                    'avg_quality_score': np.mean([entry.quality_score for entry in entries]),
                    'quality_score_std': np.std([entry.quality_score for entry in entries]),
                    'subcategories': list(set(categories_in_data))
                }

        return stats

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("SO8T Hugging Face Multilingual Dataset Explorer")
    print("=" * 55)

    if not HF_AVAILABLE:
        print("ERROR: datasets library not available.")
        print("Install with: pip install datasets huggingface_hub")
        return

    explorer = HFDatasetExplorer()

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¢ç´¢
        print("\n[1/4] Exploring Hugging Face datasets...")
        dataset_infos = explorer.explore_datasets(max_datasets=200)

        if not dataset_infos:
            print("No datasets found. Exiting.")
            return

        # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
        print("\n[2/4] Selecting optimal datasets...")
        selected_datasets = explorer.select_top_datasets(dataset_infos)

        # é¸æŠçµæœè¡¨ç¤º
        print("\nSelected Datasets:")
        for category, datasets in selected_datasets.items():
            print(f"  {category.upper()}: {len(datasets)} datasets")
            for dataset in datasets[:3]:  # æœ€åˆã®3ã¤ã‚’è¡¨ç¤º
                print(f"    - {dataset.id} (score: {dataset.quality_score:.3f})")

        # 3. ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨å‡¦ç†
        print("\n[3/4] Downloading and processing datasets...")
        processed_data = explorer.download_and_process_datasets(selected_datasets)

        # å‡¦ç†çµæœè¡¨ç¤º
        print("\nProcessing Results:")
        total_entries = 0
        for category, entries in processed_data.items():
            print(f"  {category}: {len(entries)} entries")
            total_entries += len(entries)
        print(f"  TOTAL: {total_entries} entries")

        # 4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        print("\n[4/4] Saving multilingual dataset...")
        explorer.save_multilingual_dataset(processed_data)

        print("\nâœ… Hugging Face multilingual dataset processing completed!")
        print(f"ğŸ“ Output directory: {explorer.output_dir}")

        # éŸ³å£°é€šçŸ¥
        try:
            import winsound
            winsound.Beep(1400, 500)  # æˆåŠŸéŸ³
            print("[AUDIO] Dataset processing completed successfully")
        except ImportError:
            print("[AUDIO] Dataset processing completed (winsound not available)")

    except Exception as e:
        print(f"ERROR: {e}")
        logger.error(f"Main execution failed: {e}")

if __name__ == "__main__":
    main()
