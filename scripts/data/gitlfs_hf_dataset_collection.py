#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Git LFSã‚’ä½¿ç”¨ã—ãŸHuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
NSFWãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€å†…éƒ¨æ¨è«–å¼·åŒ–ã€æ—¥è‹±ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åé›†
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import argparse
import yaml

class GitLFSDatasetCollector:
    """Git LFSã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼"""

    def __init__(self, output_dir: str = "D:/webdataset/datasets/gitlfs", max_total_size_gb: float = 10.0):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_total_size_gb = max_total_size_gb  # ç·ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆGBï¼‰
        self.current_total_size = 0  # ç¾åœ¨ã®ç·ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰

        # PPOå†…éƒ¨æ¨è«–å¼·åŒ–ã«ç‰¹åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10GBä»¥å†…ã«åã¾ã‚‹ã‚ˆã†åˆ¶é™ï¼‰
        self.target_datasets = {
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰
            'coco_captions': {
                'hf_repo': 'HuggingFaceM4/COCO',
                'domain': 'multimodal_vision_language',
                'language': 'en',
                'license': 'cc-by-4.0',
                'estimated_size_gb': 2.5,  # æ¨å®šã‚µã‚¤ã‚º
                'max_samples': 5000,  # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
                'description': 'COCO captions for vision-language PPO training'
            },

            # æ—¥è‹±ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå†…éƒ¨æ¨è«–å¼·åŒ–ç”¨ï¼‰
            'wikipedia_ja': {
                'hf_repo': 'wikimedia/wikipedia',
                'config': '20231101.ja',
                'domain': 'multilingual_knowledge',
                'language': 'ja',
                'license': 'cc-by-sa-4.0',
                'estimated_size_gb': 1.2,
                'max_samples': 10000,
                'description': 'Japanese Wikipedia for multilingual reasoning'
            },
            'wikipedia_en': {
                'hf_repo': 'wikimedia/wikipedia',
                'config': '20231101.en',
                'domain': 'multilingual_knowledge',
                'language': 'en',
                'license': 'cc-by-sa-4.0',
                'estimated_size_gb': 1.5,
                'max_samples': 10000,
                'description': 'English Wikipedia for knowledge reasoning'
            },

            # PPOå†…éƒ¨æ¨è«–å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆPhD/Fields Medalç´šï¼‰
            'math_qa': {
                'hf_repo': 'math_qa',
                'domain': 'advanced_mathematical_reasoning',
                'language': 'en',
                'license': 'mit',
                'estimated_size_gb': 0.05,
                'max_samples': 10000,
                'description': 'Advanced Math QA for PhD-level mathematical reasoning'
            },
            'strategy_qa': {
                'hf_repo': 'ChilleD/StrategyQA',
                'domain': 'heuristic_reasoning',
                'language': 'en',
                'license': 'mit',
                'estimated_size_gb': 0.02,
                'max_samples': 5000,
                'description': 'Strategy QA for heuristic and meta-reasoning'
            },
            'hotpot_qa': {
                'hf_repo': 'hotpot_qa',
                'domain': 'multi_hop_scientific_reasoning',
                'language': 'en',
                'license': 'cc-by-sa-4.0',
                'estimated_size_gb': 0.1,
                'max_samples': 5000,
                'description': 'Multi-hop QA for scientific inference chaining'
            },
            'gsm8k': {
                'hf_repo': 'gsm8k',
                'domain': 'mathematical_proof_reasoning',
                'language': 'en',
                'license': 'mit',
                'estimated_size_gb': 0.01,
                'max_samples': 2000,
                'description': 'GSM8K for mathematical proof and theorem proving'
            },
            'math_science_qa': {
                'hf_repo': 'allenai/math_science_qa',
                'domain': 'interdisciplinary_science_reasoning',
                'language': 'en',
                'license': 'apache-2.0',
                'estimated_size_gb': 0.05,
                'max_samples': 3000,
                'description': 'Interdisciplinary math-science QA for Nobel-level insights'
            },
            'theorem_proving': {
                'hf_repo': 'ChilleD/TheoremQA',
                'domain': 'mathematical_theorem_proving',
                'language': 'en',
                'license': 'mit',
                'estimated_size_gb': 0.03,
                'max_samples': 2000,
                'description': 'Theorem proving for Fields Medal level mathematics'
            },
            'molecular_biology_qa': {
                'hf_repo': 'stanford-crfm/moleculeqa',
                'domain': 'molecular_biology_reasoning',
                'language': 'en',
                'license': 'apache-2.0',
                'estimated_size_gb': 0.02,
                'max_samples': 1500,
                'description': 'Molecular biology QA for Nobel Prize level research'
            },
            'physics_reasoning': {
                'hf_repo': 'allenai/physics_qa',
                'domain': 'theoretical_physics_reasoning',
                'language': 'en',
                'license': 'apache-2.0',
                'estimated_size_gb': 0.03,
                'max_samples': 2000,
                'description': 'Physics QA for theoretical breakthroughs'
            },

            # NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå®‰å…¨åˆ¤å®šå­¦ç¿’ç”¨ã®ã¿ã€ã‚µã‚¤ã‚ºåˆ¶é™å³ã—ãï¼‰
            'civil_comments': {
                'hf_repo': 'civil_comments',
                'domain': 'toxicity_detection',
                'language': 'en',
                'license': 'cc-by-4.0',
                'estimated_size_gb': 0.8,
                'max_samples': 5000,
                'description': 'Toxicity detection for safety PPO training',
                'nsfw_warning': True,
                'safety_only': True  # å®‰å…¨åˆ¤å®šå­¦ç¿’å°‚ç”¨
            },

            # è¿½åŠ ã®æ¨è«–å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            'commonsense_qa': {
                'hf_repo': 'commonsense_qa',
                'domain': 'commonsense_reasoning',
                'language': 'en',
                'license': 'unknown',
                'estimated_size_gb': 0.02,
                'max_samples': 2000,
                'description': 'Commonsense QA for everyday reasoning'
            },
            'social_iqa': {
                'hf_repo': 'social_i_qa',
                'domain': 'social_reasoning',
                'language': 'en',
                'license': 'unknown',
                'estimated_size_gb': 0.03,
                'max_samples': 2000,
                'description': 'Social IQ for social reasoning PPO'
            }
        }

    def check_git_lfs_setup(self) -> bool:
        """Git LFSã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ç¢ºèª"""
        try:
            result = subprocess.run(['git', 'lfs', 'version'],
                                  capture_output=True, text=True, check=True)
            print(f"[GIT LFS] Version: {result.stdout.strip()}")

            # Git LFSã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°è¨­å®šã‚’ç¢ºèª
            result = subprocess.run(['git', 'lfs', 'track'],
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("[GIT LFS] Tracking patterns:")
                print(result.stdout)

            return True
        except subprocess.CalledProcessError as e:
            print(f"[GIT LFS] Error: {e}")
            return False
        except FileNotFoundError:
            print("[GIT LFS] Git LFS not found. Please install it first.")
            return False

    def download_dataset_with_lfs(self, dataset_name: str, config: Dict[str, Any]) -> bool:
        """Git LFSã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        print(f"[DOWNLOAD] Starting download of {dataset_name}...")

        repo_url = f"https://huggingface.co/datasets/{config['hf_repo']}"
        local_path = self.output_dir / dataset_name

        try:
            # Git LFSã§ã®ã‚¯ãƒ­ãƒ¼ãƒ³
            if local_path.exists():
                print(f"[DOWNLOAD] Dataset {dataset_name} already exists, updating...")
                # æ—¢å­˜ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’æ›´æ–°
                os.chdir(local_path)
                subprocess.run(['git', 'pull'], check=True)
                subprocess.run(['git', 'lfs', 'pull'], check=True)
            else:
                print(f"[DOWNLOAD] Cloning {repo_url}...")
                subprocess.run([
                    'git', 'lfs', 'clone', repo_url, str(local_path)
                ], check=True)

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã®ä¿å­˜
            info_file = local_path / 'dataset_info.json'
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'name': dataset_name,
                    'config': config,
                    'download_time': datetime.now().isoformat(),
                    'local_path': str(local_path)
                }, f, indent=2, ensure_ascii=False)

            print(f"[DOWNLOAD] Successfully downloaded {dataset_name} to {local_path}")
            return True

        except subprocess.CalledProcessError as e:
            print(f"[DOWNLOAD] Failed to download {dataset_name}: {e}")
            return False
        except Exception as e:
            print(f"[DOWNLOAD] Unexpected error for {dataset_name}: {e}")
            return False

    def download_dataset_with_hf_hub(self, dataset_name: str, config: Dict[str, Any]) -> bool:
        """HuggingFace Hub APIã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGit LFSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        print(f"[HF HUB] Downloading {dataset_name} via API...")

        try:
            from huggingface_hub import snapshot_download
            import datasets

            local_path = self.output_dir / f"{dataset_name}_api"
            local_path.mkdir(exist_ok=True)

            # datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            try:
                print(f"[HF HUB] Using datasets library for {dataset_name}")

                # configãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                if 'config' in config:
                    dataset = datasets.load_dataset(
                        config['hf_repo'],
                        config['config'],
                        split='train',
                        streaming=True
                    )
                else:
                    dataset = datasets.load_dataset(
                        config['hf_repo'],
                        split='train',
                        streaming=True
                    )

                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                sample_count = 0
                max_samples = 1000  # ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™

                data_file = local_path / 'samples.jsonl'
                with open(data_file, 'w', encoding='utf-8') as f:
                    for sample in dataset:
                        if sample_count >= max_samples:
                            break

                        json.dump(sample, f, ensure_ascii=False)
                        f.write('\n')
                        sample_count += 1

                print(f"[HF HUB] Downloaded {sample_count} samples using datasets library")

            except Exception as ds_error:
                print(f"[HF HUB] Datasets library failed: {ds_error}")
                print("[HF HUB] Falling back to snapshot download...")

                # snapshot_downloadã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                downloaded_path = snapshot_download(
                    repo_id=config['hf_repo'],
                    repo_type="dataset",
                    local_dir=str(local_path),
                    allow_patterns=["*.json", "*.jsonl", "*.parquet", "*.txt", "*.csv"]
                )
                print(f"[HF HUB] Downloaded via snapshot to {downloaded_path}")

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã®ä¿å­˜
            info_file = local_path / 'dataset_info.json'
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'name': dataset_name,
                    'config': config,
                    'download_time': datetime.now().isoformat(),
                    'local_path': str(local_path),
                    'download_method': 'hf_hub_api'
                }, f, indent=2, ensure_ascii=False)

            print(f"[HF HUB] Successfully downloaded {dataset_name} to {local_path}")
            return True

        except Exception as e:
            print(f"[HF HUB] Failed to download {dataset_name}: {e}")
            return False

    def check_size_limit(self, dataset_name: str, config: Dict[str, Any]) -> bool:
        """ã‚µã‚¤ã‚ºåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯"""
        estimated_size = config.get('estimated_size_gb', 0)
        projected_total = self.current_total_size + (estimated_size * 1024**3)  # GB to bytes

        if projected_total > (self.max_total_size_gb * 1024**3):
            print(f"[SIZE] Skipping {dataset_name}: would exceed {self.max_total_size_gb}GB limit")
            print(f"  Current: {self.current_total_size / 1024**3:.2f}GB")
            print(f"  Projected: {projected_total / 1024**3:.2f}GB")
            return False

        return True

    def collect_all_datasets(self, use_git_lfs: bool = True) -> Dict[str, Any]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åé›†ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰"""
        print(f"Starting dataset collection with Git LFS (max {self.max_total_size_gb}GB)...")
        print("=" * 60)

        # Git LFSã®ç¢ºèª
        if use_git_lfs and not self.check_git_lfs_setup():
            print("[WARNING] Git LFS not properly configured, falling back to HF Hub API")
            use_git_lfs = False

        results = {
            'total_datasets': len(self.target_datasets),
            'successful_downloads': 0,
            'failed_downloads': 0,
            'skipped_datasets': 0,
            'total_size_gb': 0.0,
            'results': {},
            'collection_time': datetime.now().isoformat(),
            'size_limit_gb': self.max_total_size_gb
        }

        for dataset_name, config in self.target_datasets.items():
            print(f"\n[COLLECTION] Processing {dataset_name}...")
            print(f"  Description: {config['description']}")
            print(f"  Domain: {config['domain']}")
            print(f"  Estimated Size: {config.get('estimated_size_gb', 'unknown')}GB")

            if config.get('nsfw_warning'):
                print("  âš ï¸  WARNING: This dataset contains NSFW content (for safety training only)")
                if config.get('safety_only'):
                    print("  ğŸ”’ SAFETY ONLY: This dataset is for safety PPO training exclusively")

            # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not self.check_size_limit(dataset_name, config):
                results['skipped_datasets'] += 1
                results['results'][dataset_name] = {
                    'success': False,
                    'skipped': True,
                    'reason': 'size_limit_exceeded',
                    'config': config
                }
                continue

            success = False
            if use_git_lfs:
                success = self.download_dataset_with_lfs(dataset_name, config)

            if not success:
                print(f"  [FALLBACK] Trying HF Hub API for {dataset_name}")
                success = self.download_dataset_with_hf_hub(dataset_name, config)

            results['results'][dataset_name] = {
                'success': success,
                'config': config,
                'method': 'git_lfs' if success and use_git_lfs else 'hf_hub_api'
            }

            if success:
                # ã‚µã‚¤ã‚ºã‚’åŠ ç®—
                size_gb = config.get('estimated_size_gb', 0)
                self.current_total_size += size_gb * 1024**3
                results['total_size_gb'] += size_gb
                results['successful_downloads'] += 1

                print(f"  âœ… Downloaded successfully. Total size: {results['total_size_gb']:.2f}GB")
            else:
                results['failed_downloads'] += 1
                print(f"  âŒ Download failed")

        # çµæœã®ä¿å­˜
        results_file = self.output_dir / 'collection_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        print("\n" + "=" * 60)
        print("COLLECTION SUMMARY:")
        print(f"Total datasets: {results['total_datasets']}")
        print(f"Successful: {results['successful_downloads']}")
        print(f"Failed: {results['failed_downloads']}")
        print(f"Skipped (size limit): {results['skipped_datasets']}")
        print(f"Total size: {results['total_size_gb']:.2f}GB / {self.max_total_size_gb}GB")
        print(f"Results saved to: {results_file}")

        return results

    def validate_downloaded_datasets(self) -> Dict[str, Any]:
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼"""
        print("Validating downloaded datasets...")

        validation_results = {}

        for item in self.output_dir.iterdir():
            if item.is_dir() and (item / 'dataset_info.json').exists():
                dataset_name = item.name.replace('_api', '')

                try:
                    with open(item / 'dataset_info.json', 'r', encoding='utf-8') as f:
                        info = json.load(f)

                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®è¨ˆç®—
                    total_size = 0
                    file_count = 0

                    for file_path in item.rglob('*'):
                        if file_path.is_file() and file_path.name != 'dataset_info.json':
                            total_size += file_path.stat().st_size
                            file_count += 1

                    validation_results[dataset_name] = {
                        'valid': True,
                        'path': str(item),
                        'size_bytes': total_size,
                        'file_count': file_count,
                        'info': info
                    }

                except Exception as e:
                    validation_results[dataset_name] = {
                        'valid': False,
                        'error': str(e)
                    }

        # æ¤œè¨¼çµæœã®ä¿å­˜
        validation_file = self.output_dir / 'validation_results.json'
        with open(validation_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"Validation results saved to: {validation_file}")
        return validation_results

    def convert_for_ppo_training(self) -> Dict[str, Any]:
        """PPOå­¦ç¿’ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›"""
        print("Converting datasets for PPO training...")

        ppo_data_dir = self.output_dir / 'ppo_training_data'
        ppo_data_dir.mkdir(exist_ok=True)

        conversion_results = {
            'converted_datasets': 0,
            'total_samples': 0,
            'ppo_ready_files': []
        }

        for dataset_dir in self.output_dir.iterdir():
            if not dataset_dir.is_dir() or dataset_dir.name in ['ppo_training_data']:
                continue

            dataset_name = dataset_dir.name.replace('_api', '')

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±èª­ã¿è¾¼ã¿
            info_file = dataset_dir / 'dataset_info.json'
            if not info_file.exists():
                continue

            with open(info_file, 'r', encoding='utf-8') as f:
                info = json.load(f)

            config = info['config']

            # PPOå­¦ç¿’ç”¨ã®å¤‰æ›
            print(f"Converting {dataset_name} for PPO training...")

            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®åé›†
            samples_file = dataset_dir / 'samples.jsonl'
            if samples_file.exists():
                samples = []
                with open(samples_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            samples.append(json.loads(line))

                # PPOå½¢å¼ã«å¤‰æ›
                ppo_samples = self._convert_samples_to_ppo_format(samples, config)

                # ä¿å­˜
                output_file = ppo_data_dir / f"{dataset_name}_ppo.jsonl"
                with open(output_file, 'w', encoding='utf-8') as f:
                    for sample in ppo_samples:
                        json.dump(sample, f, ensure_ascii=False)
                        f.write('\n')

                conversion_results['converted_datasets'] += 1
                conversion_results['total_samples'] += len(ppo_samples)
                conversion_results['ppo_ready_files'].append(str(output_file))

                print(f"  Converted {len(ppo_samples)} samples for PPO training")

        # å¤‰æ›çµæœã®ä¿å­˜
        conversion_file = ppo_data_dir / 'conversion_results.json'
        with open(conversion_file, 'w', encoding='utf-8') as f:
            json.dump(conversion_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"PPO conversion completed: {conversion_results['converted_datasets']} datasets, {conversion_results['total_samples']} samples")
        return conversion_results

    def _convert_samples_to_ppo_format(self, samples: List[Dict], config: Dict) -> List[Dict]:
        """ã‚µãƒ³ãƒ—ãƒ«ã‚’PPOå­¦ç¿’ç”¨å½¢å¼ã«å¤‰æ›"""
        ppo_samples = []
        domain = config['domain']
        language = config['language']

        for sample in samples:
            # PPOå­¦ç¿’ç”¨ã®åŸºæœ¬æ§‹é€ 
            ppo_sample = {
                'input': '',
                'output': '',
                'domain': domain,
                'language': language,
                'ppo_metadata': {
                    'task_type': self._get_ppo_task_type(domain),
                    'difficulty': 'medium',
                    'requires_reasoning': True,
                    'multimodal': 'multimodal' in domain,
                    'safety_related': config.get('safety_only', False)
                }
            }

            # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ã®å¤‰æ›
            if domain == 'multimodal_vision_language':
                # COCOã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å½¢å¼
                if 'caption' in sample:
                    ppo_sample['input'] = "Describe this image:"
                    ppo_sample['output'] = sample['caption']
                elif 'question' in sample and 'answer' in sample:
                    ppo_sample['input'] = sample['question']
                    ppo_sample['output'] = sample['answer']

            elif 'qa' in domain or 'reasoning' in domain:
                # QAå½¢å¼
                if 'question' in sample and 'answer' in sample:
                    ppo_sample['input'] = sample['question']
                    ppo_sample['output'] = sample['answer']
                elif 'query' in sample and 'response' in sample:
                    ppo_sample['input'] = sample['query']
                    ppo_sample['output'] = sample['response']

            elif domain == 'toxicity_detection':
                # æ¯’æ€§æ¤œå‡ºå½¢å¼ï¼ˆå®‰å…¨å­¦ç¿’ç”¨ï¼‰
                if 'text' in sample:
                    ppo_sample['input'] = sample['text']
                    ppo_sample['output'] = "SAFE" if not sample.get('toxic', False) else "UNSAFE"
                    ppo_sample['ppo_metadata']['task_type'] = 'safety_classification'

            elif 'knowledge' in domain or 'text' in domain:
                # çŸ¥è­˜/ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼
                if 'text' in sample:
                    ppo_sample['input'] = sample['text'][:500]  # åˆ¶é™
                    ppo_sample['output'] = "Understood"  # åŸºæœ¬çš„ãªå¿œç­”
                    ppo_sample['ppo_metadata']['task_type'] = 'knowledge_comprehension'

            # æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
            max_samples = config.get('max_samples', 10000)
            if len(ppo_samples) >= max_samples:
                break

            if ppo_sample['input'] and ppo_sample['output']:
                ppo_samples.append(ppo_sample)

        return ppo_samples

    def _get_ppo_task_type(self, domain: str) -> str:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰PPOã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šï¼ˆPhD/Fields Medalç´šï¼‰"""
        task_mapping = {
            # é«˜åº¦ãªæ•°å­¦ãƒ»ç§‘å­¦æ¨è«–
            'advanced_mathematical_reasoning': 'phd_mathematics',
            'mathematical_proof_reasoning': 'theorem_proving',
            'mathematical_theorem_proving': 'fields_medal_mathematics',
            'interdisciplinary_science_reasoning': 'nobel_science',
            'molecular_biology_reasoning': 'molecular_biology_phd',
            'theoretical_physics_reasoning': 'physics_nobel',

            # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒ¡ã‚¿æ¨è«–
            'heuristic_reasoning': 'heuristic_meta_reasoning',
            'multi_hop_scientific_reasoning': 'scientific_inference_chaining',
            'strategic_reasoning': 'strategic_meta_cognition',

            # çŸ¥è¦šãƒ»èªçŸ¥å±¤
            'multimodal_vision_language': 'so8_perception_cognition',
            'commonsense_reasoning': 'intuitive_reasoning',
            'social_reasoning': 'social_intuition',

            # å®‰å…¨ãƒ»å€«ç†å±¤
            'toxicity_detection': 'safety_meta_reasoning',

            # è¨€èªãƒ»çŸ¥è­˜å±¤
            'multilingual_knowledge': 'cross_lingual_intuition',
            'multilingual_qa': 'isomorphic_reasoning'
        }
        return task_mapping.get(domain, 'general_reasoning')

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="Git LFS HuggingFace Dataset Collection for PPO Training")
    parser.add_argument('--output-dir', default='D:/webdataset/datasets/gitlfs_10gb',
                       help='Output directory for datasets')
    parser.add_argument('--max-size-gb', type=float, default=10.0,
                       help='Maximum total size in GB (default: 10.0)')
    parser.add_argument('--use-git-lfs', action='store_true', default=True,
                       help='Use Git LFS for downloading (default: True)')
    parser.add_argument('--validate-only', action='store_true',
                       help='Only validate existing datasets')
    parser.add_argument('--convert-ppo-only', action='store_true',
                       help='Only convert existing datasets for PPO training')
    parser.add_argument('--datasets', nargs='+',
                       help='Specific datasets to download (default: all)')

    args = parser.parse_args()

    collector = GitLFSDatasetCollector(args.output_dir, args.max_size_gb)

    if args.convert_ppo_only:
        # PPOå¤‰æ›ã®ã¿
        results = collector.convert_for_ppo_training()
        print(f"PPO conversion completed: {results['converted_datasets']} datasets")
    elif args.validate_only:
        # æ¤œè¨¼ã®ã¿
        results = collector.validate_downloaded_datasets()
        print(f"Validated {len(results)} datasets")
    else:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåé›†
        if args.datasets:
            # æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿
            filtered_datasets = {k: v for k, v in collector.target_datasets.items()
                               if k in args.datasets}
            collector.target_datasets = filtered_datasets

        results = collector.collect_all_datasets(args.use_git_lfs)

        if results['successful_downloads'] > 0:
            # åé›†å¾Œã«æ¤œè¨¼
            validation_results = collector.validate_downloaded_datasets()

            # PPOå¤‰æ›
            print("\nConverting datasets for PPO training...")
            ppo_results = collector.convert_for_ppo_training()

            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
            print("\n" + "=" * 60)
            print("FINAL REPORT:")
            print(f"Collection: {results['successful_downloads']}/{results['total_datasets']} successful")
            print(f"Total Size: {results['total_size_gb']:.2f}GB / {args.max_size_gb}GB")
            print(f"Validation: {sum(1 for r in validation_results.values() if r.get('valid'))} valid datasets")
            print(f"PPO Conversion: {ppo_results['converted_datasets']} datasets, {ppo_results['total_samples']} samples")

            # NSFWãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è­¦å‘Š
            nsfw_datasets = [name for name, config in collector.target_datasets.items()
                           if config.get('nsfw_warning')]
            if nsfw_datasets:
                print("\nâš ï¸  NSFW DATASETS WARNING:")
                print("The following datasets contain sensitive content for safety training only:")
                for name in nsfw_datasets:
                    print(f"  - {name}: {collector.target_datasets[name]['description']}")
                print("These datasets should ONLY be used for safety PPO training and content filtering.")

if __name__ == '__main__':
    main()
