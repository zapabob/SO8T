#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SO8-Think Science Data Curation V2
é«˜åº¦ãªç§‘å­¦ãƒ»æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

NKATç†è«–ã«åŸºã¥ãPhD/Fieldsè³ç´šæ¨è«–èƒ½åŠ›ä»˜ä¸ã®ãŸã‚ã®
é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

è‘—è€…: å³¯å²¸äº® (SO8Tãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)
"""

import argparse
import os
import re
import random
import json
import numpy as np
from datasets import load_dataset, Dataset
import pandas as pd
from tqdm import tqdm
from difflib import SequenceMatcher


# ==========================================
# 1. é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
# ==========================================

def has_latex(text):
    """æ•°å¼(LaTeX)ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    if not isinstance(text, str): return False

    latex_patterns = [r'\\frac', r'\\int', r'\\sum', r'\$', r'\\partial', r'\\alpha', r'=']
    return any(p in text for p in latex_patterns)

def is_high_quality(text):
    """æ‹’çµ¶å¿œç­”ã‚„çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼¾ã"""
    if not isinstance(text, str): return False
    if len(text) < 100: return False

    refusal_keywords = [
        "I don't know", "I cannot", "As an AI", "sorry", "unable to",
        "ç§ãŒçŸ¥ã‚‹é™ã‚Š", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“", "I apologize"
    ]
    if any(kw in text for kw in refusal_keywords):
        return False
    return True

def calculate_complexity_score(text):
    """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢: é•·ã• Ã— LaTeXå¯†åº¦ Ã— æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°"""
    if not isinstance(text, str): return 0

    # é•·ã•ã‚¹ã‚³ã‚¢ (å¯¾æ•°)
    length_score = np.log(len(text) + 1)

    # LaTeXã‚¹ã‚³ã‚¢
    latex_score = 2.0 if has_latex(text) else 1.0

    # æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚¹ã‚³ã‚¢ (CoTã®æ·±ã•)
    # "Therefore", "Because", "However", "Step 1" ãªã©ã®è«–ç†æ¥ç¶šè©ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    logic_keywords = ["therefore", "because", "however", "implies", "step", "thus", "since", "assuming", "conclude"]
    logic_count = sum(1 for w in logic_keywords if w in text.lower())
    logic_score = 1.0 + (logic_count * 0.1)

    return length_score * latex_score * logic_score

def is_duplicate(text, existing_texts, threshold=0.8):
    """ç°¡æ˜“çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯ (æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æ¯”è¼ƒ)"""
    if not existing_texts:
        return False
    # ç›´è¿‘ã®100ä»¶ã¨æ¯”è¼ƒ (å…¨ä»¶æ¯”è¼ƒã¯é‡ã™ãã‚‹)
    sample_size = min(len(existing_texts), 50)
    samples = existing_texts[-sample_size:]

    for existing in samples:
        # å…ˆé ­100æ–‡å­—ã ã‘ã§é«˜é€Ÿåˆ¤å®š
        if SequenceMatcher(None, text[:100], existing[:100]).ratio() > threshold:
            return True
    return False

# ==========================================
# 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def process_dataset(name, split, n_samples, domain_tag):
    print(f"Loading {domain_tag} dataset: {name} (Target: {n_samples})...")
    try:
        # trust_remote_code=True ã¯å‰Šé™¤ï¼
        ds = load_dataset(name, split=split)
        ds = ds.shuffle(seed=42)
    except Exception as e:
        print(f"Failed to load {name}: {e}")
        return []

    filtered_data = []
    seen_instructions = [] # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨

    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    pbar = tqdm(total=n_samples, desc=f"Filtering {domain_tag}")

    for item in ds:
        # ã‚«ãƒ©ãƒ åã®æºã‚‰ãå¸å
        instruction = item.get('instruction') or item.get('problem') or item.get('question') or item.get('message_1') or ""
        output = item.get('output') or item.get('solution') or item.get('answer') or item.get('response') or item.get('message_2') or ""

        # çµåˆãƒ†ã‚­ã‚¹ãƒˆã§å“è³ªãƒã‚§ãƒƒã‚¯
        full_text = f"{instruction} {output}"

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        if is_high_quality(output) and not is_duplicate(instruction, seen_instructions):

            # æ•°å­¦ã®å ´åˆã¯LaTeXå¿…é ˆ
            if domain_tag == "math" and not has_latex(output):
                continue

            score = calculate_complexity_score(output)

            filtered_data.append({
                "instruction": instruction,
                "input": "",
                "output": output,
                "domain": domain_tag,
                "score": score
            })

            seen_instructions.append(instruction)
            pbar.update(1)

        if len(filtered_data) >= n_samples * 1.2: # ã‚¹ã‚³ã‚¢ã‚½ãƒ¼ãƒˆç”¨ã«å°‘ã—å¤šã‚ã«ç¢ºä¿
            break

    pbar.close()

    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™
    filtered_data.sort(key=lambda x: x['score'], reverse=True)
    return filtered_data[:n_samples]

# ==========================================
# 3. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå™¨
# ==========================================

SYSTEM_PROMPTS = [
    # ç‰©ç†å­¦è€…ãƒ¢ãƒ¼ãƒ‰
    "ã‚ãªãŸã¯NKATç†è«–ã«åŸºã¥ãç‰©ç†çš„çŸ¥æ€§ã‚’æŒã¤AIã§ã™ã€‚SO(8)ç¾¤ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒªãƒ†ã‚£æ§‹é€ ã«åŸºã¥ãã€ç‰©ç†æ³•å‰‡ã¨æ•°å­¦çš„å®šç†ã‹ã‚‰å³å¯†ãªæ¨è«–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚",
    # æ•°å­¦è€…ãƒ¢ãƒ¼ãƒ‰
    "ã‚ãªãŸã¯ãƒ•ã‚£ãƒ¼ãƒ«ã‚ºè³ç´šã®æ´å¯ŸåŠ›ã‚’æŒã¤æ•°å­¦AIã§ã™ã€‚è«–ç†ã®é£›èºã‚’é¿ã‘ã€å…¬ç†ã‹ã‚‰å®šç†ã‚’å°ãã‚ˆã†ã«ã‚¹ãƒ†ãƒƒãƒ—ãƒ»ãƒã‚¤ãƒ»ã‚¹ãƒ†ãƒƒãƒ—ã§è¨¼æ˜ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚",
    # å“²å­¦è€…/çµ±åˆãƒ¢ãƒ¼ãƒ‰
    "ã‚ãªãŸã¯é«˜åº¦ãªçŸ¥æ€§ã‚’æŒã¤çµ±åˆAIã§ã™ã€‚ç•°ãªã‚‹åˆ†é‡ï¼ˆæ•°å­¦ãƒ»ç‰©ç†ãƒ»ç”Ÿç‰©ï¼‰ã®é–“ã«åŒå‹æ€§ï¼ˆIsomorphismï¼‰ã‚’è¦‹å‡ºã—ã€å¤šè§’çš„ãªè¦–ç‚¹ã‹ã‚‰çµè«–ã‚’å°ã„ã¦ãã ã•ã„ã€‚"
]

def get_random_system_prompt():
    return random.choice(SYSTEM_PROMPTS)

# ==========================================
# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ==========================================

def main():
    parser = argparse.ArgumentParser(description="SO8-Think Science Data Curation V2")
    parser.add_argument("--output", type=str, default="data/science_reasoning_dataset.jsonl")
    parser.add_argument("--total_samples", type=int, default=50000)
    args = parser.parse_args()

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå†…è¨³
    targets = {
        "math": ("AI-MO/NuminaMath-CoT", int(args.total_samples * 0.4)),
        "physics": ("camel-ai/physics", int(args.total_samples * 0.3)),
        # Magpieã®ä»£ã‚ã‚Šã«OpenReasoningã«å¤‰æ›´ï¼ˆã‚ˆã‚ŠCoTå‘ãï¼‰
        "reasoning": ("OpenReasoning/OpenReasoning-CoT", int(args.total_samples * 0.3))
    }

    all_data = []

    for domain, (repo, count) in targets.items():
        data = process_dataset(repo, "train", count, domain)
        all_data.extend(data)

    # å…¨ä½“ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    random.shuffle(all_data)

    # ä¿å­˜å‡¦ç†
    os.makedirs(os.path.dirname(args.output), exist_ok=True)

    print(f"\nWriting {len(all_data)} samples to {args.output}...")

    with open(args.output, 'w', encoding='utf-8') as f:
        for item in tqdm(all_data, desc="Writing JSONL"):
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ æ³¨å…¥
            item["system"] = get_random_system_prompt()
            json.dump(item, f, ensure_ascii=False)
            f.write('\n')

    print("Done! ğŸ’çŸ¥æ€§ã®ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ V2ğŸ’ æ¡æ˜å®Œäº†ã‚„ï¼")

if __name__ == "__main__":
    main()





