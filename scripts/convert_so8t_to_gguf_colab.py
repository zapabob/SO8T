#!/usr/bin/env python3
"""
SO8Tç¾¤Transformer GGUFå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (GoogleColabå¯¾å¿œ)

SO8Tç¾¤Transformerãƒ¢ãƒ‡ãƒ«ã‚’8bité‡å­åŒ–GGUFå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
GoogleColabç’°å¢ƒã§å®Ÿè¡Œå¯èƒ½ã§ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’æœ€é©åŒ–ã—ã¦ã„ã¾ã™ã€‚

ç‰¹å¾´:
- SO8Tç¾¤æ§‹é€ ã®ä¿æŒ
- 8bité‡å­åŒ–ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- GGUFå½¢å¼ã§ã®åŠ¹ç‡çš„ãªä¿å­˜
- GoogleColabç’°å¢ƒã§ã®å®Ÿè¡Œæœ€é©åŒ–
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import json
import logging
from tqdm import tqdm
import time
import gc

# GoogleColabç’°å¢ƒã®æ¤œå‡º
try:
    import google.colab
    IN_COLAB = True
    print("ğŸš€ GoogleColabç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼")
except ImportError:
    IN_COLAB = False
    print("ğŸ’» ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SO8TGGUFConverter:
    """SO8Tç¾¤Transformer GGUFå¤‰æ›å™¨"""
    
    def __init__(self, 
                 model_path: str,
                 output_dir: str = "so8t_gguf_models",
                 quantization_type: str = "Q8_0",
                 max_memory_gb: float = 8.0):
        """
        SO8T GGUFå¤‰æ›å™¨ã‚’åˆæœŸåŒ–
        
        Args:
            model_path: SO8Tãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            quantization_type: é‡å­åŒ–ã‚¿ã‚¤ãƒ— (Q8_0, Q4_K_M, Q5_K_Mç­‰)
            max_memory_gb: æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (GB)
        """
        self.model_path = Path(model_path)
        self.output_dir = Path(output_dir)
        self.quantization_type = quantization_type
        self.max_memory_gb = max_memory_gb
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # GoogleColabç’°å¢ƒã§ã®æœ€é©åŒ–
        if IN_COLAB:
            self._setup_colab_environment()
        
        logger.info(f"SO8T GGUFå¤‰æ›å™¨åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {self.model_path}")
        logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        logger.info(f"é‡å­åŒ–ã‚¿ã‚¤ãƒ—: {self.quantization_type}")
    
    def _setup_colab_environment(self):
        """GoogleColabç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger.info("GoogleColabç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # ãƒ¡ãƒ¢ãƒªãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆå¯¾ç­–
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        
        # ä¸è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç„¡åŠ¹åŒ–
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
        
        logger.info("GoogleColabç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    
    def load_so8t_model(self) -> Dict[str, torch.Tensor]:
        """SO8Tãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("SO8Tãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹è¾æ›¸ã‚’èª­ã¿è¾¼ã¿
            if self.model_path.is_file() and self.model_path.suffix == '.pth':
                # PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
                state_dict = torch.load(self.model_path, map_location='cpu')
                logger.info("PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿å®Œäº†")
            elif self.model_path.is_dir():
                # HuggingFaceå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                model_file = self.model_path / "pytorch_model.bin"
                if model_file.exists():
                    state_dict = torch.load(model_file, map_location='cpu')
                else:
                    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                    state_dict = self._load_huggingface_model()
                logger.info("HuggingFaceå½¢å¼ã‹ã‚‰èª­ã¿è¾¼ã¿å®Œäº†")
            else:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«å½¢å¼: {self.model_path}")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
            self._check_memory_usage(state_dict)
            
            return state_dict
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _load_huggingface_model(self) -> Dict[str, torch.Tensor]:
        """HuggingFaceå½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("HuggingFaceå½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        state_dict = {}
        model_files = list(self.model_path.glob("pytorch_model*.bin"))
        
        if not model_files:
            raise FileNotFoundError("HuggingFaceå½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †æ¬¡èª­ã¿è¾¼ã¿
        for model_file in sorted(model_files):
            logger.info(f"èª­ã¿è¾¼ã¿ä¸­: {model_file.name}")
            file_state_dict = torch.load(model_file, map_location='cpu')
            state_dict.update(file_state_dict)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            del file_state_dict
            gc.collect()
        
        return state_dict
    
    def _check_memory_usage(self, state_dict: Dict[str, torch.Tensor]):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯"""
        total_params = sum(p.numel() for p in state_dict.values() if isinstance(p, torch.Tensor))
        total_size_gb = sum(p.numel() * p.element_size() for p in state_dict.values() if isinstance(p, torch.Tensor)) / (1024**3)
        
        logger.info(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {total_size_gb:.2f} GB")
        
        if total_size_gb > self.max_memory_gb:
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒæœ€å¤§ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è¶…é: {total_size_gb:.2f} GB > {self.max_memory_gb} GB")
            if IN_COLAB:
                logger.warning("GoogleColabç’°å¢ƒã§ã¯ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    
    def analyze_so8t_structure(self, state_dict: Dict[str, torch.Tensor]) -> Dict[str, any]:
        """SO8Tç¾¤æ§‹é€ ã‚’åˆ†æ"""
        logger.info("SO8Tç¾¤æ§‹é€ ã‚’åˆ†æä¸­...")
        
        analysis = {
            'total_layers': 0,
            'so8t_layers': 0,
            'attention_layers': 0,
            'ffn_layers': 0,
            'so8_rotation_params': 0,
            'triality_heads': 0,
            'model_architecture': 'unknown'
        }
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ ã®åˆ†æ
        for key, tensor in state_dict.items():
            if isinstance(tensor, torch.Tensor):
                # SO8Tç¾¤é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                if 'so8t' in key.lower() or 'rotation' in key.lower():
                    analysis['so8t_layers'] += 1
                    if 'rotation' in key.lower():
                        analysis['so8_rotation_params'] += 1
                
                # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤
                elif 'attention' in key.lower() or 'attn' in key.lower():
                    analysis['attention_layers'] += 1
                
                # FFNå±¤
                elif 'mlp' in key.lower() or 'ffn' in key.lower() or 'feed_forward' in key.lower():
                    analysis['ffn_layers'] += 1
                
                # Triality reasoning heads
                elif any(head in key.lower() for head in ['task_head', 'safety_head', 'authority_head']):
                    analysis['triality_heads'] += 1
        
        analysis['total_layers'] = analysis['so8t_layers'] + analysis['attention_layers'] + analysis['ffn_layers']
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ¤å®š
        if analysis['so8t_layers'] > 0 and analysis['triality_heads'] > 0:
            analysis['model_architecture'] = 'SO8TTransformerForCausalLM'
        elif analysis['so8t_layers'] > 0:
            analysis['model_architecture'] = 'SO8TTransformerModel'
        else:
            analysis['model_architecture'] = 'StandardTransformer'
        
        logger.info(f"SO8Tç¾¤æ§‹é€ åˆ†æå®Œäº†:")
        logger.info(f"  - ç·ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {analysis['total_layers']}")
        logger.info(f"  - SO8Tç¾¤ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {analysis['so8t_layers']}")
        logger.info(f"  - ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤æ•°: {analysis['attention_layers']}")
        logger.info(f"  - FFNå±¤æ•°: {analysis['ffn_layers']}")
        logger.info(f"  - SO8å›è»¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {analysis['so8_rotation_params']}")
        logger.info(f"  - Triality headsæ•°: {analysis['triality_heads']}")
        logger.info(f"  - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {analysis['model_architecture']}")
        
        return analysis
    
    def quantize_tensor(self, tensor: torch.Tensor, quantization_type: str) -> Tuple[torch.Tensor, Dict]:
        """ãƒ†ãƒ³ã‚½ãƒ«ã‚’é‡å­åŒ–"""
        if not isinstance(tensor, torch.Tensor):
            return tensor, {}
        
        original_dtype = tensor.dtype
        original_shape = tensor.shape
        
        # é‡å­åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
        if quantization_type == "Q8_0":
            # 8bité‡å­åŒ– (Q8_0)
            if tensor.dtype == torch.float32:
                # float32 -> int8
                scale = tensor.abs().max() / 127.0
                quantized = torch.round(tensor / scale).clamp(-128, 127).to(torch.int8)
                metadata = {
                    'scale': scale.item(),
                    'zero_point': 0,
                    'original_dtype': str(original_dtype),
                    'quantization_type': 'Q8_0'
                }
            else:
                quantized = tensor
                metadata = {'quantization_type': 'none'}
        
        elif quantization_type == "Q4_K_M":
            # 4bité‡å­åŒ– (Q4_K_M)
            if tensor.dtype == torch.float32:
                # float32 -> int4
                scale = tensor.abs().max() / 7.0
                quantized = torch.round(tensor / scale).clamp(-8, 7).to(torch.int8)
                metadata = {
                    'scale': scale.item(),
                    'zero_point': 0,
                    'original_dtype': str(original_dtype),
                    'quantization_type': 'Q4_K_M'
                }
            else:
                quantized = tensor
                metadata = {'quantization_type': 'none'}
        
        else:
            # é‡å­åŒ–ãªã—
            quantized = tensor
            metadata = {'quantization_type': 'none'}
        
        return quantized, metadata
    
    def convert_to_gguf_format(self, state_dict: Dict[str, torch.Tensor], analysis: Dict) -> Dict:
        """GGUFå½¢å¼ã«å¤‰æ›"""
        logger.info("GGUFå½¢å¼ã«å¤‰æ›ä¸­...")
        
        gguf_data = {
            'metadata': {
                'model_type': 'SO8TTransformer',
                'architecture': analysis['model_architecture'],
                'quantization_type': self.quantization_type,
                'total_layers': analysis['total_layers'],
                'so8t_layers': analysis['so8t_layers'],
                'attention_layers': analysis['attention_layers'],
                'ffn_layers': analysis['ffn_layers'],
                'so8_rotation_params': analysis['so8_rotation_params'],
                'triality_heads': analysis['triality_heads'],
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'converter': 'SO8TGGUFConverter'
            },
            'tensors': {},
            'quantization_info': {}
        }
        
        # ãƒ†ãƒ³ã‚½ãƒ«ã‚’é‡å­åŒ–ã—ã¦GGUFå½¢å¼ã«å¤‰æ›
        with tqdm(total=len(state_dict), desc="GGUFå¤‰æ›", unit="tensor") as pbar:
            for key, tensor in state_dict.items():
                if isinstance(tensor, torch.Tensor):
                    # ãƒ†ãƒ³ã‚½ãƒ«ã‚’é‡å­åŒ–
                    quantized_tensor, quant_metadata = self.quantize_tensor(tensor, self.quantization_type)
                    
                    # GGUFå½¢å¼ã§ä¿å­˜
                    gguf_data['tensors'][key] = {
                        'data': quantized_tensor.numpy().astype(np.int8) if quantized_tensor.dtype == torch.int8 else quantized_tensor.numpy(),
                        'shape': list(tensor.shape),
                        'dtype': str(quantized_tensor.dtype),
                        'original_dtype': str(tensor.dtype)
                    }
                    
                    # é‡å­åŒ–æƒ…å ±ã‚’ä¿å­˜
                    if quant_metadata:
                        gguf_data['quantization_info'][key] = quant_metadata
                
                pbar.update(1)
        
        logger.info("GGUFå½¢å¼å¤‰æ›å®Œäº†")
        return gguf_data
    
    def save_gguf_model(self, gguf_data: Dict, filename: str = None) -> str:
        """GGUFãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if filename is None:
            model_name = self.model_path.stem
            filename = f"{model_name}_so8t_{self.quantization_type}.gguf"
        
        output_path = self.output_dir / filename
        
        logger.info(f"GGUFãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {output_path}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§ä¿å­˜
        metadata_path = output_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(gguf_data['metadata'], f, indent=2, ensure_ascii=False)
        
        # ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’NPZå½¢å¼ã§ä¿å­˜
        tensor_data = {}
        for key, tensor_info in gguf_data['tensors'].items():
            tensor_data[key] = tensor_info['data']
        
        npz_path = output_path.with_suffix('.npz')
        np.savez_compressed(npz_path, **tensor_data)
        
        # é‡å­åŒ–æƒ…å ±ã‚’ä¿å­˜
        quant_path = output_path.with_suffix('.quant.json')
        with open(quant_path, 'w', encoding='utf-8') as f:
            json.dump(gguf_data['quantization_info'], f, indent=2, ensure_ascii=False)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
        total_size = sum(f.stat().st_size for f in [metadata_path, npz_path, quant_path])
        total_size_gb = total_size / (1024**3)
        
        logger.info(f"GGUFãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†:")
        logger.info(f"  - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path} ({metadata_path.stat().st_size / 1024:.1f} KB)")
        logger.info(f"  - ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿: {npz_path} ({npz_path.stat().st_size / (1024**2):.1f} MB)")
        logger.info(f"  - é‡å­åŒ–æƒ…å ±: {quant_path} ({quant_path.stat().st_size / 1024:.1f} KB)")
        logger.info(f"  - ç·ã‚µã‚¤ã‚º: {total_size_gb:.2f} GB")
        
        return str(output_path)
    
    def create_model_card(self, analysis: Dict) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        model_card = f"""# SO8Tç¾¤Transformer GGUFãƒ¢ãƒ‡ãƒ«

## æ¦‚è¦
SO8Tç¾¤Transformerãƒ¢ãƒ‡ãƒ«ã‚’8bité‡å­åŒ–GGUFå½¢å¼ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—**: {analysis['model_architecture']}
- **ç·ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°**: {analysis['total_layers']}
- **SO8Tç¾¤ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°**: {analysis['so8t_layers']}
- **ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤æ•°**: {analysis['attention_layers']}
- **FFNå±¤æ•°**: {analysis['ffn_layers']}
- **SO8å›è»¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: {analysis['so8_rotation_params']}
- **Triality headsæ•°**: {analysis['triality_heads']}

## é‡å­åŒ–
- **é‡å­åŒ–ã‚¿ã‚¤ãƒ—**: {self.quantization_type}
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: å¤§å¹…ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›
- **ç²¾åº¦**: é‡å­åŒ–ã«ã‚ˆã‚‹è»½å¾®ãªç²¾åº¦ä½ä¸‹

## SO8Tç¾¤æ§‹é€ 
SO8Tç¾¤Transformerã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™:
- **SO(8)ç¾¤å›è»¢**: 8æ¬¡å…ƒå›è»¢ç¾¤ã«ã‚ˆã‚‹éå¯æ›ã‚²ãƒ¼ãƒˆ
- **Triality reasoning**: 3ã¤ã®æ¨è«–ãƒ˜ãƒƒãƒ‰ï¼ˆtask, safety, authorityï¼‰
- **PETæ­£å‰‡åŒ–**: æ™‚ç³»åˆ—ä¸€è²«æ€§ã«ã‚ˆã‚‹ç¾¤ã®æ…£æ€§ä¿æŒ
- **å®‰å…¨äººæ ¼**: å­¦ç¿’ä¸­ã«ç¾¤æ§‹é€ ãŒå´©å£Šã—ãªã„è¨­è¨ˆ

## ä½¿ç”¨æ–¹æ³•
```python
import numpy as np
import json

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with open('model_metadata.json', 'r') as f:
    metadata = json.load(f)

# ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
tensor_data = np.load('model_tensors.npz')

# é‡å­åŒ–æƒ…å ±èª­ã¿è¾¼ã¿
with open('model_quantization.json', 'r') as f:
    quant_info = json.load(f)
```

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
- `model_metadata.json`: ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
- `model_tensors.npz`: é‡å­åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿
- `model_quantization.json`: é‡å­åŒ–æƒ…å ±

## ä½œæˆæ—¥æ™‚
{time.strftime('%Y-%m-%d %H:%M:%S')}

## å¤‰æ›å™¨
SO8TGGUFConverter v1.0
"""
        return model_card
    
    def convert(self) -> str:
        """SO8Tãƒ¢ãƒ‡ãƒ«ã‚’GGUFå½¢å¼ã«å¤‰æ›"""
        logger.info("ğŸš€ SO8Tç¾¤Transformer GGUFå¤‰æ›é–‹å§‹ï¼")
        
        try:
            # 1. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            logger.info("ğŸ“¥ ã‚¹ãƒ†ãƒƒãƒ—1: SO8Tãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿")
            state_dict = self.load_so8t_model()
            
            # 2. SO8Tç¾¤æ§‹é€ åˆ†æ
            logger.info("ğŸ” ã‚¹ãƒ†ãƒƒãƒ—2: SO8Tç¾¤æ§‹é€ åˆ†æ")
            analysis = self.analyze_so8t_structure(state_dict)
            
            # 3. GGUFå½¢å¼å¤‰æ›
            logger.info("ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—3: GGUFå½¢å¼å¤‰æ›")
            gguf_data = self.convert_to_gguf_format(state_dict, analysis)
            
            # 4. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            logger.info("ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—4: GGUFãƒ¢ãƒ‡ãƒ«ä¿å­˜")
            output_path = self.save_gguf_model(gguf_data)
            
            # 5. ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ä½œæˆ
            logger.info("ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ä½œæˆ")
            model_card = self.create_model_card(analysis)
            card_path = self.output_dir / "README.md"
            with open(card_path, 'w', encoding='utf-8') as f:
                f.write(model_card)
            
            # 6. ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            logger.info("ğŸ§¹ ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢")
            del state_dict, gguf_data
            gc.collect()
            
            logger.info("âœ… SO8Tç¾¤Transformer GGUFå¤‰æ›å®Œäº†ï¼")
            logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
            logger.info(f"ğŸ“„ ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰: {card_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SO8Tç¾¤Transformer GGUFå¤‰æ›')
    parser.add_argument('--model_path', type=str, required=True, help='SO8Tãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--output_dir', type=str, default='so8t_gguf_models', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--quantization', type=str, default='Q8_0', 
                       choices=['Q8_0', 'Q4_K_M', 'Q5_K_M', 'none'], help='é‡å­åŒ–ã‚¿ã‚¤ãƒ—')
    parser.add_argument('--max_memory', type=float, default=8.0, help='æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (GB)')
    
    args = parser.parse_args()
    
    # å¤‰æ›å™¨ä½œæˆ
    converter = SO8TGGUFConverter(
        model_path=args.model_path,
        output_dir=args.output_dir,
        quantization_type=args.quantization,
        max_memory_gb=args.max_memory
    )
    
    # å¤‰æ›å®Ÿè¡Œ
    output_path = converter.convert()
    print(f"\nğŸ‰ å¤‰æ›å®Œäº†ï¼å‡ºåŠ›: {output_path}")


if __name__ == "__main__":
    main()
