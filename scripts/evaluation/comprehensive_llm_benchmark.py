#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ…æ‹¬çš„LLMãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ 
Comprehensive LLM Benchmark System

è¤‡æ•°ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’çµ±åˆã—ãŸåŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼š
- lm-evaluation-harness (EleutherAI)
- LightEval (HuggingFace)
- OpenCompass
- transformers benchmark utilities
- vLLM benchmark

HFæå‡ºå¯èƒ½ãªçµ±è¨ˆå‡¦ç†ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãã‚°ãƒ©ãƒ•ã€è¦ç´„çµ±è¨ˆé‡ï¼‰
"""

import os
import sys
import json
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import ttest_ind, mannwhitneyu
import warnings
warnings.filterwarnings("ignore")

# llama.cpp.python imports
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False

# lm-evaluation-harness imports
try:
    import lm_eval
    from lm_eval import evaluator, tasks
    LM_EVAL_AVAILABLE = True
except ImportError:
    LM_EVAL_AVAILABLE = False

# LightEval imports
try:
    from lighteval import LightevalPipeline, PipelineParameters
    from lighteval.tasks import TaskParameters
    from lighteval.models import ModelConfig
    LIGHT_EVAL_AVAILABLE = True
except ImportError:
    LIGHT_EVAL_AVAILABLE = False

# transformers benchmark
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ComprehensiveBenchmarkEvaluator:
    """
    åŒ…æ‹¬çš„LLMãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡å™¨
    Comprehensive LLM Benchmark Evaluator
    """

    def __init__(self, model_configs: Dict[str, Dict[str, Any]]):
        """
        Args:
            model_configs: ãƒ¢ãƒ‡ãƒ«è¨­å®šã®è¾æ›¸
                {
                    "modela": {"path": "path/to/modela.gguf", "type": "gguf"},
                    "modelb": {"path": "path/to/modelb.gguf", "type": "gguf"},
                    "modelc": {"path": "path/to/modelc.gguf", "type": "gguf"}
                }
        """
        self.model_configs = model_configs
        self.results = {}

        # åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç¢ºèª
        self.available_libraries = self._check_available_libraries()

        logger.info(f"Available benchmark libraries: {list(self.available_libraries.keys())}")

    def _check_available_libraries(self) -> Dict[str, bool]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç¢ºèª"""
        libraries = {
            'llama_cpp': LLAMA_CPP_AVAILABLE,
            'lm_eval': LM_EVAL_AVAILABLE,
            'light_eval': LIGHT_EVAL_AVAILABLE,
            'transformers': TRANSFORMERS_AVAILABLE
        }
        return {k: v for k, v in libraries.items() if v}

    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        Run comprehensive benchmark across all available libraries
        """
        logger.info("[COMPREHENSIVE BENCHMARK] Starting comprehensive evaluation...")

        # å„ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        for model_name, model_config in self.model_configs.items():
            logger.info(f"[COMPREHENSIVE BENCHMARK] Evaluating {model_name}...")
            self.results[model_name] = self._evaluate_single_model(model_name, model_config)

        # çµ±è¨ˆåˆ†æã¨æ¯”è¼ƒ
        comparison_results = self._perform_statistical_analysis()

        # HFæå‡ºç”¨ã‚°ãƒ©ãƒ•ã¨çµ±è¨ˆé‡ç”Ÿæˆ
        hf_submission_data = self._generate_hf_submission_data(comparison_results)

        return {
            'raw_results': self.results,
            'comparison': comparison_results,
            'hf_submission': hf_submission_data
        }

    def _evaluate_single_model(self, model_name: str, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
        results = {}

        # å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§è©•ä¾¡
        if 'llama_cpp' in self.available_libraries:
            results['llama_cpp'] = self._run_llama_cpp_benchmarks(model_config)

        if 'lm_eval' in self.available_libraries:
            results['lm_eval'] = self._run_lm_eval_benchmarks(model_config)

        if 'light_eval' in self.available_libraries:
            results['light_eval'] = self._run_light_eval_benchmarks(model_config)

        if 'transformers' in self.available_libraries:
            results['transformers'] = self._run_transformers_benchmarks(model_config)

        return results

    def _run_llama_cpp_benchmarks(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """llama.cppãƒ™ãƒ¼ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        gguf_path = model_config['path']

        try:
            # åŸºæœ¬çš„ãªllama.cppè©•ä¾¡
            llm = Llama(
                model_path=gguf_path,
                n_ctx=4096,
                n_threads=min(8, os.cpu_count()),  # CPUã‚¹ãƒ¬ãƒƒãƒ‰åˆ¶é™
                n_gpu_layers=-1,
                verbose=False
            )

            # åŸºæœ¬æ€§èƒ½ãƒ†ã‚¹ãƒˆ
            results = {
                'inference_speed': self._measure_inference_speed(llm),
                'memory_usage': self._measure_memory_usage(llm),
                'perplexity': self._calculate_perplexity(llm)
            }

            return results

        except Exception as e:
            logger.error(f"llama.cpp benchmark failed: {e}")
            return {'error': str(e)}

    def _run_lm_eval_benchmarks(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """lm-evaluation-harnessãƒ™ãƒ¼ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            model_path = model_config['path']
            model_type = model_config.get('type', 'gguf')

            if model_type == 'gguf':
                # GGUFãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€llama.cppçµŒç”±ã§è©•ä¾¡
                return self._run_llama_cpp_benchmarks(model_config)

            # HFãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ç›´æ¥lm_evalä½¿ç”¨
            # ä¸»è¦ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¿ã‚¹ã‚¯
            task_names = [
                "arc_challenge", "arc_easy", "boolq", "piqa", "winogrande",
                "hellaswag", "openbookqa", "sciq", "commonsense_qa",
                "mmlu", "gsm8k", "math", "truthfulqa"
            ]

            results = {}
            for task_name in task_names[:5]:  # æœ€åˆã®5ã¤ã ã‘å®Ÿè¡Œï¼ˆæ™‚é–“ç¯€ç´„ï¼‰
                try:
                    result = lm_eval.simple_evaluate(
                        model="hf",
                        model_args=f"pretrained={model_path}",
                        tasks=[task_name],
                        device="cuda" if torch.cuda.is_available() else "cpu",
                        batch_size=1,
                        num_fewshot=0
                    )
                    results[task_name] = result['results'][task_name]
                except Exception as e:
                    logger.warning(f"lm_eval task {task_name} failed: {e}")
                    results[task_name] = {'error': str(e)}

            return results

        except Exception as e:
            logger.error(f"lm_eval benchmark failed: {e}")
            return {'error': str(e)}

    def _run_light_eval_benchmarks(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """LightEvalãƒ™ãƒ¼ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            model_path = model_config['path']
            model_type = model_config.get('type', 'gguf')

            if model_type == 'gguf':
                # GGUFãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ç°¡æ˜“è©•ä¾¡
                return self._run_llama_cpp_benchmarks(model_config)

            # HFãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€LightEvalä½¿ç”¨
            model_config_lighteval = ModelConfig(
                model_name=model_path,
                model_dtype="auto",
                model_max_length=4096
            )

            # ä¸»è¦ãªã‚¿ã‚¹ã‚¯
            task_configs = [
                TaskParameters(name="arc:challenge:acc", suite=["helm"]),
                TaskParameters(name="hellaswag:acc", suite=["helm"]),
                TaskParameters(name="mmlu:acc", suite=["helm"]),
                TaskParameters(name="truthfulqa:mc:acc", suite=["helm"])
            ]

            pipeline_params = PipelineParameters(
                model=model_config_lighteval,
                tasks=task_configs,
                batch_size=1,
                max_samples=100,  # ã‚µãƒ³ãƒ—ãƒ«åˆ¶é™
                use_chat_template=False
            )

            pipeline = LightevalPipeline(pipeline_params)
            results = pipeline.run()

            return results.to_dict()

        except Exception as e:
            logger.error(f"LightEval benchmark failed: {e}")
            return {'error': str(e)}

    def _run_transformers_benchmarks(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """transformersãƒ™ãƒ¼ã‚¹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            model_path = model_config['path']
            model_type = model_config.get('type', 'gguf')

            if model_type == 'hf':
                # HFãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€transformers benchmarkä½¿ç”¨
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                tokenizer = AutoTokenizer.from_pretrained(model_path)

                # åŸºæœ¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
                results = {
                    'model_size': self._calculate_model_size(model),
                    'vocab_size': len(tokenizer),
                    'max_position_embeddings': model.config.max_position_embeddings
                }

                return results
            else:
                # GGUFãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€åŸºæœ¬æƒ…å ±ã®ã¿
                return {
                    'model_type': 'gguf',
                    'path': model_path
                }

        except Exception as e:
            logger.error(f"transformers benchmark failed: {e}")
            return {'error': str(e)}

    def _measure_inference_speed(self, llm: 'Llama') -> Dict[str, float]:
        """æ¨è«–é€Ÿåº¦æ¸¬å®š"""
        test_prompts = [
            "Hello, how are you?",
            "Explain quantum computing in simple terms.",
            "Write a short story about AI.",
            "What is the capital of Japan?"
        ]

        total_tokens = 0
        total_time = 0

        for prompt in test_prompts:
            start_time = time.time()

            response = llm(
                prompt,
                max_tokens=50,
                temperature=0.1,
                echo=False
            )

            end_time = time.time()
            tokens_generated = len(response['choices'][0]['text'].split())
            total_tokens += tokens_generated
            total_time += (end_time - start_time)

        tokens_per_sec = total_tokens / total_time if total_time > 0 else 0

        return {
            'tokens_per_sec': tokens_per_sec,
            'total_tokens': total_tokens,
            'total_time': total_time,
            'avg_tokens_per_prompt': total_tokens / len(test_prompts)
        }

    def _measure_memory_usage(self, llm: 'Llama') -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        try:
            import psutil
            process = psutil.Process()

            # åˆæœŸãƒ¡ãƒ¢ãƒª
            initial_memory = process.memory_info().rss / (1024**3)  # GB

            # ãƒ†ã‚¹ãƒˆæ¨è«–å®Ÿè¡Œ
            llm("Test prompt", max_tokens=10, echo=False)

            # å®Ÿè¡Œå¾Œãƒ¡ãƒ¢ãƒª
            final_memory = process.memory_info().rss / (1024**3)  # GB

            return {
                'initial_memory_gb': initial_memory,
                'final_memory_gb': final_memory,
                'memory_increase_gb': final_memory - initial_memory
            }

        except Exception as e:
            return {'error': str(e)}

    def _calculate_perplexity(self, llm: 'Llama') -> Dict[str, float]:
        """ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£è¨ˆç®—"""
        test_texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Quantum physics describes the behavior of matter and energy."
        ]

        total_log_prob = 0
        total_tokens = 0

        for text in test_texts:
            try:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦å¯¾æ•°ç¢ºç‡ã‚’è¨ˆç®—
                tokens = llm.tokenize(text.encode())
                if len(tokens) > 1:
                    log_probs = []
                    for i in range(len(tokens) - 1):
                        context = tokens[:i+1]
                        next_token = tokens[i+1]

                        # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ã‚’è¨ˆç®—
                        logits = llm.eval(context)
                        probs = torch.softmax(torch.tensor(logits[0][-1]), dim=0)
                        prob = probs[next_token].item()
                        log_prob = np.log(prob) if prob > 0 else -10
                        log_probs.append(log_prob)

                    total_log_prob += sum(log_probs)
                    total_tokens += len(log_probs)

            except Exception as e:
                logger.warning(f"Perplexity calculation failed for text: {e}")

        avg_log_prob = total_log_prob / total_tokens if total_tokens > 0 else 0
        perplexity = np.exp(-avg_log_prob) if avg_log_prob != 0 else float('inf')

        return {
            'perplexity': perplexity,
            'avg_log_prob': avg_log_prob,
            'total_tokens': total_tokens
        }

    def _calculate_model_size(self, model) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 2 / (1024**2),  # rough estimate in MB
            'trainable_ratio': trainable_params / total_params if total_params > 0 else 0
        }

    def _perform_statistical_analysis(self) -> Dict[str, Any]:
        """çµ±è¨ˆåˆ†æå®Ÿè¡Œ"""
        logger.info("[STATISTICS] Performing statistical analysis...")

        # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        df_results = self._convert_results_to_dataframe()

        # çµ±è¨ˆçš„æ¯”è¼ƒ
        statistical_comparison = self._perform_statistical_comparison(df_results)

        # ABCãƒ†ã‚¹ãƒˆï¼ˆA/B/Cãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒï¼‰
        abc_test_results = self._perform_abc_test(df_results)

        return {
            'dataframe': df_results,
            'statistical_comparison': statistical_comparison,
            'abc_test': abc_test_results
        }

    def _convert_results_to_dataframe(self) -> pd.DataFrame:
        """çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›"""
        all_results = []

        for model_name, model_results in self.results.items():
            for library_name, library_results in model_results.items():
                if isinstance(library_results, dict):
                    for metric_name, metric_value in library_results.items():
                        if isinstance(metric_value, (int, float)):
                            all_results.append({
                                'model': model_name,
                                'library': library_name,
                                'metric': metric_name,
                                'value': metric_value
                            })

        return pd.DataFrame(all_results)

    def _perform_statistical_comparison(self, df: pd.DataFrame) -> Dict[str, Any]:
        """çµ±è¨ˆçš„æ¯”è¼ƒå®Ÿè¡Œ"""
        comparison_results = {}

        # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«å¯¾ã—ã¦çµ±è¨ˆçš„æ¯”è¼ƒ
        for metric in df['metric'].unique():
            metric_data = df[df['metric'] == metric]

            if len(metric_data) < 2:
                continue

            # ãƒ¢ãƒ‡ãƒ«é–“ã®çµ±è¨ˆçš„æ¯”è¼ƒ
            models = metric_data['model'].unique()
            if len(models) >= 2:
                comparison_results[metric] = {}

                for i, model1 in enumerate(models):
                    for j, model2 in enumerate(models):
                        if i < j:
                            data1 = metric_data[metric_data['model'] == model1]['value']
                            data2 = metric_data[metric_data['model'] == model2]['value']

                            if len(data1) > 0 and len(data2) > 0:
                                # t-test
                                try:
                                    t_stat, p_value = ttest_ind(data1, data2)
                                    comparison_results[metric][f'{model1}_vs_{model2}'] = {
                                        't_statistic': t_stat,
                                        'p_value': p_value,
                                        'significant': p_value < 0.05
                                    }
                                except:
                                    comparison_results[metric][f'{model1}_vs_{model2}'] = {
                                        'error': 'Statistical test failed'
                                    }

        return comparison_results

    def _perform_abc_test(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ABCãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆA/B/Cãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒï¼‰"""
        abc_results = {}

        # A/B/Cãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        available_models = set(df['model'].unique())
        abc_models = {'modela', 'modelb', 'modelc'}
        available_abc = available_models.intersection(abc_models)

        if len(available_abc) < 2:
            abc_results['error'] = f"Need at least 2 ABC models. Available: {available_abc}"
            return abc_results

        # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«å¯¾ã—ã¦ABCæ¯”è¼ƒ
        for metric in df['metric'].unique():
            metric_data = df[df['metric'] == metric]

            abc_results[metric] = {}

            # å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡å€¤ã¨æ¨™æº–åå·®
            for model in available_abc:
                model_data = metric_data[metric_data['model'] == model]
                if len(model_data) > 0:
                    abc_results[metric][model] = {
                        'mean': model_data['value'].mean(),
                        'std': model_data['value'].std(),
                        'count': len(model_data),
                        'sem': model_data['value'].sem()  # æ¨™æº–èª¤å·®
                    }

        # å‹è€…æ±ºå®šï¼ˆæœ€ã‚‚é«˜ã„å¹³å‡å€¤ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ï¼‰
        if abc_results:
            sample_metric = list(abc_results.keys())[0]  # æœ€åˆã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ç”¨
            model_scores = {}

            for model in available_abc:
                if model in abc_results[sample_metric]:
                    model_scores[model] = abc_results[sample_metric][model]['mean']

            winner = max(model_scores, key=model_scores.get)
            abc_results['winner'] = {
                'model': winner,
                'score': model_scores[winner],
                'metric': sample_metric
            }

        return abc_results

    def _generate_hf_submission_data(self, comparison_results: Dict[str, Any]) -> Dict[str, Any]:
        """HFæå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        logger.info("[HF SUBMISSION] Generating HF submission data...")

        # ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãã‚°ãƒ©ãƒ•ç”Ÿæˆ
        plots = self._generate_error_bar_plots(comparison_results)

        # è¦ç´„çµ±è¨ˆé‡
        summary_stats = self._generate_summary_statistics(comparison_results)

        # ABCãƒ†ã‚¹ãƒˆçµæœ
        abc_summary = self._generate_abc_summary(comparison_results)

        return {
            'plots': plots,
            'summary_statistics': summary_stats,
            'abc_test_results': abc_summary,
            'recommendations': self._generate_recommendations(comparison_results)
        }

    def _generate_error_bar_plots(self, comparison_results: Dict[str, Any]) -> Dict[str, str]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        plots = {}

        try:
            df = comparison_results['dataframe']

            # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«å¯¾ã—ã¦ã‚°ãƒ©ãƒ•ç”Ÿæˆ
            for metric in df['metric'].unique():
                metric_data = df[df['metric'] == metric]

                if len(metric_data) == 0:
                    continue

                plt.figure(figsize=(12, 8))
                sns.set_style("whitegrid")

                # ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ£’ã‚°ãƒ©ãƒ•
                ax = sns.barplot(
                    data=metric_data,
                    x='model',
                    y='value',
                    hue='library',
                    errorbar='sd',  # æ¨™æº–åå·®ã‚’ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ã¨ã—ã¦ä½¿ç”¨
                    capsize=0.1
                )

                plt.title(f'{metric.replace("_", " ").title()} - Model Comparison')
                plt.xlabel('Model')
                plt.ylabel(metric.replace("_", " ").title())
                plt.legend(title='Benchmark Library', bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.xticks(rotation=45)
                plt.tight_layout()

                # ç”»åƒã¨ã—ã¦ä¿å­˜
                plot_filename = f"{metric}_comparison.png"
                plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
                plt.close()

                plots[metric] = plot_filename

        except Exception as e:
            logger.error(f"Error bar plot generation failed: {e}")

        return plots

    def _generate_summary_statistics(self, comparison_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¦ç´„çµ±è¨ˆé‡ç”Ÿæˆ"""
        summary = {}

        try:
            df = comparison_results['dataframe']

            # å„ãƒ¢ãƒ‡ãƒ«ãƒ»å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®çµ±è¨ˆé‡
            for model in df['model'].unique():
                model_data = df[df['model'] == model]
                summary[model] = {}

                for metric in df['metric'].unique():
                    metric_data = model_data[model_data['metric'] == metric]

                    if len(metric_data) > 0:
                        values = metric_data['value'].values
                        summary[model][metric] = {
                            'count': len(values),
                            'mean': float(np.mean(values)),
                            'std': float(np.std(values)),
                            'min': float(np.min(values)),
                            'max': float(np.max(values)),
                            'median': float(np.median(values)),
                            'q25': float(np.percentile(values, 25)),
                            'q75': float(np.percentile(values, 75)),
                            'sem': float(stats.sem(values)) if len(values) > 1 else 0.0
                        }

        except Exception as e:
            logger.error(f"Summary statistics generation failed: {e}")

        return summary

    def _generate_abc_summary(self, comparison_results: Dict[str, Any]) -> Dict[str, Any]:
        """ABCãƒ†ã‚¹ãƒˆè¦ç´„ç”Ÿæˆ"""
        abc_results = comparison_results.get('abc_test', {})

        if 'winner' not in abc_results:
            return {'error': 'ABC test not completed'}

        winner = abc_results['winner']

        summary = {
            'winner_model': winner['model'],
            'winning_score': winner['score'],
            'winning_metric': winner['metric'],
            'model_rankings': {}
        }

        # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã§ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if abc_results:
            for metric, model_stats in abc_results.items():
                if metric not in ['winner', 'error'] and isinstance(model_stats, dict):
                    rankings = sorted(
                        [(model, stats.get('mean', 0)) for model, stats in model_stats.items()
                         if isinstance(stats, dict)],
                        key=lambda x: x[1],
                        reverse=True
                    )
                    summary['model_rankings'][metric] = rankings

        return summary

    def _generate_recommendations(self, comparison_results: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        abc_results = comparison_results.get('abc_test', {})
        if 'winner' in abc_results:
            winner = abc_results['winner']
            recommendations.append(
                f"ğŸ† Winner: {winner['model']} with score {winner['score']:.3f} on {winner['metric']}"
            )

        # çµ±è¨ˆçš„æœ‰æ„å·®ã®åˆ†æ
        statistical_comp = comparison_results.get('statistical_comparison', {})
        significant_differences = []

        for metric, comparisons in statistical_comp.items():
            for comparison_name, results in comparisons.items():
                if isinstance(results, dict) and results.get('significant', False):
                    significant_differences.append(f"{comparison_name} on {metric}")

        if significant_differences:
            recommendations.append(f"ğŸ“Š Significant differences found: {', '.join(significant_differences[:3])}")

        # ä½¿ç”¨æ¨å¥¨
        if abc_results and 'winner' in abc_results:
            winner = abc_results['winner']['model']
            if winner == 'modela':
                recommendations.append("ğŸ’¡ Model A (Borea-Phi3.5-instruct-jp GGUF) recommended for general use")
            elif winner == 'modelb':
                recommendations.append("ğŸ’¡ Model B recommended for specialized tasks")
            elif winner == 'modelc':
                recommendations.append("ğŸ’¡ Model C recommended for high-performance applications")

        return recommendations


def run_abc_test_evaluation():
    """
    ABCãƒ†ã‚¹ãƒˆè©•ä¾¡å®Ÿè¡Œ
    A: Borea-Phi3.5-instruct-jp GGUF
    B: AEGIS-Phi3.5-Enhanced
    C: AEGIS-Phi3.5-Golden-Sigmoid
    """
    logger.info("[ABC TEST] Starting ABC test evaluation...")

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    model_configs = {
        'modela': {
            'path': 'D:/webdataset/gguf_models/borea_phi35_instruct_jp_q8_0.gguf',
            'type': 'gguf',
            'description': 'Borea-Phi3.5-instruct-jp (GGUF Q8_0)'
        },
        'modelb': {
            'path': 'D:/webdataset/models/borea_phi35_alpha_gate_sigmoid_bayesian/final',
            'type': 'hf',
            'description': 'AEGIS-Phi3.5-Enhanced Model'
        },
        'modelc': {
            'path': 'D:/webdataset/models/borea_phi35_so8t_rtx3060/final',
            'type': 'hf',
            'description': 'AEGIS-Phi3.5-Golden-Sigmoid Model'
        }
    }

    # å­˜åœ¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã ã‘ã‚’è©•ä¾¡
    available_configs = {}
    for model_name, config in model_configs.items():
        if os.path.exists(config['path']):
            available_configs[model_name] = config
            logger.info(f"[ABC TEST] Found model {model_name}: {config['path']}")
        else:
            logger.warning(f"[ABC TEST] Model {model_name} not found: {config['path']}")

    if len(available_configs) < 2:
        logger.error("Need at least 2 models for ABC test")
        return None

    # åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    evaluator = ComprehensiveBenchmarkEvaluator(available_configs)
    results = evaluator.run_comprehensive_benchmark()

    # çµæœä¿å­˜
    output_dir = "D:/webdataset/results/abc_test_results"
    os.makedirs(output_dir, exist_ok=True)

    # JSONä¿å­˜
    with open(f"{output_dir}/abc_test_results.json", 'w', encoding='utf-8') as f:
        # DataFrameã‚’JSONã«å¤‰æ›ã™ã‚‹ãŸã‚ã«å‰å‡¦ç†
        save_results = results.copy()
        if 'comparison' in save_results and 'dataframe' in save_results['comparison']:
            df = save_results['comparison']['dataframe']
            save_results['comparison']['dataframe'] = df.to_dict('records')

        json.dump(save_results, f, indent=2, ensure_ascii=False, default=str)

    logger.info(f"[ABC TEST] Results saved to {output_dir}")

    # ABCãƒ†ã‚¹ãƒˆçµæœè¡¨ç¤º
    abc_results = results.get('comparison', {}).get('abc_test', {})
    if 'winner' in abc_results:
        winner = abc_results['winner']
        logger.info("ğŸ¯ ABC Test Winner:"        logger.info(f"   Model: {winner['model']}")
        logger.info(f"   Score: {winner['score']:.4f}")
        logger.info(f"   Metric: {winner['metric']}")

    return results


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Comprehensive LLM Benchmark Evaluation System"
    )
    parser.add_argument(
        '--abc_test',
        action='store_true',
        help='Run ABC test evaluation (A: Borea-Phi3.5 GGUF, B: Alpha Gate, C: RTX3060 SO8T)'
    )
    parser.add_argument(
        '--models',
        type=str,
        help='JSON file with model configurations'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='D:/webdataset/results/benchmark_results',
        help='Output directory for results'
    )

    args = parser.parse_args()

    if args.abc_test:
        # ABCãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        results = run_abc_test_evaluation()
        if results:
            logger.info("[SUCCESS] ABC test completed successfully!")
        else:
            logger.error("[FAILED] ABC test failed!")
            sys.exit(1)
    else:
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        if not args.models:
            logger.error("Please specify --models JSON file or use --abc_test")
            sys.exit(1)

        # ãƒ¢ãƒ‡ãƒ«è¨­å®šèª­ã¿è¾¼ã¿
        with open(args.models, 'r', encoding='utf-8') as f:
            model_configs = json.load(f)

        # åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        evaluator = ComprehensiveBenchmarkEvaluator(model_configs)
        results = evaluator.run_comprehensive_benchmark()

        # çµæœä¿å­˜
        os.makedirs(args.output_dir, exist_ok=True)
        with open(f"{args.output_dir}/benchmark_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"[SUCCESS] Benchmark completed! Results saved to {args.output_dir}")


if __name__ == '__main__':
    main()
