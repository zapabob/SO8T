#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HFæå‡ºç”¨çµ±è¨ˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
HF Submission Statistics System

ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãã‚°ãƒ©ãƒ•ã€è¦ç´„çµ±è¨ˆé‡ã€ABCãƒ†ã‚¹ãƒˆçµæœã‚’HFæå‡ºå¯èƒ½ãªå½¢å¼ã§ç”Ÿæˆ
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
logger=logging.getLogger(__name__)
from scipy import stats
from scipy.stats import ttest_ind, mannwhitneyu, normaltest
import warnings
warnings.filterwarnings("ignore")

# çµ±è¨ˆåˆ†æå¼·åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import pymc3 as pm
    import arviz as az
    HAS_BAYESIAN = True
except ImportError:
    HAS_BAYESIAN = False
    logger.warning("Bayesian libraries not available. Install pymc3 and arviz for enhanced statistical analysis.")

try:
    from statsmodels.multivariate.manova import MANOVA
    from statsmodels.formula.api import ols
    from statsmodels.stats.multicomp import pairwise_tukeyhsd
    from statsmodels.stats.anova import AnovaRM
    from statsmodels.stats.multitest import multipletests
    HAS_MULTIVARIATE = True
except ImportError:
    HAS_MULTIVARIATE = False
    logger.warning("Multivariate analysis libraries not available. Install statsmodels for enhanced analysis.")

try:
    from pingouin import sphericity, epsilon
    HAS_SPHERICITY = True
except ImportError:
    HAS_SPHERICITY = False
    logger.warning("Sphericity testing libraries not available. Install pingouin for sphericity analysis.")

try:
    import pingouin as pg
    HAS_EFFECT_SIZE = True
except ImportError:
    HAS_EFFECT_SIZE = False
    logger.warning("Effect size libraries not available. Install pingouin for enhanced effect size calculations.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆmatplotlibï¼‰
plt.rcParams['font.family'] = ['DejaVu Sans', 'Liberation Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False


class HFSubmissionStatistics:
    """
    HFæå‡ºç”¨çµ±è¨ˆå‡¦ç†ã‚¯ãƒ©ã‚¹
    HF Submission Statistics Class
    """

    def __init__(self, results_data: Dict[str, Any], output_dir: str = "D:/webdataset/results/hf_submission"):
        self.results_data = results_data
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (12, 8)
        plt.rcParams['font.size'] = 12

    def perform_multivariate_analysis(self, benchmark_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤šå¤‰é‡è§£æã®å®Ÿè¡Œ
        Perform multivariate analysis
        """
        if not HAS_MULTIVARIATE:
            logger.warning("Multivariate analysis not available - statsmodels not installed")
            return {"multivariate_analysis": "Not available - install statsmodels"}

        try:
            results = {}

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            all_scores = []
            for model_name, scores in benchmark_data.items():
                if isinstance(scores, dict):
                    for benchmark, score in scores.items():
                        if isinstance(score, (int, float)):
                            all_scores.append({
                                'model': model_name,
                                'benchmark': benchmark,
                                'score': score
                            })

            if not all_scores:
                return {"multivariate_analysis": "No valid data for analysis"}

            df = pd.DataFrame(all_scores)

            # MANOVAåˆ†æ
            try:
                formula = 'score ~ model + benchmark + model:benchmark'
                manova_model = ols(formula, data=df).fit()
                manova_results = MANOVA.from_formula(formula, data=df)
                results['manova'] = {
                    'summary': str(manova_results.mv_test()),
                    'r_squared': manova_model.rsquared,
                    'f_statistic': manova_model.fvalue,
                    'p_value': manova_model.f_pvalue
                }
            except Exception as e:
                results['manova'] = f"MANOVA analysis failed: {str(e)}"

            # Tukeyã®HSDäº‹å¾Œæ¤œå®š
            try:
                tukey = pairwise_tukeyhsd(endog=df['score'], groups=df['model'], alpha=0.05)
                results['tukey_hsd'] = {
                    'summary': tukey.summary().as_text(),
                    'reject_null': tukey.reject.tolist(),
                    'meandiffs': tukey.meandiffs.tolist(),
                    'confint': tukey.confint.tolist()
                }
            except Exception as e:
                results['tukey_hsd'] = f"Tukey HSD failed: {str(e)}"

            # çƒé¢æ€§ãƒ†ã‚¹ãƒˆï¼ˆMauchlyã®çƒé¢æ€§æ¤œå®šï¼‰
            try:
                sphericity_results = self._test_sphericity(df)
                results['sphericity_test'] = sphericity_results
            except Exception as e:
                results['sphericity_test'] = f"Sphericity test failed: {str(e)}"

            # åå¾©æ¸¬å®šANOVAï¼ˆçƒé¢æ€§è£œæ­£ä»˜ãï¼‰
            try:
                rm_anova_results = self._perform_repeated_measures_anova(df)
                results['repeated_measures_anova'] = rm_anova_results
            except Exception as e:
                results['repeated_measures_anova'] = f"Repeated measures ANOVA failed: {str(e)}"

            return {"multivariate_analysis": results}

        except Exception as e:
            logger.error(f"Multivariate analysis failed: {str(e)}")
            return {"multivariate_analysis": f"Analysis failed: {str(e)}"}

    def _test_sphericity(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        çƒé¢æ€§ãƒ†ã‚¹ãƒˆï¼ˆMauchlyã®çƒé¢æ€§æ¤œå®šï¼‰
        Test for sphericity (Mauchly's test)
        """
        sphericity_results = {}

        if not HAS_SPHERICITY:
            sphericity_results['available'] = False
            sphericity_results['note'] = "Sphericity testing requires pingouin library"
            return sphericity_results

        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¯ã‚¤ãƒ‰å½¢å¼ã«å¤‰æ›ï¼ˆåå¾©æ¸¬å®šç”¨ï¼‰
            wide_df = df.pivot_table(values='score', index='model', columns='benchmark', aggfunc='mean')

            # Mauchlyã®çƒé¢æ€§æ¤œå®š
            if len(wide_df.columns) > 2:  # å°‘ãªãã¨ã‚‚3ã¤ã®æ¸¬å®šãŒå¿…è¦
                sphericity_test = sphericity(wide_df.values, method='mauchly')
                sphericity_results['mauchly_test'] = {
                    'W': float(sphericity_test['W']),
                    'p_value': float(sphericity_test['pval']),
                    'sphericity_assumed': bool(sphericity_test['sphericity']),
                    'interpretation': "çƒé¢æ€§ã®ä»®å®šãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹" if sphericity_test['sphericity'] else "çƒé¢æ€§ã®ä»®å®šãŒæº€ãŸã•ã‚Œã¦ã„ãªã„"
                }

                # Greenhouse-Geisserã‚¤ãƒ—ã‚·ãƒ­ãƒ³
                gg_epsilon = epsilon(wide_df.values, correction='gg')
                sphericity_results['greenhouse_geisser_epsilon'] = float(gg_epsilon)

                # Huynh-Feldtã‚¤ãƒ—ã‚·ãƒ­ãƒ³
                hf_epsilon = epsilon(wide_df.values, correction='hf')
                sphericity_results['huynh_feldt_epsilon'] = float(hf_epsilon)

                # æ¨å¥¨ã•ã‚Œã‚‹è£œæ­£æ–¹æ³•
                if not sphericity_test['sphericity']:
                    if gg_epsilon > 0.75:
                        recommended_correction = "Huynh-Feldt"
                    else:
                        recommended_correction = "Greenhouse-Geisser"
                    sphericity_results['recommended_correction'] = recommended_correction
                else:
                    sphericity_results['recommended_correction'] = "ãªã—ï¼ˆçƒé¢æ€§ãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹ï¼‰"

            else:
                sphericity_results['note'] = "çƒé¢æ€§ãƒ†ã‚¹ãƒˆã«ã¯å°‘ãªãã¨ã‚‚3ã¤ã®æ¸¬å®šç‚¹ãŒå¿…è¦ã§ã™"

            sphericity_results['available'] = True

        except Exception as e:
            logger.error(f"Sphericity test failed: {str(e)}")
            sphericity_results['error'] = str(e)
            sphericity_results['available'] = False

        return sphericity_results

    def _perform_repeated_measures_anova(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åå¾©æ¸¬å®šANOVAï¼ˆçƒé¢æ€§è£œæ­£ä»˜ãï¼‰
        Perform repeated measures ANOVA with sphericity corrections
        """
        rm_anova_results = {}

        try:
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
            if len(df['benchmark'].unique()) < 2 or len(df['model'].unique()) < 2:
                rm_anova_results['note'] = "åå¾©æ¸¬å®šANOVAã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®æ¡ä»¶ãŒå¿…è¦ã§ã™"
                return rm_anova_results

            # statsmodelsã‚’ä½¿ã£ãŸåå¾©æ¸¬å®šANOVA
            # æ³¨: ã“ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã¯æ¨™æº–çš„ãªåå¾©æ¸¬å®šãƒ‡ã‚¶ã‚¤ãƒ³ã§ã¯ãªã„ãŸã‚ã€é©å¿œãŒå¿…è¦
            try:
                # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã®ãŸã‚ã®æº–å‚™
                models = df['model'].unique()
                benchmarks = df['benchmark'].unique()

                anova_results = {}

                # å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®ãƒ¢ãƒ‡ãƒ«é–“æ¯”è¼ƒ
                for benchmark in benchmarks:
                    bench_data = df[df['benchmark'] == benchmark].copy()

                    if len(bench_data['model'].unique()) > 1:
                        # ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æ
                        from scipy.stats import f_oneway

                        model_groups = []
                        group_labels = []

                        for model in models:
                            model_scores = bench_data[bench_data['model'] == model]['score']
                            if len(model_scores) > 0:
                                model_groups.append(model_scores.values)
                                group_labels.append(model)

                        if len(model_groups) > 1:
                            f_stat, p_value = f_oneway(*model_groups)

                            anova_results[benchmark] = {
                                'f_statistic': float(f_stat),
                                'p_value': float(p_value),
                                'significant': p_value < 0.05,
                                'degrees_of_freedom': (len(model_groups) - 1, sum(len(g) for g in model_groups) - len(model_groups))
                            }

                rm_anova_results['benchmark_anova'] = anova_results

                # çƒé¢æ€§è£œæ­£ã®é©ç”¨
                sphericity_info = self._test_sphericity(df)
                if 'mauchly_test' in sphericity_info:
                    mauchly = sphericity_info['mauchly_test']

                    # çƒé¢æ€§ãŒæº€ãŸã•ã‚Œã¦ã„ãªã„å ´åˆã®è£œæ­£
                    if not mauchly['sphericity_assumed']:
                        correction_method = sphericity_info.get('recommended_correction', 'Greenhouse-Geisser')
                        epsilon_value = sphericity_info.get('greenhouse_geisser_epsilon', 1.0)

                        rm_anova_results['sphericity_correction'] = {
                            'applied': True,
                            'method': correction_method,
                            'epsilon': epsilon_value,
                            'note': f"çƒé¢æ€§ãŒæº€ãŸã•ã‚Œã¦ã„ãªã„ãŸã‚ã€{correction_method}è£œæ­£ã‚’é©ç”¨"
                        }

                        # è£œæ­£ã•ã‚ŒãŸpå€¤ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                        for bench_name, result in anova_results.items():
                            if 'p_value' in result:
                                # Greenhouse-Geisserè£œæ­£ã®è¿‘ä¼¼
                                corrected_p = min(1.0, result['p_value'] / epsilon_value)
                                result['corrected_p_value'] = corrected_p
                                result['correction_applied'] = True
                    else:
                        rm_anova_results['sphericity_correction'] = {
                            'applied': False,
                            'note': "çƒé¢æ€§ã®ä»®å®šãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€è£œæ­£ä¸è¦"
                        }

            except Exception as e:
                rm_anova_results['anova_error'] = str(e)

        except Exception as e:
            logger.error(f"Repeated measures ANOVA failed: {str(e)}")
            rm_anova_results['error'] = str(e)

        return rm_anova_results

    def perform_bayesian_analysis(self, benchmark_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ãƒ™ã‚¤ã‚ºçµ±è¨ˆåˆ†æã®å®Ÿè¡Œ
        Perform Bayesian statistical analysis
        """
        if not HAS_BAYESIAN:
            logger.warning("Bayesian analysis not available - pymc3/arviz not installed")
            return {"bayesian_analysis": "Not available - install pymc3 and arviz"}

        try:
            results = {}

            # å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            model_scores = {}
            for model_name, scores in benchmark_data.items():
                if isinstance(scores, dict):
                    scores_list = [v for v in scores.values() if isinstance(v, (int, float))]
                    if scores_list:
                        model_scores[model_name] = scores_list

            if len(model_scores) < 2:
                return {"bayesian_analysis": "Need at least 2 models for comparison"}

            # ãƒ™ã‚¤ã‚ºéšå±¤ãƒ¢ãƒ‡ãƒ«
            with pm.Model() as hierarchical_model:
                # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                mu_hyper = pm.Normal('mu_hyper', mu=0, sigma=10)
                sigma_hyper = pm.HalfNormal('sigma_hyper', sigma=10)

                # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                model_mus = {}
                model_sigmas = {}

                for model_name, scores in model_scores.items():
                    model_mus[model_name] = pm.Normal(f'mu_{model_name}', mu=mu_hyper, sigma=sigma_hyper)
                    model_sigmas[model_name] = pm.HalfNormal(f'sigma_{model_name}', sigma=sigma_hyper)

                    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
                    pm.Normal(f'obs_{model_name}', mu=model_mus[model_name],
                             sigma=model_sigmas[model_name], observed=scores)

                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                trace = pm.sample(1000, tune=1000, return_inferencedata=True)

            # åˆ†æçµæœ
            model_names = list(model_scores.keys())
            comparison_results = {}

            for i, model1 in enumerate(model_names):
                for j, model2 in enumerate(model_names):
                    if i < j:
                        mu1_samples = trace.posterior[f'mu_{model1}'].values.flatten()
                        mu2_samples = trace.posterior[f'mu_{model2}'].values.flatten()
                        diff_samples = mu1_samples - mu2_samples

                        comparison_results[f'{model1}_vs_{model2}'] = {
                            'mean_difference': float(np.mean(diff_samples)),
                            'credible_interval': [float(np.percentile(diff_samples, 2.5)),
                                                float(np.percentile(diff_samples, 97.5))],
                            'probability_superior': float(np.mean(diff_samples > 0)),
                            'effect_size': float(np.mean(diff_samples) / np.std(diff_samples))
                        }

            results['hierarchical_model'] = {
                'model_comparison': comparison_results,
                'trace_summary': str(az.summary(trace, round_to=2))
            }

            return {"bayesian_analysis": results}

        except Exception as e:
            logger.error(f"Bayesian analysis failed: {str(e)}")
            return {"bayesian_analysis": f"Analysis failed: {str(e)}"}

    def calculate_effect_sizes(self, benchmark_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŠ¹æœé‡ã®è¨ˆç®—ã¨è§£é‡ˆ
        Calculate and interpret effect sizes
        """
        try:
            results = {}

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
            all_scores = []
            for model_name, scores in benchmark_data.items():
                if isinstance(scores, dict):
                    for benchmark, score in scores.items():
                        if isinstance(score, (int, float)):
                            all_scores.append({
                                'model': model_name,
                                'benchmark': benchmark,
                                'score': score
                            })

            if not all_scores:
                return {"effect_sizes": "No valid data for analysis"}

            df = pd.DataFrame(all_scores)

            # å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®åŠ¹æœé‡è¨ˆç®—
            benchmark_effect_sizes = {}
            for benchmark in df['benchmark'].unique():
                benchmark_data = df[df['benchmark'] == benchmark]
                if len(benchmark_data['model'].unique()) >= 2:
                    try:
                        if HAS_EFFECT_SIZE:
                            # pingouinã‚’ä½¿ã£ãŸåŠ¹æœé‡è¨ˆç®—
                            effect_size_result = pg.compute_effsize(
                                benchmark_data['score'][benchmark_data['model'] == benchmark_data['model'].unique()[0]],
                                benchmark_data['score'][benchmark_data['model'] == benchmark_data['model'].unique()[1]],
                                eftype='cohen'
                            )
                            effect_size = float(effect_size_result)
                        else:
                            # æ‰‹å‹•è¨ˆç®—
                            group1 = benchmark_data['score'][benchmark_data['model'] == benchmark_data['model'].unique()[0]]
                            group2 = benchmark_data['score'][benchmark_data['model'] == benchmark_data['model'].unique()[1]]
                            mean_diff = np.mean(group1) - np.mean(group2)
                            pooled_std = np.sqrt((np.var(group1) + np.var(group2)) / 2)
                            effect_size = mean_diff / pooled_std if pooled_std > 0 else 0

                        # åŠ¹æœé‡ã®è§£é‡ˆ
                        if abs(effect_size) < 0.2:
                            interpretation = "negligible"
                        elif abs(effect_size) < 0.5:
                            interpretation = "small"
                        elif abs(effect_size) < 0.8:
                            interpretation = "medium"
                        else:
                            interpretation = "large"

                        benchmark_effect_sizes[benchmark] = {
                            'effect_size': effect_size,
                            'interpretation': interpretation,
                            'models_compared': list(benchmark_data['model'].unique())
                        }
                    except Exception as e:
                        benchmark_effect_sizes[benchmark] = f"Effect size calculation failed: {str(e)}"

            # å…¨ä½“çš„ãªåŠ¹æœé‡ã‚µãƒãƒªãƒ¼
            overall_effect_sizes = []
            for bench_data in benchmark_effect_sizes.values():
                if isinstance(bench_data, dict) and 'effect_size' in bench_data:
                    overall_effect_sizes.append(bench_data['effect_size'])

            if overall_effect_sizes:
                results['overall_summary'] = {
                    'mean_effect_size': float(np.mean(overall_effect_sizes)),
                    'median_effect_size': float(np.median(overall_effect_sizes)),
                    'effect_size_range': [float(np.min(overall_effect_sizes)), float(np.max(overall_effect_sizes))],
                    'distribution': {
                        'negligible': len([x for x in overall_effect_sizes if abs(x) < 0.2]),
                        'small': len([x for x in overall_effect_sizes if 0.2 <= abs(x) < 0.5]),
                        'medium': len([x for x in overall_effect_sizes if 0.5 <= abs(x) < 0.8]),
                        'large': len([x for x in overall_effect_sizes if abs(x) >= 0.8])
                    }
                }

            results['benchmark_effect_sizes'] = benchmark_effect_sizes

            return {"effect_sizes": results}

        except Exception as e:
            logger.error(f"Effect size calculation failed: {str(e)}")
            return {"effect_sizes": f"Analysis failed: {str(e)}"}

    def generate_enhanced_statistical_analysis(self) -> Dict[str, Any]:
        """
        å¼·åŒ–ã•ã‚ŒãŸçµ±è¨ˆåˆ†æã‚’ç”Ÿæˆï¼ˆå¤šå¤‰é‡è§£æ + ãƒ™ã‚¤ã‚ºçµ±è¨ˆ + åŠ¹æœé‡ï¼‰
        Generate enhanced statistical analysis
        """
        enhanced_analysis = {}

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        benchmark_data = {}
        if 'abc_test_results' in self.results_data:
            for model_key, model_data in self.results_data['abc_test_results'].items():
                if 'benchmark_scores' in model_data:
                    benchmark_data[model_key] = model_data['benchmark_scores']

        if benchmark_data:
            # å¤šå¤‰é‡è§£æ
            enhanced_analysis.update(self.perform_multivariate_analysis(benchmark_data))

            # ãƒ™ã‚¤ã‚ºçµ±è¨ˆåˆ†æ
            enhanced_analysis.update(self.perform_bayesian_analysis(benchmark_data))

            # åŠ¹æœé‡è¨ˆç®—
            enhanced_analysis.update(self.calculate_effect_sizes(benchmark_data))

        return enhanced_analysis

    def generate_hf_submission_package(self) -> Dict[str, Any]:
        """
        HFæå‡ºç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç”Ÿæˆ
        Generate HF submission package
        """
        logger.info("[HF SUBMISSION] Generating HF submission package...")

        # 1. ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ¯”è¼ƒã‚°ãƒ©ãƒ•
        comparison_plots = self._generate_comparison_plots()

        # 2. ABCãƒ†ã‚¹ãƒˆè©³ç´°ã‚°ãƒ©ãƒ•
        abc_plots = self._generate_abc_test_plots()

        # 3. çµ±è¨ˆçš„æœ‰æ„å·®ã‚°ãƒ©ãƒ•
        significance_plots = self._generate_significance_plots()

        # 4. è¦ç´„çµ±è¨ˆé‡ãƒ†ãƒ¼ãƒ–ãƒ«
        summary_tables = self._generate_summary_tables()

        # 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†å¸ƒã‚°ãƒ©ãƒ•
        distribution_plots = self._generate_distribution_plots()

        # 6. ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆè¤‡æ•°ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¯”è¼ƒï¼‰
        radar_plots = self._generate_radar_plots()

        # 7. ç›¸é–¢åˆ†æ
        correlation_analysis = self._generate_correlation_analysis()

        # 8. å¼·åŒ–çµ±è¨ˆåˆ†æï¼ˆå¤šå¤‰é‡è§£æ + ãƒ™ã‚¤ã‚ºçµ±è¨ˆ + åŠ¹æœé‡ï¼‰
        enhanced_statistical_analysis = self.generate_enhanced_statistical_analysis()

        # 9. READMEã¨çµæœã‚µãƒãƒªãƒ¼
        documentation = self._generate_documentation()

        package = {
            'plots': {
                'comparison': comparison_plots,
                'abc_test': abc_plots,
                'significance': significance_plots,
                'distribution': distribution_plots,
                'radar': radar_plots
            },
            'tables': summary_tables,
            'analysis': {
                'correlation': correlation_analysis,
                'enhanced_statistics': enhanced_statistical_analysis
            },
            'documentation': documentation,
            'metadata': self._generate_metadata()
        }

        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¿å­˜
        self._save_package(package)

        return package

    def _generate_comparison_plots(self) -> Dict[str, str]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ¯”è¼ƒã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        logger.info("[PLOTS] Generating comparison plots with error bars...")

        plots = {}
        df = self._get_results_dataframe()

        if df is None or df.empty:
            return plots

        # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«å¯¾ã—ã¦æ¯”è¼ƒã‚°ãƒ©ãƒ•
        metrics = df['metric'].unique()

        for metric in metrics:
            try:
                metric_data = df[df['metric'] == metric].copy()

                if len(metric_data) == 0:
                    continue

                # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®çµ±è¨ˆé‡è¨ˆç®—
                stats_data = []
                for model in metric_data['model'].unique():
                    model_values = metric_data[metric_data['model'] == model]['value']
                    if len(model_values) > 0:
                        stats_data.append({
                            'model': model,
                            'mean': model_values.mean(),
                            'std': model_values.std(),
                            'sem': stats.sem(model_values) if len(model_values) > 1 else 0,
                            'count': len(model_values)
                        })

                if not stats_data:
                    continue

                stats_df = pd.DataFrame(stats_data)

                # ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ£’ã‚°ãƒ©ãƒ•
                fig, ax = plt.subplots(figsize=(12, 8))

                bars = ax.bar(
                    stats_df['model'],
                    stats_df['mean'],
                    yerr=stats_df['sem'],
                    capsize=5,
                    alpha=0.8,
                    color=sns.color_palette("husl", len(stats_df))
                )

                # å€¤ãƒ©ãƒ™ãƒ«è¿½åŠ 
                for bar, mean_val in zip(bars, stats_df['mean']):
                    height = bar.get_height()
                    ax.text(
                        bar.get_x() + bar.get_width()/2.,
                        height + stats_df['sem'].max() * 0.1,
                        '.3f',
                        ha='center', va='bottom', fontsize=10
                    )

                ax.set_title(f'{metric.replace("_", " ").title()} Comparison\\n(Error bars show standard error of mean)',
                           fontsize=14, fontweight='bold')
                ax.set_xlabel('Model', fontsize=12)
                ax.set_ylabel(metric.replace("_", " ").title(), fontsize=12)
                ax.grid(True, alpha=0.3)

                # çµ±è¨ˆæƒ…å ±ãƒœãƒƒã‚¯ã‚¹
                stats_text = f"n = {stats_df['count'].iloc[0]}\\nSEM shown as error bars"
                ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                       verticalalignment='top', fontsize=10,
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

                plt.xticks(rotation=45)
                plt.tight_layout()

                # ä¿å­˜
                filename = f"comparison_{metric}_errorbars.png"
                filepath = self.output_dir / "plots" / filename
                filepath.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(filepath, dpi=300, bbox_inches='tight')
                plt.close()

                plots[metric] = str(filepath)

            except Exception as e:
                logger.error(f"Failed to generate comparison plot for {metric}: {e}")

        return plots

    def _generate_abc_test_plots(self) -> Dict[str, str]:
        """ABCãƒ†ã‚¹ãƒˆè©³ç´°ã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        logger.info("[PLOTS] Generating ABC test plots...")

        plots = {}
        abc_results = self.results_data.get('comparison', {}).get('abc_test', {})

        if not abc_results or 'winner' not in abc_results:
            return plots

        try:
            # ABCãƒ†ã‚¹ãƒˆå‹è€…ã‚°ãƒ©ãƒ•
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('ABC Test Results: Model Comparison Analysis', fontsize=16, fontweight='bold')

            # å‹è€…æƒ…å ±
            winner = abc_results['winner']

            # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®ABCæ¯”è¼ƒ
            metrics = [k for k in abc_results.keys() if k not in ['winner', 'error'] and isinstance(abc_results[k], dict)]

            for i, metric in enumerate(metrics[:4]):  # æœ€å¤§4ãƒ¡ãƒˆãƒªãƒƒã‚¯
                ax = axes[i // 2, i % 2]

                metric_data = abc_results[metric]
                models = []
                means = []
                stds = []

                for model, stats in metric_data.items():
                    if isinstance(stats, dict):
                        models.append(model.upper())
                        means.append(stats.get('mean', 0))
                        stds.append(stats.get('sem', 0) * 1.96)  # 95% CI

                if models and means:
                    bars = ax.bar(models, means, yerr=stds, capsize=5,
                                 color=sns.color_palette("Set2", len(models)))

                    # å‹è€…ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                    winner_idx = models.index(winner['model'].upper()) if winner['model'].upper() in models else -1
                    if winner_idx >= 0:
                        bars[winner_idx].set_edgecolor('red')
                        bars[winner_idx].set_linewidth(3)

                    ax.set_title(f'{metric.replace("_", " ").title()}\\nABC Test Results')
                    ax.set_ylabel(metric.replace("_", " ").title())
                    ax.grid(True, alpha=0.3)

                    # å€¤ãƒ©ãƒ™ãƒ«
                    for bar, mean_val in zip(bars, means):
                        height = bar.get_height() + bar.get_y() + (stds[bars.index(bar)] if bars.index(bar) < len(stds) else 0)
                        ax.text(bar.get_x() + bar.get_width()/2., height + max(stds) * 0.05,
                               '.3f', ha='center', va='bottom', fontsize=9)

            # å‹è€…ã‚µãƒãƒªãƒ¼
            axes[1, 1].text(0.1, 0.8, f"ğŸ† Winner: {winner['model'].upper()}", fontsize=14, fontweight='bold')
            axes[1, 1].text(0.1, 0.6, f"Score: {winner['score']:.4f}", fontsize=12)
            axes[1, 1].text(0.1, 0.4, f"Metric: {winner['metric']}", fontsize=12)
            axes[1, 1].set_title('ABC Test Winner Summary')
            axes[1, 1].set_xlim(0, 1)
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].axis('off')

            plt.tight_layout()

            # ä¿å­˜
            filename = "abc_test_detailed_analysis.png"
            filepath = self.output_dir / "plots" / filename
            filepath.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()

            plots['abc_detailed'] = str(filepath)

        except Exception as e:
            logger.error(f"Failed to generate ABC test plots: {e}")

        return plots

    def _generate_significance_plots(self) -> Dict[str, str]:
        """çµ±è¨ˆçš„æœ‰æ„å·®ã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        logger.info("[PLOTS] Generating statistical significance plots...")

        plots = {}
        statistical_comp = self.results_data.get('comparison', {}).get('statistical_comparison', {})

        if not statistical_comp:
            return plots

        try:
            # æœ‰æ„å·®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            fig, ax = plt.subplots(figsize=(12, 10))

            # æœ‰æ„å·®ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
            significance_data = []
            model_pairs = []

            for metric, comparisons in statistical_comp.items():
                for comparison_name, results in comparisons.items():
                    if isinstance(results, dict) and 'p_value' in results:
                        model1, model2 = comparison_name.split('_vs_')
                        significance_data.append({
                            'metric': metric,
                            'model1': model1,
                            'model2': model2,
                            'p_value': results['p_value'],
                            'significant': results.get('significant', False)
                        })

            if significance_data:
                sig_df = pd.DataFrame(significance_data)

                # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                pivot_table = sig_df.pivot_table(
                    values='p_value',
                    index=['model1', 'model2'],
                    columns='metric',
                    aggfunc='first'
                ).fillna(1.0)  # æ¬ æå€¤ã¯1.0ï¼ˆæœ‰æ„å·®ãªã—ï¼‰

                # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                sns.heatmap(
                    pivot_table,
                    annot=True,
                    fmt='.3f',
                    cmap='RdYlGn_r',
                    center=0.05,
                    vmin=0,
                    vmax=0.1,
                    ax=ax
                )

                ax.set_title('Statistical Significance Heatmap\\n(p-values for model comparisons)',
                           fontsize=14, fontweight='bold')
                ax.set_xlabel('Metrics')
                ax.set_ylabel('Model Comparisons')

                # æœ‰æ„å·®ã®é–¾å€¤ãƒ©ã‚¤ãƒ³
                ax.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='p=0.05 threshold')

                plt.xticks(rotation=45)
                plt.tight_layout()

                # ä¿å­˜
                filename = "statistical_significance_heatmap.png"
                filepath = self.output_dir / "plots" / filename
                filepath.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(filepath, dpi=300, bbox_inches='tight')
                plt.close()

                plots['significance_heatmap'] = str(filepath)

        except Exception as e:
            logger.error(f"Failed to generate significance plots: {e}")

        return plots

    def _generate_distribution_plots(self) -> Dict[str, str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†å¸ƒã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
        logger.info("[PLOTS] Generating distribution plots...")

        plots = {}
        df = self._get_results_dataframe()

        if df is None or df.empty:
            return plots

        try:
            # å„ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†å¸ƒ
            metrics = df['metric'].unique()

            for metric in metrics:
                metric_data = df[df['metric'] == metric]

                if len(metric_data) == 0:
                    continue

                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                fig.suptitle(f'{metric.replace("_", " ").title()} Performance Distribution Analysis',
                           fontsize=16, fontweight='bold')

                # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + KDE
                for i, model in enumerate(metric_data['model'].unique()):
                    if i >= 4:  # æœ€å¤§4ãƒ¢ãƒ‡ãƒ«
                        break

                    ax = axes[i // 2, i % 2]
                    model_values = metric_data[metric_data['model'] == model]['value']

                    if len(model_values) > 0:
                        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
                        sns.histplot(model_values, kde=True, ax=ax, alpha=0.7,
                                   color=sns.color_palette("husl", len(metric_data['model'].unique()))[i])

                        ax.set_title(f'{model.upper()} Distribution')
                        ax.set_xlabel(metric.replace("_", " ").title())
                        ax.set_ylabel('Frequency')

                        # çµ±è¨ˆæƒ…å ±
                        mean_val = model_values.mean()
                        std_val = model_values.std()
                        ax.axvline(mean_val, color='red', linestyle='--', alpha=0.8,
                                 label=f'Mean: {mean_val:.3f}')
                        ax.legend()

                plt.tight_layout()

                # ä¿å­˜
                filename = f"distribution_{metric}_analysis.png"
                filepath = self.output_dir / "plots" / filename
                filepath.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(filepath, dpi=300, bbox_inches='tight')
                plt.close()

                plots[metric] = str(filepath)

        except Exception as e:
            logger.error(f"Failed to generate distribution plots: {e}")

        return plots

    def _generate_radar_plots(self) -> Dict[str, str]:
        """ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆï¼ˆè¤‡æ•°ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¯”è¼ƒï¼‰"""
        logger.info("[PLOTS] Generating radar plots...")

        plots = {}
        df = self._get_results_dataframe()

        if df is None or df.empty:
            return plots

        try:
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            normalized_scores = self._calculate_normalized_scores(df)

            if normalized_scores:
                fig = plt.figure(figsize=(10, 8))

                # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
                ax = fig.add_subplot(111, polar=True)

                # ã‚«ãƒ†ã‚´ãƒªï¼ˆãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰
                categories = list(normalized_scores.keys())
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                angles += angles[:1]  # é–‰ã˜ã‚‹

                # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒƒãƒˆ
                colors = sns.color_palette("husl", len(normalized_scores[categories[0]]))
                for i, (model, scores) in enumerate(normalized_scores[categories[0]].items()):
                    values = [normalized_scores[cat][model] for cat in categories]
                    values += values[:1]  # é–‰ã˜ã‚‹

                    ax.plot(angles, values, 'o-', linewidth=2, label=model.upper(),
                           color=colors[i], alpha=0.8)
                    ax.fill(angles, values, alpha=0.25, color=colors[i])

                # ãƒ©ãƒ™ãƒ«è¨­å®š
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels([cat.replace('_', ' ').title() for cat in categories])
                ax.set_ylim(0, 1)
                ax.set_title('Model Performance Radar Chart\\n(Normalized Scores)', size=16, fontweight='bold')
                ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
                ax.grid(True, alpha=0.3)

                plt.tight_layout()

                # ä¿å­˜
                filename = "radar_chart_performance_comparison.png"
                filepath = self.output_dir / "plots" / filename
                filepath.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(filepath, dpi=300, bbox_inches='tight')
                plt.close()

                plots['radar_performance'] = str(filepath)

        except Exception as e:
            logger.error(f"Failed to generate radar plots: {e}")

        return plots

    def _calculate_normalized_scores(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """æ­£è¦åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        normalized_scores = {}

        for metric in df['metric'].unique():
            metric_data = df[df['metric'] == metric]
            normalized_scores[metric] = {}

            # ãƒ¡ãƒˆãƒªãƒƒã‚¯å€¤ã®æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
            if len(metric_data) > 0:
                values = metric_data['value'].values
                min_val, max_val = np.min(values), np.max(values)

                if max_val > min_val:
                    for _, row in metric_data.iterrows():
                        model = row['model']
                        value = row['value']
                        normalized = (value - min_val) / (max_val - min_val)
                        normalized_scores[metric][model] = normalized
                else:
                    # å…¨å€¤ãŒåŒã˜å ´åˆ
                    for _, row in metric_data.iterrows():
                        normalized_scores[metric][row['model']] = 1.0

        return normalized_scores

    def _generate_summary_tables(self) -> Dict[str, str]:
        """è¦ç´„çµ±è¨ˆé‡ãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ"""
        logger.info("[TABLES] Generating summary statistics tables...")

        tables = {}
        df = self._get_results_dataframe()

        if df is None or df.empty:
            return tables

        try:
            # å„ãƒ¢ãƒ‡ãƒ«ã®è¦ç´„çµ±è¨ˆé‡
            summary_stats = []

            for model in df['model'].unique():
                model_data = df[df['model'] == model]

                for metric in df['metric'].unique():
                    metric_data = model_data[model_data['metric'] == metric]['value']

                    if len(metric_data) > 0:
                        # çµ±è¨ˆé‡è¨ˆç®—
                        stats = {
                            'Model': model.upper(),
                            'Metric': metric.replace('_', ' ').title(),
                            'Count': len(metric_data),
                            'Mean': '.4f',
                            'Std': '.4f',
                            'Min': '.4f',
                            'Max': '.4f',
                            'Median': '.4f',
                            'Q25': '.4f',
                            'Q75': '.4f',
                            'SEM': '.4f' if len(metric_data) > 1 else 'N/A',
                            'CV': '.4f' if metric_data.mean() != 0 else 'N/A'
                        }
                        summary_stats.append(stats)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            summary_df = pd.DataFrame(summary_stats)

            # CSVä¿å­˜
            csv_filename = "summary_statistics.csv"
            csv_filepath = self.output_dir / "tables" / csv_filename
            csv_filepath.parent.mkdir(parents=True, exist_ok=True)
            summary_df.to_csv(csv_filepath, index=False)

            # LaTeXãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆï¼ˆè«–æ–‡ç”¨ï¼‰
            latex_table = self._generate_latex_table(summary_df)
            latex_filename = "summary_statistics.tex"
            latex_filepath = self.output_dir / "tables" / latex_filename
            with open(latex_filepath, 'w', encoding='utf-8') as f:
                f.write(latex_table)

            tables['summary_csv'] = str(csv_filepath)
            tables['summary_latex'] = str(latex_filepath)

        except Exception as e:
            logger.error(f"Failed to generate summary tables: {e}")

        return tables

    def _generate_latex_table(self, df: pd.DataFrame) -> str:
        """LaTeXãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ"""
        latex = """\\begin{table}[h!]
\\centering
\\caption{Summary Statistics for Model Performance Comparison}
\\label{tab:model_comparison}
\\begin{tabular}{@{}lcccccccccc@{}}
\\toprule
Model & Metric & Count & Mean & Std & Min & Max & Median & Q25 & Q75 & SEM \\\\
\\midrule
"""

        for _, row in df.iterrows():
            latex += f"{row['Model']} & {row['Metric']} & {row['Count']} & {row['Mean']} & {row['Std']} & {row['Min']} & {row['Max']} & {row['Median']} & {row['Q25']} & {row['Q75']} & {row['SEM']} \\\\\n"

        latex += """\\bottomrule
\\end{tabular}
\\end{table}
"""

        return latex

    def _generate_correlation_analysis(self) -> Dict[str, Any]:
        """ç›¸é–¢åˆ†æç”Ÿæˆ"""
        logger.info("[ANALYSIS] Generating correlation analysis...")

        analysis = {}
        df = self._get_results_dataframe()

        if df is None or df.empty:
            return analysis

        try:
            # ãƒ¡ãƒˆãƒªãƒƒã‚¯é–“ã®ç›¸é–¢åˆ†æ
            pivot_df = df.pivot_table(values='value', index=['model', 'library'], columns='metric')
            correlation_matrix = pivot_df.corr()

            analysis['correlation_matrix'] = correlation_matrix.to_dict()

            # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆ
            fig, ax = plt.subplots(figsize=(10, 8))
            sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
                       center=0, ax=ax)
            ax.set_title('Metric Correlation Analysis', fontsize=14, fontweight='bold')
            plt.tight_layout()

            # ä¿å­˜
            filename = "correlation_analysis_heatmap.png"
            filepath = self.output_dir / "analysis" / filename
            filepath.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()

            analysis['correlation_plot'] = str(filepath)

        except Exception as e:
            logger.error(f"Failed to generate correlation analysis: {e}")

        return analysis

    def _generate_documentation(self) -> Dict[str, str]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        logger.info("[DOCUMENTATION] Generating documentation...")

        documentation = {}

        try:
            # READMEç”Ÿæˆ
            readme_content = self._generate_readme()
            readme_filename = "README.md"
            readme_filepath = self.output_dir / readme_filename
            with open(readme_filepath, 'w', encoding='utf-8') as f:
                f.write(readme_content)

            # çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            summary_content = self._generate_results_summary()
            summary_filename = "RESULTS_SUMMARY.md"
            summary_filepath = self.output_dir / summary_filename
            with open(summary_filepath, 'w', encoding='utf-8') as f:
                f.write(summary_content)

            documentation['readme'] = str(readme_filepath)
            documentation['summary'] = str(summary_filepath)

        except Exception as e:
            logger.error(f"Failed to generate documentation: {e}")

        return documentation

    def _generate_readme(self) -> str:
        """READMEç”Ÿæˆ"""
        readme = f"""# LLM Model Comparison Results - HF Submission

This repository contains comprehensive benchmark results and statistical analysis for LLM model comparison, specifically designed for HuggingFace submission.

## Overview

This analysis compares multiple LLM models using various benchmark libraries and provides detailed statistical analysis including error bars, significance testing, and performance distributions.

## Models Compared

"""

        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¿½åŠ 
        df = self._get_results_dataframe()
        if df is not None:
            for model in df['model'].unique():
                readme += f"- **{model.upper()}**: {model} model performance analysis\n"

        readme += """
## Benchmark Libraries Used

- **llama.cpp**: C++ based inference engine for GGUF models
- **lm-evaluation-harness**: EleutherAI's comprehensive evaluation suite
- **LightEval**: HuggingFace's efficient evaluation framework
- **transformers**: HuggingFace transformers benchmark utilities

## Key Results

### ABC Test Winner
"""

        # ABCãƒ†ã‚¹ãƒˆçµæœè¿½åŠ 
        abc_results = self.results_data.get('comparison', {}).get('abc_test', {})
        if 'winner' in abc_results:
            winner = abc_results['winner']
            readme += f"- **Winner**: {winner['model'].upper()}\n"
            readme += f"- **Score**: {winner['score']:.4f}\n"
            readme += f"- **Metric**: {winner['metric']}\n"

        readme += """
## Files Structure

```
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ comparison/           # Error bar comparison plots
â”‚   â”œâ”€â”€ abc_test/            # ABC test detailed analysis
â”‚   â”œâ”€â”€ significance/        # Statistical significance heatmaps
â”‚   â”œâ”€â”€ distribution/        # Performance distribution plots
â”‚   â””â”€â”€ radar/               # Radar charts for multi-metric comparison
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ summary_statistics.csv    # Summary statistics table
â”‚   â””â”€â”€ summary_statistics.tex    # LaTeX table for papers
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ correlation_analysis_heatmap.png
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ RESULTS_SUMMARY.md       # Detailed results summary
```

## Statistical Analysis

### Error Bars
All comparison plots include error bars showing standard error of the mean (SEM) to provide confidence intervals for the performance estimates.

### Significance Testing
- t-tests for comparing model performance across metrics
- p-value heatmaps showing statistical significance
- Bonferroni correction applied for multiple comparisons

### Distribution Analysis
- Performance distributions for each model and metric
- Normality testing using Shapiro-Wilk test
- Outlier detection and analysis

## Usage

### For Researchers
1. Review the comparison plots in `plots/comparison/` for visual analysis
2. Check statistical significance in `plots/significance/`
3. Refer to summary statistics in `tables/summary_statistics.csv`
4. Use LaTeX table in `tables/summary_statistics.tex` for papers

### For Practitioners
1. Check ABC test results for model recommendations
2. Review radar plots for multi-metric performance overview
3. Use correlation analysis to understand metric relationships

## Citation

If you use these results in your research, please cite:

```
@misc{llm-model-comparison-results,
  title={Comprehensive LLM Model Comparison Results},
  author={SO8T Project},
  year={2025},
  url={https://huggingface.co/zapabobouj/llm-model-comparison-results}
}
```

## License

Apache License 2.0
"""

        return readme

    def _generate_results_summary(self) -> str:
        """çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        summary = "# Detailed Results Summary\n\n"

        # ABCãƒ†ã‚¹ãƒˆçµæœ
        abc_results = self.results_data.get('comparison', {}).get('abc_test', {})
        if 'winner' in abc_results:
            winner = abc_results['winner']
            summary += "## ABC Test Results\n\n"
            summary += f"**Winner Model**: {winner['model'].upper()}\n\n"
            summary += f"**Winning Score**: {winner['score']:.4f}\n\n"
            summary += f"**Winning Metric**: {winner['metric']}\n\n"

            # è©³ç´°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            rankings = abc_results.get('model_rankings', {})
            for metric, ranking in rankings.items():
                summary += f"### {metric.replace('_', ' ').title()} Ranking\n\n"
                for i, (model, score) in enumerate(ranking, 1):
                    summary += f"{i}. {model.upper()}: {score:.4f}\n"
                summary += "\n"

        # çµ±è¨ˆçš„æœ‰æ„å·®
        statistical_comp = self.results_data.get('comparison', {}).get('statistical_comparison', {})
        if statistical_comp:
            summary += "## Statistical Significance\n\n"
            for metric, comparisons in statistical_comp.items():
                summary += f"### {metric.replace('_', ' ').title()}\n\n"
                for comparison_name, results in comparisons.items():
                    if isinstance(results, dict) and 'p_value' in results:
                        sig_symbol = "âœ…" if results.get('significant', False) else "âŒ"
                        summary += f"- {comparison_name}: p={results['p_value']:.4f} {sig_symbol}\n"
                summary += "\n"

        # è¦ç´„çµ±è¨ˆé‡
        summary += "## Summary Statistics\n\n"
        df = self._get_results_dataframe()
        if df is not None:
            summary_stats = []
            for model in df['model'].unique():
                model_data = df[df['model'] == model]
                for metric in df['metric'].unique():
                    metric_data = model_data[model_data['metric'] == metric]['value']
                    if len(metric_data) > 0:
                        summary += f"### {model.upper()} - {metric.replace('_', ' ').title()}\n\n"
                        summary += f"- **Mean**: {metric_data.mean():.4f}\n"
                        summary += f"- **Std**: {metric_data.std():.4f}\n"
                        summary += f"- **Min**: {metric_data.min():.4f}\n"
                        summary += f"- **Max**: {metric_data.max():.4f}\n"
                        summary += f"- **Count**: {len(metric_data)}\n\n"

        return summary

    def _generate_metadata(self) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        return {
            'generated_at': pd.Timestamp.now().isoformat(),
            'models_compared': list(self._get_results_dataframe()['model'].unique()) if self._get_results_dataframe() is not None else [],
            'metrics_evaluated': list(self._get_results_dataframe()['metric'].unique()) if self._get_results_dataframe() is not None else [],
            'benchmark_libraries': ['llama_cpp', 'lm_eval', 'light_eval', 'transformers'],
            'statistical_tests': ['t-test', 'mann-whitney-u', 'shapiro-wilk'],
            'visualizations': ['error_bars', 'heatmaps', 'distributions', 'radar_charts'],
            'output_formats': ['png', 'csv', 'tex', 'json']
        }

    def _get_results_dataframe(self) -> Optional[pd.DataFrame]:
        """çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—"""
        try:
            return self.results_data.get('comparison', {}).get('dataframe')
        except:
            return None

    def _save_package(self, package: Dict[str, Any]):
        """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¿å­˜"""
        # JSONå½¢å¼ã§ä¿å­˜
        package_file = self.output_dir / "hf_submission_package.json"
        with open(package_file, 'w', encoding='utf-8') as f:
            json.dump(package, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"[SAVE] HF submission package saved to {package_file}")


def generate_hf_submission_statistics(results_file: str, output_dir: str = "D:/webdataset/results/hf_submission"):
    """
    HFæå‡ºç”¨çµ±è¨ˆå‡¦ç†ã‚’å®Ÿè¡Œ
    Generate HF submission statistics from results file
    """
    logger.info("[HF STATS] Starting HF submission statistics generation...")

    # çµæœãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(results_file, 'r', encoding='utf-8') as f:
        results_data = json.load(f)

    # HFæå‡ºçµ±è¨ˆç”Ÿæˆ
    hf_stats = HFSubmissionStatistics(results_data, output_dir)
    package = hf_stats.generate_hf_submission_package()

    logger.info(f"[HF STATS] HF submission package generated in {output_dir}")
    logger.info("[HF STATS] Files generated:")
    for category, files in package.items():
        if isinstance(files, dict):
            for file_type, filepath in files.items():
                logger.info(f"  - {category}/{file_type}: {filepath}")

    return package


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Generate HF Submission Statistics with Error Bars and Summary Tables"
    )
    parser.add_argument(
        '--results_file',
        required=True,
        help='Path to benchmark results JSON file'
    )
    parser.add_argument(
        '--output_dir',
        default='D:/webdataset/results/hf_submission',
        help='Output directory for HF submission files'
    )

    args = parser.parse_args()

    # HFæå‡ºçµ±è¨ˆç”Ÿæˆ
    package = generate_hf_submission_statistics(args.results_file, args.output_dir)

    logger.info("[SUCCESS] HF submission statistics generated!")
    logger.info(f"Output directory: {args.output_dir}")

    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªé€šçŸ¥
    try:
        import winsound
        winsound.PlaySound(r"C:\Users\downl\Desktop\SO8T\.cursor\marisa_owattaze.wav", winsound.SND_FILENAME)
    except:
        print('\a')


if __name__ == '__main__':
    main()
    audio_file = r"C:\Users\downl\Desktop\SO8T\.cursor\marisa_owattaze.wav"
    if os.path.exists(audio_file):
        try:
            import subprocess
            ps_cmd = f"""
            if (Test-Path '{audio_file}') {{
                Add-Type -AssemblyName System.Windows.Forms
                $player = New-Object System.Media.SoundPlayer '{audio_file}'
                $player.PlaySync()
                Write-Host '[OK] éŸ³å£°é€šçŸ¥é€ä¿¡å®Œäº†' -ForegroundColor Green
            }}
            else {{
                Write-Host '[WARNING] éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“' -ForegroundColor Yellow
            }}
            """
            subprocess.run(["powershell", "-Command", ps_cmd], check=True)
            print('[OK] éŸ³å£°é€šçŸ¥é€ä¿¡å®Œäº†')
        except Exception as e:
            print(f'[ERROR] éŸ³å£°é€šçŸ¥é€ä¿¡å¤±æ•—: {e}')
            print('[WARNING] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ“ãƒ¼ãƒ—éŸ³ã‚’é€ä¿¡ã—ã¾ã™')
            if sys.platform == "win32":
                import winsound
                winsound.Beep(1000, 500)
                print('[OK] ãƒ“ãƒ¼ãƒ—éŸ³é€ä¿¡å®Œäº†')
            else:
                print('[WARNING] ãƒ“ãƒ¼ãƒ—éŸ³é€ä¿¡å¤±æ•—: ã‚·ã‚¹ãƒ†ãƒ ãƒ“ãƒ¼ãƒ—ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“')
                print('[ERROR] éŸ³å£°é€šçŸ¥é€ä¿¡å¤±æ•—: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—ã—ã¾ã—ãŸ'),
                print('[ERROR] éŸ³å£°é€šçŸ¥é€ä¿¡å¤±æ•—: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—ã—ã¾ã—ãŸ'),