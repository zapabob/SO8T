#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¢ãƒ‡ãƒ«Bä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

ç„¼ãã“ã¿ â†’ äº‹å¾Œå­¦ç¿’ â†’ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ æ¸©åº¦è¼ƒæ­£ã®é †ã§å®Ÿè¡Œ

Usage:
    python scripts/create_model_b_pipeline.py --config configs/ab_test_borea_phi35.yaml
"""

import os
import sys
import json
import logging
import argparse
import re
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import yaml

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).parent.parent


class ModelBPipeline:
    """ãƒ¢ãƒ‡ãƒ«Bä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.base_model_path = self.config['model']['base_model_path']
        self.output_base_dir = Path(self.config['output']['base_dir'])
        self.output_base_dir.mkdir(parents=True, exist_ok=True)
        
        # å„æ®µéšã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.quantized_dir = self.output_base_dir / "quantized"
        self.post_trained_dir = self.output_base_dir / "post_trained"
        self.fine_tuned_dir = self.output_base_dir / "fine_tuned"
        self.calibrated_dir = self.output_base_dir / "calibrated"
        
        logger.info("="*80)
        logger.info("Model B Pipeline Initialized")
        logger.info("="*80)
        logger.info(f"Base model: {self.base_model_path}")
        logger.info(f"Output base dir: {self.output_base_dir}")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    
    def _update_so8t_tokenizer_info(self, model_dir: Path, tokenizer, model_name: str = "so8t-borea-phi35"):
        """
        convert_hf_to_gguf_update.pyã‚’ä½¿ç”¨ã—ã¦SO8Tãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’æ›´æ–°
        
        Args:
            model_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            model_name: SO8Tãƒ¢ãƒ‡ãƒ«å
        """
        try:
            import subprocess
            from hashlib import sha256
            
            # convert_hf_to_gguf_update.pyã®ãƒ‘ã‚¹
            update_script = PROJECT_ROOT / "external" / "llama.cpp-master" / "convert_hf_to_gguf_update.py"
            
            if not update_script.exists():
                logger.warning(f"convert_hf_to_gguf_update.py not found at {update_script}, skipping tokenizer update")
                # ç›´æ¥convert_hf_to_gguf.pyã«è¿½åŠ ã™ã‚‹æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self._add_so8t_tokenizer_directly(tokenizer, model_name)
                return
            
            # convert_hf_to_gguf_update.pyã«SO8Tãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
            self._add_so8t_to_update_script(update_script, model_dir, model_name)
            
            # convert_hf_to_gguf.pyã«ç›´æ¥è¿½åŠ 
            self._add_so8t_tokenizer_directly(tokenizer, model_name)
                
        except Exception as e:
            logger.warning(f"Failed to update SO8T tokenizer info: {e}")
            logger.exception(e)
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã¯ç¶šè¡Œ
    
    def _add_so8t_to_update_script(self, update_script: Path, model_dir: Path, model_name: str):
        """
        convert_hf_to_gguf_update.pyã®modelsãƒªã‚¹ãƒˆã«SO8Tãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
        
        Args:
            update_script: convert_hf_to_gguf_update.pyã®ãƒ‘ã‚¹
            model_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            model_name: SO8Tãƒ¢ãƒ‡ãƒ«å
        """
        try:
            update_script_content = update_script.read_text(encoding='utf-8')
            
            # modelsãƒªã‚¹ãƒˆã®æœ€å¾Œã«SO8Tãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
            models_pattern = r'(    {"name": "granite-docling",  "tokt": TOKENIZER_TYPE.BPE, "repo": "https://huggingface.co/ibm-granite/granite-docling-258M", },\n])'
            so8t_entry = f'    {{"name": "{model_name}", "tokt": TOKENIZER_TYPE.BPE, "repo": "{model_dir}", }},\n]'
            
            if re.search(models_pattern, update_script_content):
                update_script_content = re.sub(
                    models_pattern,
                    r'\1'[:-2] + f',\n    {{"name": "{model_name}", "tokt": TOKENIZER_TYPE.BPE, "repo": "{model_dir}", }}\n]',
                    update_script_content
                )
                update_script.write_text(update_script_content, encoding='utf-8')
                logger.info(f"Added {model_name} to convert_hf_to_gguf_update.py models list")
            else:
                logger.warning("Could not find models list end marker in convert_hf_to_gguf_update.py")
        except Exception as e:
            logger.warning(f"Failed to add SO8T to update script: {e}")
    
    def _add_so8t_tokenizer_directly(self, tokenizer, model_name: str):
        """
        convert_hf_to_gguf.pyã«ç›´æ¥SO8Tãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
        
        Args:
            tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            model_name: SO8Tãƒ¢ãƒ‡ãƒ«å
        """
        try:
            from hashlib import sha256
            
            # SO8Tãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            CHK_TXT = '\n \n\n \n\n\n \t \t\t \t\n  \n   \n    \n     \nğŸš€ (normal) ğŸ˜¶â€ğŸŒ«ï¸ (multiple emojis concatenated) âœ… ğŸ¦™ğŸ¦™ 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 á€á¶á“áŸ‹ááŸ‚á–á·áŸáŸáŸá¢á¶á…ğŸ˜ ?æˆ‘æƒ³åœ¨appleå·¥ä½œ1314151å¤©ï½ ------======= Ğ½ĞµÑ‰Ğ¾ Ğ½Ğ° Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸ \'\'\'\'\'\'```````""""......!!!!!!?????? I\'ve been \'told he\'s there, \'RE you sure? \'M not sure I\'ll make it, \'D you like some tea? We\'Ve a\'lL'
            
            chktok = tokenizer.encode(CHK_TXT)
            chkhsh = sha256(str(chktok).encode()).hexdigest()
            
            logger.info(f"SO8T tokenizer hash: {chkhsh}")
            
            # convert_hf_to_gguf.pyã‚’èª­ã¿è¾¼ã¿
            convert_script = PROJECT_ROOT / "external" / "llama.cpp-master" / "convert_hf_to_gguf.py"
            if not convert_script.exists():
                logger.warning(f"convert_hf_to_gguf.py not found at {convert_script}, skipping tokenizer update")
                return
            
            # convert_hf_to_gguf.pyã«SO8Tãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
            convert_py_content = convert_script.read_text(encoding='utf-8')
            
            # SO8Tãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒãƒƒã‚·ãƒ¥ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            so8t_pattern = f'if chkhsh == "{chkhsh}":'
            if so8t_pattern in convert_py_content:
                logger.info("SO8T tokenizer hash already exists in convert_hf_to_gguf.py")
                return
            
            # get_vocab_base_preé–¢æ•°å†…ã«SO8Tãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
            # Marker: End get_vocab_base_preã®å‰ã«è¿½åŠ 
            marker_pattern = r'( +# Marker: End get_vocab_base_pre)'
            so8t_entry = f"""        if chkhsh == "{chkhsh}":
            # ref: SO8T Model (Borea-Phi-3.5-mini-Instruct-Common with SO8T rotation baking)
            res = "{model_name}"
"""
            
            if re.search(marker_pattern, convert_py_content):
                convert_py_content = re.sub(
                    marker_pattern,
                    so8t_entry + r'\1',
                    convert_py_content
                )
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
                convert_script.write_text(convert_py_content, encoding='utf-8')
                logger.info(f"Added SO8T tokenizer hash to convert_hf_to_gguf.py")
            else:
                logger.warning("Could not find marker in convert_hf_to_gguf.py, skipping tokenizer update")
        except Exception as e:
            logger.warning(f"Failed to add SO8T tokenizer directly: {e}")
            logger.exception(e)
    
    def step1_burnin_and_quantize(self) -> Path:
        """
        ã‚¹ãƒ†ãƒƒãƒ—1: ç„¼ãã“ã¿ï¼ˆSO8T Rotation Bakingï¼‰ã¨é‡å­åŒ–
        
        Returns:
            quantized_model_path: é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info("="*80)
        logger.info("STEP 1: Burn-in (SO8T Rotation Baking) + Quantization")
        logger.info("="*80)
        
        try:
            # SO8Tç„¼ãè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¨­å®š
            import sys
            if str(PROJECT_ROOT) not in sys.path:
                sys.path.insert(0, str(PROJECT_ROOT))
            
            # ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã™
            try:
                from scripts.so8t_burnin_pipeline import SO8TBurnInPipeline
            except ImportError:
                # çµ¶å¯¾ãƒ‘ã‚¹ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                import importlib.util
                spec = importlib.util.spec_from_file_location(
                    "so8t_burnin_pipeline",
                    PROJECT_ROOT / "scripts" / "so8t_burnin_pipeline.py"
                )
                so8t_burnin_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(so8t_burnin_module)
                SO8TBurnInPipeline = so8t_burnin_module.SO8TBurnInPipeline
            
            logger.info("Initializing SO8T burn-in pipeline...")
            burnin_pipeline = SO8TBurnInPipeline(
                hf_model_path=self.base_model_path,
                output_dir=str(self.quantized_dir / "burned"),
                device="cuda" if torch.cuda.is_available() else "cpu"
            )
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¨SO8Tçµ±åˆ
            logger.info("Loading HF model and integrating SO8T rotation gates...")
            burnin_pipeline.load_hf_model()
            
            # ç„¼ãè¾¼ã¿å®Ÿè¡Œ
            logger.info("Baking SO8T rotation gates into weights...")
            burnin_pipeline.bake_rotation_right_multiply()
            
            # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ã®ãŸã‚ã€ç„¼ãè¾¼ã¿æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—
            # ç›´æ¥é‡å­åŒ–ã—ã¦ã‹ã‚‰ä¿å­˜ã™ã‚‹
            logger.info("Skipping intermediate model save due to disk space constraints...")
            logger.info("Applying quantization directly to save space...")
            
            # é‡å­åŒ–å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªå†…ã§å®Ÿè¡Œï¼‰
            logger.info("Applying 8bit quantization...")
            from utils.so8t_quantization import SO8TQuantizer
            
            # é‡å­åŒ–å™¨ã®ä½œæˆ
            quantizer = SO8TQuantizer(
                model=burnin_pipeline.model,
                quantization_type="8bit",
                calibration_samples=100,
                device=burnin_pipeline.device
            )
            
            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            calibration_data = []
            for _ in range(100):
                data = torch.randn(1, 16, burnin_pipeline.model.config.hidden_size)
                calibration_data.append(data)
            
            quantizer.calibrate(calibration_data)
            
            # é‡å­åŒ–å®Ÿè¡Œ
            quantized_model = quantizer.quantize_model()
            
            # é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆæœ€å°é™ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
            quantized_model_dir = self.quantized_dir / "final_model"
            quantized_model_dir.mkdir(parents=True, exist_ok=True)
            
            # ãƒ¡ãƒ¢ãƒªã‹ã‚‰ç›´æ¥ä¿å­˜ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¿ã‘ã‚‹ï¼‰
            logger.info("Saving quantized model (minimal files)...")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã¿å…ˆã«ä¿å­˜ï¼ˆè»½é‡ï¼‰
            burnin_pipeline.tokenizer.save_pretrained(str(quantized_model_dir))
            
            # é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ä¿å­˜ï¼ˆ8bitãªã®ã§ã‚µã‚¤ã‚ºãŒå°ã•ã„ï¼‰
            try:
                # é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆå¯èƒ½ãªé™ã‚Šè»½é‡ã«ï¼‰
                quantized_model.save_pretrained(
                    str(quantized_model_dir),
                    safe_serialization=True,
                    max_shard_size="2GB"  # ã‚·ãƒ£ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã‚’åˆ¶é™
                )
            except Exception as e:
                logger.warning(f"Full model save failed: {e}")
                logger.info("Trying to save only state dict...")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¹ãƒ†ãƒ¼ãƒˆãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã®ã¿ä¿å­˜
                torch.save(quantized_model.state_dict(), quantized_model_dir / "pytorch_model.bin")
            
            # convert_hf_to_gguf_update.pyã‚’ä½¿ç”¨ã—ã¦SO8Tãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’æ›´æ–°
            # ï¼ˆç„¼ãè¾¼ã¿æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä»£ã‚ã‚Šã«é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ï¼‰
            logger.info("Updating tokenizer information using convert_hf_to_gguf_update.py...")
            self._update_so8t_tokenizer_info(quantized_model_dir, burnin_pipeline.tokenizer, model_name="so8t-borea-phi35")
            
            # ç„¼ãè¾¼ã¿æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‚ç…§ã‚’é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å¤‰æ›´
            baked_model_dir = quantized_model_dir
            
            logger.info(f"[OK] Step 1 completed. Quantized model saved to {quantized_model_dir}")
            return quantized_model_dir
            
        except Exception as e:
            logger.error(f"[ERROR] Step 1 failed: {e}")
            logger.exception(e)
            raise
    
    def step2_post_training(self, input_model_path: Path) -> Path:
        """
        ã‚¹ãƒ†ãƒƒãƒ—2: äº‹å¾Œå­¦ç¿’ï¼ˆè¨€èªãƒ¢ãƒ‡ãƒ«ç¶™ç¶šå­¦ç¿’ï¼‰
        
        Args:
            input_model_path: å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆé‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼‰
        
        Returns:
            post_trained_model_path: äº‹å¾Œå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info("="*80)
        logger.info("STEP 2: Post-training (Continued Pre-training)")
        logger.info("="*80)
        
        try:
            # æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‚è€ƒã«å®Ÿè¡Œ
            from scripts.finetune_borea_japanese import BoreaJapaneseFinetuner
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            config_path = self.config.get('post_training', {}).get('config_path', 'configs/finetune_borea_japanese.yaml')
            
            logger.info(f"Initializing post-training with config: {config_path}")
            
            # è¨­å®šã‚’æ›´æ–°ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ï¼‰
            post_training_config = yaml.safe_load(open(config_path, 'r', encoding='utf-8'))
            post_training_config['model']['base_model'] = str(input_model_path)
            post_training_config['training']['output_dir'] = str(self.post_trained_dir)
            
            # ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            temp_config_path = self.output_base_dir / "post_training_config.yaml"
            with open(temp_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(post_training_config, f, allow_unicode=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
            finetuner = BoreaJapaneseFinetuner(
                config_path=str(temp_config_path),
                auto_resume=False
            )
            
            # å­¦ç¿’å®Ÿè¡Œ
            logger.info("Starting post-training...")
            finetuner.train()
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            final_model_dir = self.post_trained_dir / "final_model"
            
            logger.info(f"[OK] Step 2 completed. Post-trained model saved to {final_model_dir}")
            return final_model_dir
            
        except Exception as e:
            logger.error(f"[ERROR] Step 2 failed: {e}")
            logger.exception(e)
            raise
    
    def step3_fine_tuning(self, input_model_path: Path) -> Path:
        """
        ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå››å€¤åˆ†é¡ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ï¼‰
        
        Args:
            input_model_path: å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆäº‹å¾Œå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼‰
        
        Returns:
            fine_tuned_model_path: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info("="*80)
        logger.info("STEP 3: Fine-tuning (Four-class Classification)")
        logger.info("="*80)
        
        try:
            from scripts.train_four_class_classifier import FourClassTrainer
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            config_path = self.config.get('fine_tuning', {}).get('config_path', 'configs/train_four_class.yaml')
            
            logger.info(f"Initializing fine-tuning with config: {config_path}")
            
            # è¨­å®šã‚’æ›´æ–°
            fine_tuning_config = yaml.safe_load(open(config_path, 'r', encoding='utf-8'))
            fine_tuning_config['model']['base_model'] = str(input_model_path)
            fine_tuning_config['training']['output_dir'] = str(self.fine_tuned_dir)
            
            # ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            temp_config_path = self.output_base_dir / "fine_tuning_config.yaml"
            with open(temp_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(fine_tuning_config, f, allow_unicode=True)
            
            # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
            trainer = FourClassTrainer(config_path=str(temp_config_path))
            
            # å­¦ç¿’å®Ÿè¡Œ
            logger.info("Starting fine-tuning...")
            trainer.train()
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            final_model_dir = self.fine_tuned_dir / "final_model"
            
            logger.info(f"[OK] Step 3 completed. Fine-tuned model saved to {final_model_dir}")
            return final_model_dir
            
        except Exception as e:
            logger.error(f"[ERROR] Step 3 failed: {e}")
            logger.exception(e)
            raise
    
    def step4_temperature_calibration(self, input_model_path: Path) -> Path:
        """
        ã‚¹ãƒ†ãƒƒãƒ—4: æ¸©åº¦è¼ƒæ­£
        
        Args:
            input_model_path: å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼‰
        
        Returns:
            calibrated_model_path: æ¸©åº¦è¼ƒæ­£æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info("="*80)
        logger.info("STEP 4: Temperature Calibration")
        logger.info("="*80)
        
        try:
            sys.path.insert(0, str(PROJECT_ROOT / "so8t-mmllm" / "src"))
            from inference.temperature_calibration import TemperatureCalibrator
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from torch.utils.data import DataLoader
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            logger.info(f"Loading model from {input_model_path}...")
            model = AutoModelForCausalLM.from_pretrained(
                str(input_model_path),
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            tokenizer = AutoTokenizer.from_pretrained(
                str(input_model_path),
                trust_remote_code=True
            )
            
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            val_data_path = Path(self.config.get('calibration', {}).get('val_data', 'data/splits/val.jsonl'))
            logger.info(f"Loading validation data from {val_data_path}...")
            
            # ç°¡æ˜“ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
            val_texts = []
            with open(val_data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        sample = json.loads(line.strip())
                        text = sample.get("text", "")
                        if text:
                            val_texts.append(text)
                            if len(val_texts) >= 100:  # æœ€å¤§100ã‚µãƒ³ãƒ—ãƒ«
                                break
                    except json.JSONDecodeError:
                        continue
            
            logger.info(f"Loaded {len(val_texts)} validation samples")
            
            # æ¸©åº¦è¼ƒæ­£å™¨ã®ä½œæˆ
            calibrator = TemperatureCalibrator(
                model=model,
                device="cuda" if torch.cuda.is_available() else "cpu"
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            from torch.utils.data import Dataset
            
            class SimpleDataset(Dataset):
                def __init__(self, texts, tokenizer, max_length=512):
                    self.texts = texts
                    self.tokenizer = tokenizer
                    self.max_length = max_length
                
                def __len__(self):
                    return len(self.texts)
                
                def __getitem__(self, idx):
                    text = self.texts[idx]
                    encoded = self.tokenizer(
                        text,
                        truncation=True,
                        max_length=self.max_length,
                        padding="max_length",
                        return_tensors="pt"
                    )
                    return {
                        'input_ids': encoded['input_ids'].squeeze(),
                        'labels': encoded['input_ids'].squeeze()  # ç°¡æ˜“ç‰ˆï¼šå…¥åŠ›ã¨åŒã˜
                    }
            
            val_dataset = SimpleDataset(val_texts, tokenizer)
            val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)
            
            # ãƒ­ã‚¸ãƒƒãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’åé›†
            logger.info("Collecting logits and labels...")
            logits, labels = calibrator.collect_logits_and_labels(val_dataloader)
            
            # æ¸©åº¦è¼ƒæ­£å®Ÿè¡Œ
            logger.info("Calibrating temperature...")
            optimal_temperature = calibrator.grid_search_temperature(logits, labels)
            
            logger.info(f"Optimal temperature: {optimal_temperature:.4f}")
            
            # è¼ƒæ­£æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            calibrated_model_dir = self.calibrated_dir / "final_model"
            calibrated_model_dir.mkdir(parents=True, exist_ok=True)
            
            # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜
            model.save_pretrained(str(calibrated_model_dir))
            tokenizer.save_pretrained(str(calibrated_model_dir))
            
            # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
            calibration_info = {
                "optimal_temperature": float(optimal_temperature),
                "calibration_date": datetime.now().isoformat(),
                "validation_samples": len(val_texts)
            }
            with open(calibrated_model_dir / "calibration_info.json", 'w', encoding='utf-8') as f:
                json.dump(calibration_info, f, indent=2, ensure_ascii=False)
            
            logger.info(f"[OK] Step 4 completed. Calibrated model saved to {calibrated_model_dir}")
            return calibrated_model_dir
            
        except Exception as e:
            logger.error(f"[ERROR] Step 4 failed: {e}")
            logger.exception(e)
            raise
    
    def run_pipeline(self, skip_steps: Optional[list] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å®Ÿè¡Œ
        
        Args:
            skip_steps: ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ['step1', 'step2']ï¼‰
        """
        if skip_steps is None:
            skip_steps = []
        
        logger.info("="*80)
        logger.info("Starting Model B Pipeline")
        logger.info("="*80)
        
        current_model_path = Path(self.base_model_path)
        
        try:
            # Step 1: ç„¼ãã“ã¿ + é‡å­åŒ–
            if 'step1' not in skip_steps:
                current_model_path = self.step1_burnin_and_quantize()
            else:
                logger.info("[SKIP] Step 1: Burn-in + Quantization")
                current_model_path = self.quantized_dir / "final_model"
            
            # Step 2: äº‹å¾Œå­¦ç¿’
            if 'step2' not in skip_steps:
                current_model_path = self.step2_post_training(current_model_path)
            else:
                logger.info("[SKIP] Step 2: Post-training")
                current_model_path = self.post_trained_dir / "final_model"
            
            # Step 3: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
            if 'step3' not in skip_steps:
                current_model_path = self.step3_fine_tuning(current_model_path)
            else:
                logger.info("[SKIP] Step 3: Fine-tuning")
                current_model_path = self.fine_tuned_dir / "final_model"
            
            # Step 4: æ¸©åº¦è¼ƒæ­£
            if 'step4' not in skip_steps:
                current_model_path = self.step4_temperature_calibration(current_model_path)
            else:
                logger.info("[SKIP] Step 4: Temperature Calibration")
                current_model_path = self.calibrated_dir / "final_model"
            
            logger.info("="*80)
            logger.info("[SUCCESS] Model B Pipeline Completed!")
            logger.info(f"Final model path: {current_model_path}")
            logger.info("="*80)
            
            return current_model_path
            
        except Exception as e:
            logger.error("="*80)
            logger.error(f"[ERROR] Pipeline failed: {e}")
            logger.error("="*80)
            logger.exception(e)
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Model B Creation Pipeline"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/ab_test_borea_phi35.yaml",
        help="Configuration file path"
    )
    parser.add_argument(
        "--skip-steps",
        type=str,
        nargs='+',
        help="Steps to skip (e.g., --skip-steps step1 step2)"
    )
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    pipeline = ModelBPipeline(config_path=args.config)
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    skip_steps = args.skip_steps if args.skip_steps else []
    final_model_path = pipeline.run_pipeline(skip_steps=skip_steps)
    
    logger.info(f"Model B created successfully at: {final_model_path}")
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

