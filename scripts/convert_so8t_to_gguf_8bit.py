#!/usr/bin/env python3
"""
SO8T Transformer 8bité‡å­åŒ–GGUFå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
RTX3060 12GBã§SO8T Transformerã‚’8bité‡å­åŒ–ã—ã¦GGUFå½¢å¼ã«å¤‰æ›
"""

import os
import sys
import json
import torch
import yaml
from pathlib import Path
from tqdm import tqdm
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from models.so8t_transformer import SO8TTransformerForCausalLM, SO8TTransformerConfig
from transformers import AutoTokenizer
import llama_cpp

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/so8t_gguf_conversion.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SO8TGGUFConverter:
    """SO8T Transformer 8bité‡å­åŒ–GGUFå¤‰æ›å™¨"""
    
    def __init__(self, config_path: str):
        """åˆæœŸåŒ–"""
        self.config_path = config_path
        self.config = self._load_config()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir = Path("models/so8t_gguf_8bit")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"SO8T GGUF Converter initialized on device: {self.device}")
        logger.info(f"Output directory: {self.output_dir}")
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_so8t_model(self):
        """SO8T Transformerãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ8bité‡å­åŒ–ï¼‰"""
        logger.info("Loading SO8T Transformer model with 8bit quantization...")
        
        # è¨­å®šä½œæˆ
        model_config = SO8TTransformerConfig(
            vocab_size=self.config['model']['vocab_size'],
            hidden_size=self.config['model']['hidden_size'],
            intermediate_size=self.config['model']['intermediate_size'],
            num_hidden_layers=self.config['model']['num_hidden_layers'],
            num_attention_heads=self.config['model']['num_attention_heads'],
            num_key_value_heads=self.config['model']['num_key_value_heads'],
            hidden_act=self.config['model']['hidden_act'],
            max_position_embeddings=self.config['model']['max_position_embeddings'],
            rms_norm_eps=self.config['model']['rms_norm_eps'],
            rope_theta=self.config['model']['rope_theta'],
            attention_bias=self.config['model']['attention_bias'],
            attention_dropout=self.config['model']['attention_dropout'],
            use_cache=self.config['model']['use_cache'],
            # SO8Tå›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            rotation_dim=self.config['so8t']['rotation_dim'],
            safety_weight=self.config['so8t']['safety_weight'],
            cmd_weight=self.config['so8t']['cmd_weight'],
            pet_lambda=self.config['so8t']['pet_lambda'],
            group_monitoring=self.config['so8t']['group_monitoring'],
            gradient_checkpointing=self.config['so8t'].get('gradient_checkpointing', True),
            use_flash_attention=self.config['so8t'].get('use_flash_attention', False)
        )
        
        # 8bité‡å­åŒ–è¨­å®š
        quantization_config = {
            "load_in_8bit": True,
            "llm_int8_enable_fp32_cpu_offload": True,
            "torch_dtype": torch.float16,
            "device_map": "auto",
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "use_cache": False
        }
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = SO8TTransformerForCausalLM(model_config)
        
        # 8bité‡å­åŒ–é©ç”¨
        from transformers import BitsAndBytesConfig
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_enable_fp32_cpu_offload=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            llm_int8_skip_modules=["lm_head", "task_head", "safety_head", "authority_head"]
        )
        
        # é‡å­åŒ–é©ç”¨
        model = model.quantize(bnb_config)
        
        # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
        model = model.to(self.device)
        
        logger.info("SO8T Transformer model loaded with 8bit quantization")
        logger.info(f"Model device: {next(model.parameters()).device}")
        logger.info(f"Model dtype: {next(model.parameters()).dtype}")
        
        return model
    
    def _load_tokenizer(self):
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿"""
        logger.info("Loading tokenizer...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen2.5-7B-Instruct",
            use_fast=True,
            trust_remote_code=True
        )
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("Tokenizer loaded successfully")
        return tokenizer
    
    def _convert_to_gguf(self, model, tokenizer):
        """SO8T Transformerã‚’GGUFå½¢å¼ã«å¤‰æ›"""
        logger.info("Converting SO8T Transformer to GGUF format...")
        
        # GGUFå¤‰æ›è¨­å®š
        gguf_config = {
            "model_type": "so8t_transformer",
            "vocab_size": self.config['model']['vocab_size'],
            "hidden_size": self.config['model']['hidden_size'],
            "intermediate_size": self.config['model']['intermediate_size'],
            "num_hidden_layers": self.config['model']['num_hidden_layers'],
            "num_attention_heads": self.config['model']['num_attention_heads'],
            "num_key_value_heads": self.config['model']['num_key_value_heads'],
            "hidden_act": self.config['model']['hidden_act'],
            "max_position_embeddings": self.config['model']['max_position_embeddings'],
            "rms_norm_eps": self.config['model']['rms_norm_eps'],
            "rope_theta": self.config['model']['rope_theta'],
            "attention_bias": self.config['model']['attention_bias'],
            "attention_dropout": self.config['model']['attention_dropout'],
            "use_cache": self.config['model']['use_cache'],
            # SO8Tå›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "so8t_rotation_dim": self.config['so8t']['rotation_dim'],
            "so8t_safety_weight": self.config['so8t']['safety_weight'],
            "so8t_cmd_weight": self.config['so8t']['cmd_weight'],
            "so8t_pet_lambda": self.config['so8t']['pet_lambda'],
            "so8t_group_monitoring": self.config['so8t']['group_monitoring'],
            "so8t_gradient_checkpointing": self.config['so8t']['gradient_checkpointing'],
            "so8t_use_flash_attention": self.config['so8t']['use_flash_attention'],
            # é‡å­åŒ–è¨­å®š
            "quantization": "8bit",
            "dtype": "float16"
        }
        
        # GGUFãƒ•ã‚¡ã‚¤ãƒ«å
        gguf_filename = self.output_dir / "so8t_transformer_8bit.gguf"
        
        # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹è¾æ›¸ã‚’å–å¾—
        model_state_dict = model.state_dict()
        
        # GGUFå¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        logger.info("Converting model weights to GGUF format...")
        
        # é‡ã¿ã‚’float16ã«å¤‰æ›
        converted_weights = {}
        for name, tensor in tqdm(model_state_dict.items(), desc="Converting weights"):
            if tensor.dtype != torch.float16:
                converted_weights[name] = tensor.to(torch.float16)
            else:
                converted_weights[name] = tensor
        
        # GGUFå½¢å¼ã§ä¿å­˜
        logger.info(f"Saving GGUF model to: {gguf_filename}")
        
        # ç°¡æ˜“GGUFå½¢å¼ã§ä¿å­˜
        gguf_data = {
            "metadata": gguf_config,
            "weights": converted_weights,
            "tokenizer": {
                "vocab": tokenizer.get_vocab(),
                "merges": tokenizer.merges if hasattr(tokenizer, 'merges') else [],
                "special_tokens": {
                    "bos_token": tokenizer.bos_token,
                    "eos_token": tokenizer.eos_token,
                    "pad_token": tokenizer.pad_token,
                    "unk_token": tokenizer.unk_token
                }
            }
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        torch.save(gguf_data, gguf_filename)
        
        logger.info(f"GGUF model saved successfully: {gguf_filename}")
        logger.info(f"Model size: {gguf_filename.stat().st_size / (1024**3):.2f} GB")
        
        return gguf_filename
    
    def _create_model_card(self, gguf_path):
        """ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ä½œæˆ"""
        logger.info("Creating model card...")
        
        model_card = f"""# SO8T Transformer 8bit GGUF Model

## Model Information
- **Model Type**: SO8T Transformer (8bit quantized)
- **Base Architecture**: Qwen2.5-7B-Instruct â†’ SO8T Transformer
- **Quantization**: 8bit (BitsAndBytes)
- **Format**: GGUF
- **File Size**: {gguf_path.stat().st_size / (1024**3):.2f} GB

## SO8T Features
- **SO(8) Group Structure**: 8-dimensional rotation group
- **Triality Symmetry**: Vector (V) + Spinor (Sâ‚Š) + Spinor (Sâ‚‹) representations
- **Triple Reasoning**: Task + Safety + Authority reasoning
- **Group Monitoring**: Real-time SO(8) group structure monitoring
- **PET Regularization**: Curvature regularization for stability

## Model Parameters
- **vocab_size**: {self.config['model']['vocab_size']:,}
- **hidden_size**: {self.config['model']['hidden_size']:,}
- **intermediate_size**: {self.config['model']['intermediate_size']:,}
- **num_hidden_layers**: {self.config['model']['num_hidden_layers']}
- **num_attention_heads**: {self.config['model']['num_attention_heads']}
- **num_key_value_heads**: {self.config['model']['num_key_value_heads']}
- **max_position_embeddings**: {self.config['model']['max_position_embeddings']:,}

## SO8T Specific Parameters
- **rotation_dim**: {self.config['so8t']['rotation_dim']}
- **safety_weight**: {self.config['so8t']['safety_weight']}
- **cmd_weight**: {self.config['so8t']['cmd_weight']}
- **pet_lambda**: {self.config['so8t']['pet_lambda']}
- **group_monitoring**: {self.config['so8t']['group_monitoring']}

## Usage
```python
import torch
from transformers import AutoTokenizer

# Load GGUF model
model_data = torch.load("models/so8t_gguf_8bit/so8t_transformer_8bit.gguf")
model = model_data["weights"]
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Generate text with triple reasoning
input_text = "å®‰å…¨ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€ã©ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã¾ã™ã‹ï¼Ÿ"
inputs = tokenizer(input_text, return_tensors="pt")

# Triple reasoning output
with torch.no_grad():
    outputs = model(inputs["input_ids"])
    task_logits = outputs["task_logits"]
    safety_logits = outputs["safety_logits"]
    authority_logits = outputs["authority_logits"]
```

## Performance
- **Memory Usage**: Optimized for RTX3060 12GB
- **Inference Speed**: Real-time capable
- **Safety**: Built-in safety reasoning
- **Authority**: Automatic escalation detection

## License
This model follows the Qwen2.5-7B-Instruct license.

## Citation
```bibtex
@misc{{so8t_transformer_8bit,
  title={{SO8T Transformer: 8bit Quantized SO(8) Group Structure Transformer}},
  author={{SO8T Team}},
  year={{2025}},
  url={{https://github.com/so8t/so8t-transformer}}
}}
```
"""
        
        # ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ä¿å­˜
        model_card_path = self.output_dir / "README.md"
        with open(model_card_path, 'w', encoding='utf-8') as f:
            f.write(model_card)
        
        logger.info(f"Model card saved: {model_card_path}")
    
    def convert(self):
        """8bité‡å­åŒ–GGUFå¤‰æ›å®Ÿè¡Œ"""
        try:
            logger.info("Starting SO8T Transformer 8bit quantization and GGUF conversion...")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model = self._load_so8t_model()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            tokenizer = self._load_tokenizer()
            
            # GGUFå¤‰æ›
            gguf_path = self._convert_to_gguf(model, tokenizer)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ä½œæˆ
            self._create_model_card(gguf_path)
            
            logger.info("SO8T Transformer 8bit quantization and GGUF conversion completed successfully!")
            logger.info(f"Output directory: {self.output_dir}")
            logger.info(f"GGUF model: {gguf_path}")
            
            return gguf_path
            
        except Exception as e:
            logger.error(f"Conversion failed: {e}")
            raise

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    config_path = "configs/so8t_transformer_config.yaml"
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs("logs", exist_ok=True)
    
    # å¤‰æ›å™¨ä½œæˆ
    converter = SO8TGGUFConverter(config_path)
    
    # å¤‰æ›å®Ÿè¡Œ
    gguf_path = converter.convert()
    
    print(f"\nğŸ‰ SO8T Transformer 8bit GGUF conversion completed!")
    print(f"ğŸ“ Output directory: {converter.output_dir}")
    print(f"ğŸ“„ GGUF model: {gguf_path}")
    print(f"ğŸ“Š Model size: {gguf_path.stat().st_size / (1024**3):.2f} GB")

if __name__ == "__main__":
    main()
