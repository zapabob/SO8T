# SO8T Transformer 8bit量子化GGUF変換設定
# RTX3060 12GBでSO8T Transformerを8bit量子化してGGUF形式に変換

model:
  # Basic model parameters (Qwen2.5-7B-Instruct準拠)
  vocab_size: 152064
  hidden_size: 3584
  intermediate_size: 18944
  num_hidden_layers: 28
  num_attention_heads: 28
  num_key_value_heads: 4
  hidden_act: "silu"
  max_position_embeddings: 32768
  rms_norm_eps: 1e-6
  rope_theta: 1000000.0
  attention_bias: false
  attention_dropout: 0.0
  use_cache: false
  
  # Tokenizer
  tokenizer_name: "Qwen/Qwen2.5-7B-Instruct"

# SO8T specific parameters
so8t:
  rotation_dim: 8
  safety_weight: 0.1
  cmd_weight: 0.9
  pet_lambda: 0.01
  group_monitoring: true
  gradient_checkpointing: true
  use_flash_attention: false

# 8bit量子化設定
quantization:
  load_in_8bit: true
  llm_int8_enable_fp32_cpu_offload: true
  llm_int8_threshold: 6.0
  llm_int8_has_fp16_weight: false
  llm_int8_skip_modules: ["lm_head", "task_head", "safety_head", "authority_head"]
  torch_dtype: "float16"
  device_map: "auto"
  trust_remote_code: true
  low_cpu_mem_usage: true
  use_cache: false

# GGUF変換設定
gguf:
  model_type: "so8t_transformer"
  quantization: "8bit"
  dtype: "float16"
  output_dir: "models/so8t_gguf_8bit"
  filename: "so8t_transformer_8bit.gguf"
  include_metadata: true
  include_tokenizer: true
  include_weights: true

# メモリ最適化設定
memory:
  max_memory: "12GB"
  gpu_memory_fraction: 0.9
  cpu_offload: true
  gradient_checkpointing: true
  use_cache: false

# ログ設定
logging:
  level: "INFO"
  log_file: "logs/so8t_gguf_conversion.log"
  console_output: true
