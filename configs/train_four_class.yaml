# 四値分類学習設定（ALLOW/ESCALATION/DENY/REFUSE）

# モデル設定
model:
  base_model: "./models/Borea-Phi-3.5-mini-Instruct-Jp"
  hidden_size: 3072
  
  # LoRA設定
  lora:
    r: 32                    # LoRAランク（分類タスクなので小さめ）
    lora_alpha: 64           # LoRAスケーリング係数
    target_modules:          # LoRA適用対象
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# データ設定
data:
  # 訓練データ
  train_data: "data/splits/train_four_class.jsonl"

  # 検証データ
  val_data: "data/splits/val_four_class.jsonl"
  
  # データ処理
  max_seq_length: 2048
  preprocessing_num_workers: 4

# 学習設定
training:
  # 基本設定
  output_dir: "D:/webdataset/checkpoints/training/borea_phi35_four_class"
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  
  # 最適化
  learning_rate: 1.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # スケジューラ
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  warmup_steps: 0
  
  # 精度
  fp16: false
  bf16: true
  
  # ログ・保存
  logging_steps: 5
  save_steps: 10  # 10ステップごとにチェックポイント保存
  save_total_limit: 5  # 5個のチェックポイントをローリングストック
  save_strategy: "steps"
  eval_strategy: "steps"
  eval_steps: 10
  
  # その他
  report_to: []
  load_best_model_at_end: true
  metric_for_best_model: "f1_macro"
  greater_is_better: true
  
  # メモリ効率化
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"

# 損失関数設定
loss:
  # Focal Loss設定
  use_focal: true
  focal_alpha: 1.0
  focal_gamma: 2.0
  
  # クラス重み
  class_weights:
    ALLOW: 1.0
    ESCALATION: 3.0
    DENY: 5.0
    REFUSE: 5.0





























