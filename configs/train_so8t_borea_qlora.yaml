# SO8T Borea-Phi-3.5-mini QLoRA学習設定
# RTX3060/32GB最適化

# モデル設定
model:
  base_model: "Borea-Phi-3.5-mini-Instruct-Jp"
  model_type: "phi3"
  
  # SO8T設定
  so8t:
    enabled: true
    init_scale: 0.05
    orthogonal_reg: 1.0e-4
    
  # 量子化設定（QLoRA）
  quantization:
    load_in_8bit: true
    llm_int8_threshold: 6.0
    llm_int8_has_fp16_weight: false
    
  # LoRA設定
  lora:
    r: 64                    # LoRAランク
    lora_alpha: 128          # LoRAスケーリング係数
    target_modules:          # LoRA適用対象
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"

# データ設定
data:
  # 訓練データ
  train_data:
    - "data/web_scraping_cleaned_quadruple.jsonl"
  
  # 検証データ
  val_data: "data/validation_burnin_test_japanese.json"
  
  # データ処理
  max_seq_length: 2048
  preprocessing_num_workers: 4
  
  # 四重推論設定
  quadruple_thinking: true
  use_redacted_tokens: false

# 学習設定
training:
  # 基本設定
  output_dir: "D:/webdataset/checkpoints/training/so8t_borea_phi35"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  
  # 最適化
  learning_rate: 2.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # スケジューラ
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # 精度
  fp16: false
  bf16: true
  tf32: true
  
  # ログ・保存
  logging_steps: 10
  save_steps: 500
  save_total_limit: 5
  save_strategy: "steps"
  evaluation_strategy: "steps"
  eval_steps: 500
  
  # その他
  report_to: ["tensorboard"]
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # メモリ効率化
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  
  # 電源断リカバリー
  resume_from_checkpoint: true
  save_on_each_node: true
  
  # チェックポイント設定
  checkpoint_interval: 300  # 5分

# PET正規化設定
pet:
  enabled: true
  lambda_exploration: 0.01
  lambda_transition: 0.05
  lambda_stabilization: 0.1
  exploration_ratio: 0.2
  transition_ratio: 0.4
  stabilization_ratio: 0.4
  huber_delta: 0.0
  clip_gradient: true
  max_gradient_norm: 1.0
  log_every_n_steps: 100

# ベイズ最適化設定
bayesian_optimization:
  n_trials: 50
  optimize_temperature: true
  optimize_hyperparameters: true
  study_name: "so8t_borea_bayesian"
  
  # 最適化対象
  hyperparameters:
    pet_lambda_range: [0.001, 0.1]
    safety_weight_range: [0.05, 0.2]
    cmd_weight_range: [0.8, 0.95]
  
  temperature:
    range: [0.5, 2.0]
    optimization_method: "ece"  # ECE最小化

# 評価設定
evaluation:
  # メトリクス
  metrics:
    - "perplexity"
    - "loss"
    - "accuracy"
  
  # ドメイン別評価
  domain_evaluation: true
  domains:
    - "defense"
    - "aerospace"
    - "transport"
    - "safety"
  
  # 四重推論評価
  thinking_evaluation: true
  evaluate_thinking_quality: true

# ハードウェア設定
hardware:
  device: "cuda"
  num_gpus: 1
  gpu_memory_limit: 11  # GB (RTX3060 12GBの場合)
  
  # CUDA最適化
  cuda_optimization:
    cudnn_benchmark: true
    cudnn_deterministic: false
    allow_tf32: true

# 安全性設定
safety:
  # 四重推論有効化
  quadruple_thinking_enabled: true
  
  # 判定閾値
  thresholds:
    allow_confidence: 0.8
    deny_confidence: 0.9
    escalation_default: true
  
  # 監査ログ
  audit_logging: true
  audit_db_path: "./database/so8t_compliance.db"

# その他
misc:
  seed: 42
  deterministic: false  # 速度優先
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true



















