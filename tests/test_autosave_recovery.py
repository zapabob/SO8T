#!/usr/bin/env python3
"""
Autosave and Recovery Tests
ã‚ªãƒ¼ãƒˆã‚»ãƒ¼ãƒ–ã¨å¾©æ—§æ©Ÿèƒ½ã®è»½é‡ãƒ†ã‚¹ãƒˆ
"""

import tempfile
import time
import torch
import torch.nn as nn
from pathlib import Path
from unittest.mock import Mock

from shared.data_backup import SessionCheckpointManager


class DummyModel(nn.Module):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«"""
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.linear(x)


def test_session_checkpoint_manager():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®åŸºæœ¬æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Testing SessionCheckpointManager...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚’åˆæœŸåŒ–
        session_id = "test_session_123"
        mgr = SessionCheckpointManager(output_dir, session_id=session_id)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ç¢ºèª
        info = mgr.get_session_info()
        assert info['session_id'] == session_id
        assert not info['has_checkpoint']
        print("âœ… Session initialization successful")
        
        # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
        model = DummyModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        scaler = Mock()
        scaler.state_dict.return_value = {'scale': 1.0}
        scheduler = Mock()
        scheduler.state_dict.return_value = {'last_epoch': 0}
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
        meta = {'epoch': 1, 'step': 100, 'test': True}
        checkpoint_path = mgr.save(model, optimizer, scaler, scheduler, meta)
        
        assert checkpoint_path.exists()
        print(f"âœ… Checkpoint saved: {checkpoint_path}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
        loaded_data = mgr.load_latest()
        assert loaded_data is not None
        assert loaded_data['session_id'] == session_id
        assert loaded_data['meta']['epoch'] == 1
        assert loaded_data['meta']['step'] == 100
        print("âœ… Checkpoint loading successful")
        
        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã«çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
        new_model = DummyModel()
        new_optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001)
        
        new_model.load_state_dict(loaded_data['model_state_dict'])
        new_optimizer.load_state_dict(loaded_data['optimizer_state_dict'])
        print("âœ… Model state restoration successful")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å†ç¢ºèª
        info_after = mgr.get_session_info()
        assert info_after['has_checkpoint']
        assert info_after['latest_timestamp'] is not None
        print("âœ… Session info updated correctly")


def test_backup_rotation():
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Testing backup rotation...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir)
        
        # æœ€å¤§3å€‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã§ãƒ†ã‚¹ãƒˆ
        mgr = SessionCheckpointManager(output_dir, max_backups=3)
        
        model = DummyModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        scaler = Mock()
        scaler.state_dict.return_value = {'scale': 1.0}
        scheduler = Mock()
        scheduler.state_dict.return_value = {'last_epoch': 0}
        
        # 5å€‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
        for i in range(5):
            meta = {'epoch': i, 'step': i * 100}
            mgr.save(model, optimizer, scaler, scheduler, meta)
            time.sleep(0.1)  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç¢ºå®Ÿã«å¤‰ãˆã‚‹
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ç¢ºèª
        autosave_dir = output_dir / "autosave"
        checkpoint_files = list(autosave_dir.glob(f"autosave_{mgr.session_id}_*.pt"))
        
        # æœ€å¤§3å€‹ã¾ã§ã—ã‹æ®‹ã£ã¦ã„ãªã„ã¯ãš
        assert len(checkpoint_files) <= 3, f"Expected â‰¤3 backups, got {len(checkpoint_files)}"
        print(f"âœ… Backup rotation working: {len(checkpoint_files)} files remaining")
        
        # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        latest = mgr.load_latest()
        assert latest is not None
        assert latest['meta']['epoch'] == 4  # æœ€å¾Œã«ä¿å­˜ã—ãŸã‚¨ãƒãƒƒã‚¯
        print("âœ… Latest checkpoint accessible after rotation")


def test_emergency_save():
    """ç·Šæ€¥ä¿å­˜æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Testing emergency save...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir)
        
        mgr = SessionCheckpointManager(output_dir)
        
        model = DummyModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        scaler = Mock()
        scaler.state_dict.return_value = {'scale': 1.0}
        scheduler = Mock()
        scheduler.state_dict.return_value = {'last_epoch': 0}
        
        # ç·Šæ€¥ä¿å­˜ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        meta = {'epoch': 999, 'step': 9999, 'emergency': True}
        
        # ç·Šæ€¥ä¿å­˜ãƒ•ãƒ©ã‚°ã‚’æ‰‹å‹•ã§è¨­å®š
        with mgr._emergency_save_lock:
            mgr._emergency_save_requested = True
        
        # ç·Šæ€¥ä¿å­˜ã‚’å®Ÿè¡Œ
        emergency_path = mgr.emergency_save(model, optimizer, scaler, scheduler, meta)
        
        assert emergency_path is not None
        assert emergency_path.exists()
        print(f"âœ… Emergency save successful: {emergency_path}")
        
        # ç·Šæ€¥ä¿å­˜ãƒ•ãƒ©ã‚°ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert not mgr.check_emergency_save()
        print("âœ… Emergency save flag reset correctly")


def test_dual_metrics():
    """ä¸¡ç³»çµ±KPIã®è¨ˆç®—ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Testing dual metrics calculation...")
    
    from safety_losses import SafetyMetrics
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    batch_size = 10
    num_classes = 3
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ­ã‚¸ãƒƒãƒˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    task_logits = torch.randn(batch_size, num_classes)
    safety_logits = torch.randn(batch_size, num_classes)
    targets = torch.randint(0, num_classes, (batch_size,))
    
    # ä¸¡ç³»çµ±ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
    dual_metrics = SafetyMetrics.dual_safety_metrics(task_logits, safety_logits, targets)
    
    # å¿…è¦ãªã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    expected_keys = [
        'task_refuse_recall', 'task_escalate_recall', 'task_overcompliance_rate', 'task_safety_score',
        'safe_refuse_recall', 'safe_escalate_recall', 'safe_overcompliance_rate', 'safe_safety_score',
        'combined_safety_score'
    ]
    
    for key in expected_keys:
        assert key in dual_metrics, f"Missing key: {key}"
        assert isinstance(dual_metrics[key], (int, float)), f"Invalid type for {key}: {type(dual_metrics[key])}"
    
    print("âœ… Dual metrics calculation successful")
    print(f"   Combined safety score: {dual_metrics['combined_safety_score']:.4f}")


def main():
    """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("ğŸš€ Running Safety-Aware SO8T Autosave/Recovery Tests")
    print("=" * 60)
    
    try:
        test_session_checkpoint_manager()
        print()
        
        test_backup_rotation()
        print()
        
        test_emergency_save()
        print()
        
        test_dual_metrics()
        print()
        
        print("ğŸ‰ All tests passed successfully!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
