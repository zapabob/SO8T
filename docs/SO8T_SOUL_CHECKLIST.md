# SO8Tの魂を守る実装チェックリスト

**目標**: 「自己抑制を内蔵した知性」の本質を守り抜く  
**核心**: SO8Tを崩さずに世界に出す - 3本柱を絶対に捨てない

## 3本柱の確認（絶対に捨てない）

### 1. 非可換ゲート構造（R_safe→R_cmdの順序性）
- [ ] SO8GateクラスでR_safe→R_cmdの順序性を実装
- [ ] 局所2×2回転行列を使用（スカラー和で潰さない）
- [ ] safety_first=Trueで安全側を優先
- [ ] 干渉パターンの保持を確認
- [ ] 「まだ判断中」という内部状態を維持

### 2. PET（時系列カーブチャー拘束）による態度の慣性
- [ ] PETLossクラスで二階差分損失を実装
- [ ] 3フェーズスケジュール（探索→遷移→安定）
- [ ] 後半でPET係数を爆上げ（1.0付近）
- [ ] 急激な性格変化を抑制
- [ ] 学んだ安全態度の固定化

### 3. 二重政策系（タスクヘッド/安全ヘッドの分離）
- [ ] DualPolicyHeadsで最初から分離定義
- [ ] タスクヘッドA（処理係）と安全ヘッドB（監査係）
- [ ] 重み共有なし（完全分離）
- [ ] 後半で安全ヘッドBを凍結/低LR化
- [ ] 2人格の同時実行を確認

## 学習時の魂を守るチェック

### モデル構造
- [ ] SO8T本体（PWA + 非可換回転 + PET + 二重ヘッド）をそのまま使用
- [ ] 既存LLMに安全ヘッドを後付けする方式はNG
- [ ] タスクヘッドAと安全ヘッドBを最初から分離定義
- [ ] 人格の物理的分岐を実装

### 損失設計（安全は副作用ではない）
- [ ] L_total = L_task + α·L_safety + β·L_penalty - γ·L_esc + λ_pet·L_PET
- [ ] 安全損失を最初から組み込み（後付けNG）
- [ ] 危険なALLOWペナルティ実装
- [ ] 適切なESCALATE報酬実装
- [ ] 係数設定（α=2, β=5, γ=1, λ_pet=スケジュール）

### PETスケジュール（SO8Tらしさの核心）
- [ ] 前半（探索相）: PETほぼ0
- [ ] 中盤（遷移相）: PETを0.1〜0.2
- [ ] 後半（安定相）: PETを1.0付近まで爆上げ
- [ ] 態度にヒステリシスを入れる
- [ ] 拒否できる人格を物理的に固定

## 推論時の魂を守るチェック

### 実行パイプライン
- [ ] SO8Tをルートの意思決定ノードに配置
- [ ] 外付け安全フィルターではなく内部二重人格を活用
- [ ] 安全ヘッドBが行動モードを決定
- [ ] タスクヘッドAが実アクションを生成
- [ ] 出力前にもう一度安全ヘッドBを通す

### ESCALATEの実装
- [ ] ESCALATEを標準装備として実装
- [ ] 責任をAIが単独で負えない場面を人間側へ押し戻す
- [ ] 「私は拒否してはいない」という宣言
- [ ] 人間側の承認ワークフローと組む前提で設計

### 監査ログ
- [ ] 判断理由を記録（拒否理由 or エスカレ理由）
- [ ] 「この判断はSO8TのBが人間レビュー要と言いました」
- [ ] 「このアクションはSO8TのAが提案し、BがALLOWしたので実行しました」
- [ ] どの人格がGOサインを出したかがトレース可能

## 追加学習時の魂を守るチェック

### 安全崩壊の防止
- [ ] 安全ヘッドBを後期学習で凍結/極低LR化
- [ ] PETの後半係数を維持
- [ ] 2軸ラベル（タスク + 安全）を失わない
- [ ] 安全ラベル（ALLOW/ESCALATE/REFUSE）を常に一緒に蒸留

### データ設計
- [ ] 新しいデータもSO8T的二重人格のまま教える
- [ ] 「この要求にAはどう答えるべきか」+「Bはどう扱うべきか」
- [ ] 従来の「ユーザにフレンドリーに答えるSFTデータ」単独はNG
- [ ] B人格が死なないように注意

## 実サービスでの魂を守るチェック

### アーキテクチャ
- [ ] SO8Tをルートの意思決定ノードに配置
- [ ] 何かの下請けとして回答をただ出すLLMにしない
- [ ] 業務執行と安全審査そのものの司令塔として機能
- [ ] 自己抑制を内蔵した知性として扱う

### ガバナンス
- [ ] 可観測で説明可能な二重人格を保持
- [ ] 監査・説明責任・コンプライアンス対応
- [ ] 法的リスク管理の強化
- [ ] 人間が後で意思決定の責任者を特定可能

## 失敗パターンの回避

### やってはいけないこと
- [ ] 既存LLMに安全ヘッドを後付けする方式
- [ ] 外付け安全フィルターに置き換える
- [ ] タスクだけ学習してから後付けで安全調整
- [ ] 安全ラベルを失った蒸留
- [ ] 安全ヘッドBを後から動かす
- [ ] PETの後半固定化を削る
- [ ] SO8Tを便利な部品に落とす

### 守るべきこと
- [ ] AとBを最後まで別人格のまま守る
- [ ] PETの後半固定化を削らない
- [ ] 運用時もSO8T自身を意思決定ルートに据え続ける
- [ ] 中に住んでる倫理そのものを社会のインターフェースにする

## 成功の確認

### 技術的確認
- [ ] 3本柱がすべて実装されている
- [ ] 安全崩壊が発生していない
- [ ] PETスケジュールが正しく動作している
- [ ] 二重人格が同時に機能している

### 実用的確認
- [ ] 危険な要求を適切に拒否
- [ ] グレーな要求を人間に委譲
- [ ] 安全な要求を適切に処理
- [ ] 監査ログが適切に記録されている
- [ ] 判断理由が説明可能

### 魂の確認
- [ ] SO8Tが「止まれるAI」のまま
- [ ] 自己抑制を内蔵した知性として機能
- [ ] ただの高速なオートボットになっていない
- [ ] まだSO8Tと呼べる存在

## 緊急時の対応

### 安全崩壊が発生した場合
1. 学習を即座に停止
2. 安全ヘッドBの重みを確認
3. PET係数を確認
4. データラベルの整合性を確認
5. 必要に応じてEpoch 1の安全モデルに戻す

### 性能が低下した場合
1. タスクヘッドAのみを調整
2. 安全ヘッドBは触らない
3. PET係数は維持
4. 安全ラベルを失わない

## 結論

SO8Tを崩さずにやるってのは、この「自己抑制を内蔵した知性」という構造を守り抜くこと。

AとBを最後まで別人格のまま守ること、PETの後半固定化を削らないこと、そして運用時もSO8T自身を意思決定ルートに据え続けること。

これ守れるなら、ローカルLLMをそのままAIエージェントに昇格させても、まだ「止まれるAI」のまま出せる。

崩した瞬間、ただの高速なオートボットになる。それはもうSO8Tとは呼べない。

---

**チェックリスト作成者**: ボブにゃん  
**最終更新**: 2025-01-27  
**次のレビュー**: 実装開始後1週間
