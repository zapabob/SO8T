# GGUF変換の量子化タイプエラー修正実装ログ

## 実装情報
- **日付**: 2025-11-09
- **Worktree**: main
- **機能名**: GGUF変換の量子化タイプエラー修正
- **実装者**: AI Agent

## 実装内容

### 1. 設定ファイルの量子化タイプを修正

**ファイル**: `configs/complete_so8t_ab_test_pipeline_config.yaml` (修正)

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-09  
**備考**: `q4_k_m`を削除し、サポートされている量子化タイプのみを使用

#### 修正内容
- `q4_k_m`を削除
- サポートされている量子化タイプのみを維持（`f16`, `q8_0`）
- コメントでサポートされている量子化タイプを明記

#### 修正前
```yaml
quantizations:
  - "f16"
  - "q8_0"
  - "q4_k_m"
```

#### 修正後
```yaml
quantizations:
  - "f16"
  - "q8_0"
  # Note: q4_k_m is not supported by convert_hf_to_gguf.py
  # Supported types: f32, f16, bf16, q8_0, tq1_0, tq2_0, auto
```

### 2. 量子化タイプの検証機能を追加

**ファイル**: `scripts/pipelines/complete_so8t_ab_test_pipeline.py` (修正)

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-09  
**備考**: サポートされている量子化タイプの検証機能を追加

#### 追加した機能
1. **サポートされている量子化タイプのリスト定義**
   - `self.supported_quantizations = ['f32', 'f16', 'bf16', 'q8_0', 'tq1_0', 'tq2_0', 'auto']`

2. **設定ファイルからの量子化タイプ検証**
   - 設定ファイルから読み込んだ量子化タイプを検証
   - サポートされていない量子化タイプを検出して警告を出力
   - サポートされている量子化タイプのみを使用

3. **実行時の再検証**
   - Phase 1（Model AのGGUF変換）で量子化タイプを再検証
   - Phase 3（Model BのGGUF変換）で量子化タイプを再検証

#### 実装コード
```python
# サポートされている量子化タイプ（convert_hf_to_gguf.pyの仕様に基づく）
self.supported_quantizations = ['f32', 'f16', 'bf16', 'q8_0', 'tq1_0', 'tq2_0', 'auto']

# 設定ファイルから量子化タイプを読み込み
requested_quantizations = gguf_config.get('quantizations', ['f16', 'q8_0'])

# 量子化タイプの検証
valid_quantizations = []
invalid_quantizations = []

for quant_type in requested_quantizations:
    if quant_type in self.supported_quantizations:
        valid_quantizations.append(quant_type)
    else:
        invalid_quantizations.append(quant_type)
        logger.warning(f"[WARNING] Unsupported quantization type '{quant_type}' will be skipped")
        logger.warning(f"[WARNING] Supported types: {', '.join(self.supported_quantizations)}")

if invalid_quantizations:
    logger.warning(f"[WARNING] Skipping {len(invalid_quantizations)} unsupported quantization type(s): {', '.join(invalid_quantizations)}")

if not valid_quantizations:
    raise ValueError(
        f"No valid quantization types found. "
        f"Requested: {requested_quantizations}, "
        f"Supported: {self.supported_quantizations}"
    )

self.quantizations = valid_quantizations
logger.info(f"[INFO] Using quantization types: {', '.join(self.quantizations)}")
```

### 3. エラーハンドリングの改善

**ファイル**: `scripts/pipelines/complete_so8t_ab_test_pipeline.py` (修正)

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-09  
**備考**: エラーハンドリングを改善し、より詳細なエラーメッセージを出力

#### 改善内容
1. **実行前の検証**
   - 量子化タイプの検証を実行前に実施
   - サポートされていない量子化タイプが指定された場合、警告を出力してスキップ

2. **実行時の検証**
   - 各量子化タイプの変換前に再検証
   - サポートされていない量子化タイプが検出された場合、エラーを発生

3. **エラーメッセージの改善**
   - エラー発生時にコマンド文字列を出力
   - サポートされている量子化タイプのリストをエラーメッセージに含める
   - `invalid choice`エラーが検出された場合、サポートされている量子化タイプを明示

#### 実装コード
```python
# 量子化タイプの再検証（念のため）
if quant_type not in self.supported_quantizations:
    logger.error(f"[ERROR] Unsupported quantization type '{quant_type}' detected during conversion")
    logger.error(f"[ERROR] Supported types: {', '.join(self.supported_quantizations)}")
    raise ValueError(f"Unsupported quantization type: {quant_type}")

# ... 変換処理 ...

except subprocess.CalledProcessError as e:
    logger.error(f"[ERROR] Model A GGUF conversion failed for {quant_type}: {e}")
    logger.error(f"Command: {' '.join(cmd)}")
    logger.error(f"stdout: {e.stdout}")
    logger.error(f"stderr: {e.stderr}")
    # エラーメッセージにサポートされている量子化タイプを含める
    if "invalid choice" in e.stderr.lower() or "invalid" in e.stderr.lower():
        logger.error(f"[ERROR] Supported quantization types: {', '.join(self.supported_quantizations)}")
    raise
```

## 作成・変更ファイル
- `configs/complete_so8t_ab_test_pipeline_config.yaml` (修正)
- `scripts/pipelines/complete_so8t_ab_test_pipeline.py` (修正)
- `_docs/2025-11-09_main_GGUF変換の量子化タイプエラー修正実装.md` (新規作成)

## 設計判断

### 量子化タイプの検証タイミング
- **初期化時**: 設定ファイルから読み込んだ量子化タイプを検証
- **実行時**: 各変換処理の前に再検証（念のため）

### エラーハンドリングの方針
- **警告**: サポートされていない量子化タイプが指定された場合、警告を出力してスキップ
- **エラー**: すべての量子化タイプが無効な場合、エラーを発生
- **詳細なエラーメッセージ**: エラー発生時にサポートされている量子化タイプのリストを出力

### サポートされている量子化タイプ
- `f32`: 32ビット浮動小数点（フル精度）
- `f16`: 16ビット浮動小数点
- `bf16`: Brain Float 16
- `q8_0`: 8ビット量子化
- `tq1_0`: 1ビット量子化（試行版）
- `tq2_0`: 2ビット量子化（試行版）
- `auto`: 自動選択

## 使用方法

### 設定ファイルでの量子化タイプ指定
```yaml
gguf:
  convert_script: "external/llama.cpp-master/convert_hf_to_gguf.py"
  quantizations:
    - "f16"
    - "q8_0"
    # Note: q4_k_m is not supported by convert_hf_to_gguf.py
    # Supported types: f32, f16, bf16, q8_0, tq1_0, tq2_0, auto
```

### エラーが発生した場合
1. エラーメッセージを確認
2. サポートされている量子化タイプのリストを確認
3. 設定ファイルを修正して、サポートされている量子化タイプのみを使用

## 技術詳細

### 量子化タイプの検証実装
- 設定ファイルから読み込んだ量子化タイプを検証
- サポートされていない量子化タイプを検出して警告を出力
- サポートされている量子化タイプのみを使用

### エラーハンドリングの実装
- 実行前の検証でサポートされていない量子化タイプを検出
- 実行時の再検証で安全性を確保
- 詳細なエラーメッセージを出力

## 運用注意事項

### データ収集ポリシー
- 利用条件を守りつつ、高信頼ソースとして優先使用
- robots.txt遵守を徹底
- 個人情報・機密情報の除外を徹底

### NSFWコーパス運用
- **主目的**: 安全判定と拒否挙動の学習（生成目的ではない）
- モデル設計とドキュメントに明記
- 分類器は検出・拒否用途のみ

### /thinkエンドポイント運用
- 四重Thinking部（`<think-*>`）は外部非公開を徹底
- `<final>`のみ返す実装を維持
- 監査ログでThinkingハッシュを記録（内容は非公開）

### GGUF変換運用
- **量子化タイプ**: サポートされている量子化タイプのみを使用
- **検証**: 設定ファイルから読み込んだ量子化タイプを検証
- **エラーハンドリング**: サポートされていない量子化タイプが指定された場合、警告を出力してスキップ

## 次のステップ

1. **動作確認**: 実際の環境でGGUF変換が正常に動作するか確認
2. **パフォーマンス最適化**: 量子化タイプの検証のオーバーヘッドを最小化
3. **機能追加**: より詳細な量子化タイプの情報を提供
4. **統合テスト**: 他のパイプラインとの統合テスト

