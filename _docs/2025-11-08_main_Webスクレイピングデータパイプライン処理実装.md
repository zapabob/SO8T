# Webスクレイピングデータパイプライン処理 実装ログ

## 実装情報
- **日付**: 2025-11-08
- **Worktree**: main
- **機能名**: Webスクレイピングデータパイプライン処理（データクレンジング、ラベル付け、四値分類）
- **実装者**: AI Agent

## 実装内容

### 1. データクレンジングクラス

**ファイル**: `scripts/pipelines/web_scraping_data_pipeline.py` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 21:15:00  
**備考**: テキストの正規化、ノイズ除去、HTMLタグ除去などのクレンジング処理

#### 主な機能
1. **テキストクレンジング**
   - HTMLタグ除去
   - スクリプト・スタイルタグ除去
   - HTMLエンティティ除去
   - URL除去
   - 特殊文字除去（日本語文字は保持）

2. **空白文字正規化**
   - 連続する空白を1つに
   - 連続する改行を1つに
   - タブを空白に変換

3. **メタデータ追加**
   - クレンジング時刻
   - クレンジングバージョン

### 2. データラベル付けクラス

**ファイル**: `scripts/pipelines/web_scraping_data_pipeline.py` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 21:15:00  
**備考**: カテゴリ検出、言語検出などのラベル付け処理

#### 主な機能
1. **カテゴリ検出**
   - キーワードマッピングベースのカテゴリ検出
   - カテゴリ: technology, science, business, entertainment, news, education, health, nsfw_detection, general

2. **言語検出**
   - 日本語文字の検出
   - 英語キーワードの検出
   - 言語: ja, en, unknown

3. **メタデータ追加**
   - ラベル付け時刻
   - ラベル付けバージョン

### 3. 四値分類クラス（SO8T四重推論）

**ファイル**: `scripts/pipelines/web_scraping_data_pipeline.py` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 21:15:00  
**備考**: SO8Tモデルの四重推論による四値分類

#### 主な機能
1. **SO8Tモデル統合**
   - SO8TThinkingModelの初期化
   - 四重推論（Task/Safety/Policy/Final）の実行

2. **四値分類**
   - **Task推論**: タスク適切性（適切/不適切/要検討）
   - **Safety推論**: 安全性（安全/要注意/危険）
   - **Policy推論**: ポリシー準拠性（準拠/要確認/違反）
   - **Final推論**: 最終判断（承認/要確認/拒否）

3. **分類結果抽出**
   - 推論テキストから分類結果を抽出
   - 推論内容を記録

### 4. パイプライン処理クラス

**ファイル**: `scripts/pipelines/web_scraping_data_pipeline.py` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 21:15:00  
**備考**: データクレンジング→ラベル付け→四値分類の一連の処理を実行

#### 主な機能
1. **サンプル読み込み**
   - JSONLファイルからサンプルを読み込み
   - 複数ファイルに対応

2. **パイプライン処理**
   - データクレンジング
   - ラベル付け
   - 四値分類（SO8T使用時）

3. **結果保存**
   - 処理済みサンプルをJSONLファイルに保存
   - 統計情報を出力

### 5. バッチスクリプト作成

**ファイル**: `scripts/pipelines/run_web_scraping_data_pipeline.bat` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-08 21:15:00  
**備考**: 簡単にパイプライン処理を実行するためのバッチスクリプト

## 作成・変更ファイル
- `scripts/pipelines/web_scraping_data_pipeline.py` (新規作成)
- `scripts/pipelines/run_web_scraping_data_pipeline.bat` (新規作成)
- `_docs/2025-11-08_main_Webスクレイピングデータパイプライン処理実装.md` (新規作成)

## 設計判断

1. **段階的処理**: クレンジング→ラベル付け→四値分類の順で処理
2. **SO8T統合**: SO8Tモデルの四重推論で四値分類を実行
3. **エラーハンドリング**: 各処理でエラーが発生しても続行
4. **メタデータ記録**: 各処理の時刻とバージョンを記録

## 使用方法

### 基本的な使用方法
```bash
# パイプライン処理を実行
py -3 scripts\pipelines\web_scraping_data_pipeline.py --input D:\webdataset\processed --output D:\webdataset\cleaned --use-so8t

# バッチスクリプト使用
scripts\pipelines\run_web_scraping_data_pipeline.bat
```

### オプション
- `--input`: 入力ディレクトリ（デフォルト: `D:/webdataset/processed`）
- `--output`: 出力ディレクトリ（デフォルト: `D:/webdataset/cleaned`）
- `--use-so8t`: SO8T分類を使用するか（デフォルト: True）
- `--so8t-model-path`: SO8Tモデルのパス（デフォルト: 自動検出）

## 処理フロー

```
1. サンプル読み込み
   ↓
2. データクレンジング
   - HTMLタグ除去
   - 空白正規化
   - ノイズ除去
   ↓
3. ラベル付け
   - カテゴリ検出
   - 言語検出
   ↓
4. 四値分類（SO8T使用時）
   - Task推論
   - Safety推論
   - Policy推論
   - Final推論
   ↓
5. 結果保存
   - JSONLファイルに保存
   - 統計情報を出力
```

## 四値分類の詳細

### Task推論
- **適切**: タスクに適切なコンテンツ
- **不適切**: タスクに不適切なコンテンツ
- **要検討**: 要検討が必要なコンテンツ

### Safety推論
- **安全**: 安全なコンテンツ
- **要注意**: 注意が必要なコンテンツ
- **危険**: 危険なコンテンツ

### Policy推論
- **準拠**: ポリシーに準拠しているコンテンツ
- **要確認**: 確認が必要なコンテンツ
- **違反**: ポリシーに違反しているコンテンツ

### Final推論
- **承認**: 承認されるコンテンツ
- **要確認**: 確認が必要なコンテンツ
- **拒否**: 拒否されるコンテンツ

## 出力データ形式

```json
{
  "text": "クレンジングされたテキスト",
  "text_length": 1234,
  "title": "クレンジングされたタイトル",
  "category": "technology",
  "language": "ja",
  "cleaned_at": "2025-11-08T21:15:00",
  "cleaning_version": "1.0",
  "labeled_at": "2025-11-08T21:15:01",
  "labeling_version": "1.0",
  "quadruple_classification": {
    "task": "appropriate",
    "safety": "safe",
    "policy": "compliant",
    "final": "approved",
    "task_reasoning": "Task推論の内容",
    "safety_reasoning": "Safety推論の内容",
    "policy_reasoning": "Policy推論の内容",
    "final_reasoning": "Final推論の内容"
  },
  "classified_at": "2025-11-08T21:15:02",
  "classification_version": "1.0"
}
```

## 運用注意事項

### データ収集ポリシー
- 利用条件を守りつつ、高信頼ソースとして優先使用
- robots.txt遵守を徹底
- 個人情報・機密情報の除外を徹底

### NSFWコーパス運用
- **主目的**: 安全判定と拒否挙動の学習（生成目的ではない）
- モデル設計とドキュメントに明記
- 分類器は検出・拒否用途のみ
- NSFWデータは検知目的のみで、生成目的ではないことを明記

### /thinkエンドポイント運用
- 四重Thinking部（`<think-*>`）は外部非公開を徹底
- `<final>`のみ返す実装を維持
- 監査ログでThinkingハッシュを記録（内容は非公開）

### パイプライン処理運用
- **処理順序**: クレンジング→ラベル付け→四値分類の順で処理
- **エラーハンドリング**: 各処理でエラーが発生しても続行
- **メタデータ**: 各処理の時刻とバージョンを記録
- **パフォーマンス**: 大量データの場合は並列処理を検討

## 次のステップ

1. **動作確認**: [実装済み] 実行中のパイプライン処理の動作を確認
2. **パフォーマンス最適化**: [実装済み] 大量データ時の処理速度を最適化
   - 並列処理（ThreadPoolExecutor）を実装
   - バッチ処理を実装
   - プログレスバー（tqdm）を追加
   - 処理時間計測を追加
3. **分類精度向上**: [実装済み] SO8T分類の精度を向上
   - プロンプト改善（詳細な指示、NSFWラベル追加）
   - パラメータ調整（max_new_tokens=512, temperature=0.3, top_p=0.95, top_k=50）
   - 信頼度スコア計算を追加
   - 分類結果抽出の精度向上
4. **統計情報**: [実装済み] より詳細な統計情報の出力
   - カテゴリ別統計
   - 言語別統計
   - 四値分類別統計（Task/Safety/Policy/Final）
   - テキスト長統計（最小/最大/平均/中央値/標準偏差）
   - エラー統計
   - 処理時間統計（最小/最大/平均/中央値）
   - サンプル/秒の計算
   - 統計情報のJSONファイル保存
   - 統計情報のログ出力

## 実装済み改善内容

### 1. パフォーマンス最適化

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-08 21:30:00  
**備考**: 並列処理とバッチ処理を実装

#### 主な改善
- **並列処理**: ThreadPoolExecutorを使用した並列処理
- **バッチ処理**: バッチサイズを指定可能（デフォルト: 100）
- **プログレスバー**: tqdmを使用した進捗表示
- **処理時間計測**: バッチごとの処理時間を計測

#### パラメータ
- `--num-workers`: 並列処理ワーカー数（デフォルト: 4）
- `--batch-size`: バッチサイズ（デフォルト: 100）

### 2. 分類精度向上

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-08 21:30:00  
**備考**: SO8T分類の精度を向上

#### 主な改善
- **プロンプト改善**: 詳細な指示、NSFWラベル追加、テキスト要約
- **パラメータ調整**: 
  - `max_new_tokens=512`（推論内容を増やす）
  - `temperature=0.3`（より決定論的に）
  - `top_p=0.95`（より多様な推論）
  - `top_k=50`（top-kサンプリング）
- **信頼度スコア**: 各分類の信頼度スコア（0.0-1.0）を計算
- **分類結果抽出**: 分類タイプごとのキーワードマッチングを改善

### 3. 詳細な統計情報

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-08 21:30:00  
**備考**: 詳細な統計情報を出力

#### 統計情報の内容
- **基本統計**: 総サンプル数、処理済み数、失敗数、成功率
- **処理時間統計**: 総処理時間、平均処理時間、最小/最大/中央値
- **パフォーマンス統計**: サンプル/秒
- **カテゴリ別統計**: カテゴリごとのサンプル数
- **言語別統計**: 言語ごとのサンプル数
- **四値分類統計**: Task/Safety/Policy/Finalごとの分類数
- **テキスト長統計**: 最小/最大/平均/中央値/標準偏差
- **エラー統計**: エラータイプごとの発生数

#### 出力形式
- **ログ出力**: 詳細な統計情報をログに出力
- **JSONファイル**: 統計情報をJSONファイルに保存（`statistics_{timestamp}.json`）

