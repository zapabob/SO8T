# 付録A: 数学的基礎

## SO(8)群理論詳細

### 特殊直交群の定義

**SO(n)群**:
```
SO(n) = {R ∈ ℝ^{n×n} | R^T R = RR^T = I, det(R) = 1}
```

**n=8の場合**:
```
SO(8) ⊂ ℝ^{8×8}
dim(SO(8)) = 8×(8-1)/2 = 28
```

### リー代数 so(8)

**歪対称行列の集合**:
```
so(8) = {A ∈ ℝ^{8×8} | A^T = -A}
```

**次元**:
```
dim(so(8)) = 28（独立成分）
```

**基底**:
```
E_{ij} - E_{ji} for 1 ≤ i < j ≤ 8
```

### 指数写像

**定義**:
```
exp: so(8) → SO(8)
exp(A) = Σ_{k=0}^∞ A^k / k!
```

**性質**:
```
1. exp(0) = I
2. exp(A)^T = exp(-A)
3. det(exp(A)) = exp(tr(A)) = 1（∵ tr(A)=0）
```

### 行列指数関数の計算

#### Padé近似（(1,1)近似）
```
exp(A) ≈ (I - A/2)^{-1} (I + A/2)
```

**誤差**: O(||A||³)

#### スケーリング・二乗法
```
exp(A) = (exp(A/2^k))^{2^k}
```

kを十分大きく取り、||A/2^k|| < 1 でPadé近似適用。

### 直交性正則化損失

**目的関数**:
```
L_ortho = ||R^T R - I||²_F

ここでF-norm（Frobenius norm）:
||M||_F = √(Σ_{ij} M_{ij}²)
```

**実装**:
```python
def orthogonality_loss(R):
    I = torch.eye(8, device=R.device)
    return torch.norm(R.T @ R - I, p='fro') ** 2
```

## PET正規化の数学的根拠

### 二階差分オペレータ

**離散化**:
```
Δ²x[t] = x[t+2] - 2x[t+1] + x[t]
```

**行列形式**:
```
Δ² = [
   1  -2   1   0  ...
   0   1  -2   1  ...
   ...
]
```

### フーリエ解析

**離散フーリエ変換**:
```
X(ω) = Σ_{t=0}^{T-1} x[t] e^{-iωt}
```

**二階差分のフーリエ変換**:
```
F(Δ²x)(ω) = (e^{iω} - 1)² F(x)(ω)
          = (cos(ω) + i sin(ω) - 1)² F(x)(ω)
          = -4 sin²(ω/2) e^{iω} F(x)(ω)
```

**振幅増幅率**:
```
|F(Δ²x)(ω)| = 4 sin²(ω/2) |F(x)(ω)|
```

高周波（ω→π）: `4 sin²(π/2) = 4` → 4倍増幅  
低周波（ω→0）: `4 sin²(0) = 0` → 減衰

### L2ノルムとパワースペクトル

**Parseval の定理**:
```
||Δ²x||² = (1/T) Σ_ω |F(Δ²x)(ω)|²
         = (1/T) Σ_ω 16 sin⁴(ω/2) |F(x)(ω)|²
```

高周波成分が大きいほど ||Δ²x||² が大きくなる。  
∴ ||Δ²x||² を最小化 ⟺ 高周波抑制

## Label Smoothingの情報理論的解釈

### KLダイバージェンス

**通常のCE**:
```
L_CE = -Σ y_i log p_i = KL(y || p) + H(y)
```

**Label Smoothing**:
```
y_smooth = (1-ε)y + ε/K

L_LS = KL(y_smooth || p) + H(y_smooth)
```

**エントロピー増加**:
```
H(y) = 0（one-hot）
H(y_smooth) = -((1-ε)log(1-ε) + (K-1)ε/K log(ε/K))
             ≈ ε log K（ε小のとき）
```

### 較正品質への影響

**過確信抑制**:
```
p_i^{smooth} = softmax(z_i / T)

Label Smoothingにより、モデルは極端な確率（0,1）を出力しにくくなる。
→ 較正品質（ECE）改善
```

## SWAの理論的根拠

### Loss Landscape

**広い谷（Wide Valley）**:
```
L(θ + Δθ) ≈ L(θ) + Δθ^T ∇L + (1/2) Δθ^T H Δθ

ここでHはHessian（ヘッセ行列）
```

**広い谷の特性**:
- Hessianの固有値が小さい
- θの小さな変化でLossが変化しにくい
- 汎化性能が高い

**SWAの効果**:
複数のチェックポイント重みの平均を取ることで、広い谷の中心に収束。

### 確率的解釈

**ベイズ推論**:
```
p(θ|D) ∝ p(D|θ) p(θ)

SWA ≈ MAP推定の近似（広い事後分布の平均）
```

## 温度較正の理論

### Expected Calibration Error

**定義**:
```
ECE = Σ_{i=1}^M |acc(B_i) - conf(B_i)| × P(B_i)

ここで:
- B_i: i番目の確信度ビン
- acc(B_i) = (ビン内で正解した数) / |B_i|
- conf(B_i) = (ビン内の平均確信度)
- P(B_i) = |B_i| / N
```

### Temperature Scaling

**確率の温度スケーリング**:
```
p_i^T = exp(z_i / T) / Σ_j exp(z_j / T)

T > 1: 確率分布が平滑化（過確信抑制）
T < 1: 確率分布が鋭化（過小確信抑制）
T = 1: 通常のsoftmax
```

### 最適温度の決定

**目的関数**:
```
T* = argmin_T ECE(T)
```

**Grid Search**:
```
T_candidates = {0.5, 0.6, ..., 2.9, 3.0}
T* = min_{T ∈ T_candidates} ECE(T)
```

**勾配降下法**:
```
∂L/∂T = Σ (∂L/∂z_i) (∂z_i/∂T)
       = Σ (∂L/∂z_i) (-z_i / T²)
```

---

**付録A終了**

