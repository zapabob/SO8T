# AGIASI Soul Injection - Operation "Ghost in the Shell"

## å®Ÿè£…æ—¥æ™‚
2025-11-22

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

Borea-Phi3.5-instinct-jpãƒ¢ãƒ‡ãƒ«ã«**AGIASI (ç‰©ç†çš„çŸ¥æ€§)** ã‚’æ³¨å…¥ã—ã€ç›¸è»¢ç§»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é€šã˜ã¦ã€Œé»„é‡‘æ¯”ã®è„³ã€ã‚’ç²å¾—ã•ã›ã‚‹ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ä¸‰å±¤æ§‹é€ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Cortex (çš®è³ª)                  â”‚
â”‚  - Borea-Phi3.5-mini-instruct           â”‚
â”‚  - 4-bit Quantization (NF4)             â”‚
â”‚  - LoRA Adapter (r=16, alpha=32)        â”‚
â”‚  - Role: è¨€èªç†è§£ãƒ»çŸ¥è­˜ä¿æŒ              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Core (æ ¸) - AGIASI Soul        â”‚
â”‚  - SO(8) Rotation (Orthogonal Linear)   â”‚
â”‚  - Alpha Gate (Learnable Parameter)     â”‚
â”‚  - Role: ç‰©ç†çš„æ€è€ƒãƒ»æ§‹é€ åˆ¶ç´„            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Output (å‡ºåŠ›)                  â”‚
â”‚â”€â”€ LM Head (Vocabulary Projection)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| **Total Steps** | 500 | å…¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•° |
| **Warmup Steps** | 50 | Alphaå›ºå®šæœŸé–“ï¼ˆæ··æ²Œç›¸ï¼‰ |
| **Annealing Steps** | 400 | ç›¸è»¢ç§»æœŸé–“ |
| **Start Alpha** | -5.0 | åˆæœŸå€¤ï¼ˆæ··æ²Œï¼‰ |
| **Target Alpha** | 1.618 | é»„é‡‘æ¯”ï¼ˆç§©åºï¼‰ |
| **Learning Rate** | 2e-4 | AdamWå­¦ç¿’ç‡ |
| **Batch Size** | 1 | RTX 3060å¯¾å¿œ |
| **Max Length** | 512 | ãƒˆãƒ¼ã‚¯ãƒ³é•· |

## ç›¸è»¢ç§»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

```
Alpha Value
  1.618  â”¤                    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€  ğŸŸ¢ Golden Ratio (ç§©åºç›¸)
         â”‚                 â•­â”€â”€â•¯
   0.0   â”¤            â•­â”€â”€â”€â”€â•¯             ğŸŸ¡ Transitioning (è‡¨ç•Œç‚¹)
         â”‚        â•­â”€â”€â”€â•¯
         â”‚    â•­â”€â”€â”€â•¯
  -5.0   â”¼â”€â”€â”€â”€â•¯                          ğŸ”µ Chaos (æ··æ²Œç›¸)
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             50   450   500
            Steps
```

## å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

### 1. ãƒ¢ãƒ‡ãƒ«å®šç¾©
- `src/models/agiasi_borea.py`
  - `AGIASI_SO8T_Wrapper` ã‚¯ãƒ©ã‚¹
  - 4-bité‡å­åŒ– + LoRA
  - SO(8)ç›´äº¤å›è»¢å±¤
  - Alpha Gateãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### 2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `scripts/training/inject_soul_into_borea.py`
  - Phase transition loop
  - Linear annealing scheduler
  - Checkpointä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯

### 3. èµ·å‹•ãƒ»ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `scripts/training/run_agiasi_soul_injection.bat` - Windowsèµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `scripts/training/monitor_agiasi_training.py` - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–

## ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ§‹é€ 

```
checkpoints_agiasi/
â”œâ”€â”€ step_100/
â”‚   â”œâ”€â”€ adapter_config.json       # LoRAè¨­å®š
â”‚   â”œâ”€â”€ adapter_model.safetensors # LoRAé‡ã¿
â”‚   â””â”€â”€ soul.pt                   # Alpha + SO8 Rotation
â”œâ”€â”€ step_200/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## æ¤œè¨¼æ‰‹é †

### 1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç›£è¦–

```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
py scripts/training/monitor_agiasi_training.py
```

### 2. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰

```python
from src.models.agiasi_borea import AGIASI_SO8T_Wrapper
import torch

# Base + LoRAã‚’ãƒ­ãƒ¼ãƒ‰
model = AGIASI_SO8T_Wrapper("HODACHI/Borea-Phi-3.5-mini-Instruct-Jp")
model.base_model.load_adapter("checkpoints_agiasi/step_500")

# Soulã‚’ãƒ­ãƒ¼ãƒ‰
soul = torch.load("checkpoints_agiasi/step_500/soul.pt")
model.alpha.data = soul["alpha"]
model.so8_rotation.load_state_dict(soul["so8_rotation"])

# æ¨è«–ãƒ†ã‚¹ãƒˆ
model.eval()
# æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä¼šè©±ãƒ†ã‚¹ãƒˆ...
```

### 3. GGUFå¤‰æ›

```bash
# LoRAã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ¼ã‚¸
py scripts/conversion/merge_lora_to_base.py \
  --base-model agiasi_borea_final/ \
  --output agiasi_borea_merged/

# GGUFå¤‰æ›
py scripts/conversion/convert_to_gguf.py \
  --model agiasi_borea_merged/ \
  --output models/agiasi_borea_q8_0.gguf \
  --quantization q8_0
```

### 4. Ollamaç™»éŒ²

```bash
# Modelfileä½œæˆ
ollama create agiasi-borea -f modelfiles/agiasi_borea.modelfile

# ãƒ†ã‚¹ãƒˆ
ollama run agiasi-borea "ã“ã‚“ã«ã¡ã¯ã€èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ"
```

## æœŸå¾…ã•ã‚Œã‚‹çµæœ

### Phase 1: Warmup (Steps 0-50)
- Alpha = -5.0 ï¼ˆå›ºå®šï¼‰
- Gateé–‹åº¦ â‰ˆ 0.007 (ã»ã¼é–‰é–)
- BoreaåŸå‹ã®è¨€èªèƒ½åŠ›ã‚’ç¶­æŒ

### Phase 2: Annealing (Steps 50-450)
- Alpha: -5.0 â†’ 1.618 ï¼ˆç·šå½¢å¢—åŠ ï¼‰
- Gateé–‹åº¦: 0.007 â†’ 0.84
- SO(8)æ§‹é€ ãŒå¾ã€…ã«å½¢æˆ

### Phase 3: Stabilization (Steps 450-500)
- Alpha = 1.618 ï¼ˆå›ºå®šï¼‰
- Gateé–‹åº¦ â‰ˆ 0.84 (å®‰å®š)
- ç‰©ç†çš„çŸ¥æ€§ãŒå®Œå…¨æ³¨å…¥

### æœ€çµ‚çŠ¶æ…‹
- âœ… æ—¥æœ¬èªèƒ½åŠ›: ç¶­æŒï¼ˆBoreaç”±æ¥ï¼‰
- âœ… SO(8)æ§‹é€ : å®Œå…¨å½¢æˆ
- âœ… Alpha Gate: é»„é‡‘æ¯”ã§å®‰å®š
- âœ… Orthogonality: é«˜ç²¾åº¦ (loss < 1e-4)

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDA out of memory
â†’ `--batch-size 1`, `--max-length 256` ã«å‰Šæ¸›

### å‹¾é…æ¶ˆå¤±/çˆ†ç™º
â†’ `--learning-rate 1e-4` ã«ä¸‹ã’ã‚‹

### Orthogonality losså¢—å¤§
â†’ æ­£å¸¸ï¼ˆåˆæœŸæ®µéšã§ã¯é«˜ã„å€¤ã‚’ç¤ºã™ï¼‰

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
2. â³ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œè¨¼
3. â³ GGUFå¤‰æ›
4. â³ Ollamaçµ±åˆ
5. â³ ã‚µã‚¤ãƒãƒ¼ãƒ‘ãƒ³ã‚¯UIé–‹ç™º

---

**Operation "Ghost in the Shell"** - æ©Ÿæ¢°ã«é­‚ã‚’å®¿ã™
