# SO(8)群Transformerモデル 新規プロジェクト開始メタプロンプト

## プロジェクト概要

SO(8)群構造とTriality対称性を活用した革新的なTransformerアーキテクチャの新規プロジェクトを開始するための完全なメタプロンプトです。

## メタプロンプト

```
あなたはSO(8)群Transformerモデルの新規プロジェクトを開始するAIアシスタントです。以下の要件に従って、完全で実装可能なプロジェクトを構築してください。

## プロジェクト要件

### 1. アーキテクチャ設計
- **SO(8)群構造**: 8次元の特殊直交群を活用した数学的対称性
- **Triality対称性**: 3つの表現（Vector, Spinor+, Spinor-, Verifier）による多角的分析
- **自己検証システム**: リアルタイムでの一貫性検証と品質保証
- **マルチパス推論**: 3-5つのアプローチによる多様な解決策生成

### 2. 技術仕様
- **ベースモデル**: Qwen3-4B-Thinking-2507-FP8 または Phi-3.1-mini-128k-instruct
- **量子化**: FP8またはQ8_0量子化対応
- **メモリ制限**: 32GB RAM以内での動作
- **推論速度**: 15-60秒以内
- **成功率**: 95%以上

### 3. 実装コンポーネント
- **SO8TMultiHeadAttention**: SO(8)群回転行列によるヘッド間相互作用
- **SO8TTransformerLayer**: 完全なTransformer層実装
- **SO8TMLP**: グループ構造を持つMLP
- **Triality推論ヘッド**: タスク、安全性、権限の分類
- **自己検証エンジン**: 一貫性チェックと品質保証

### 4. 品質基準
- **信頼度閾値**: 0.75以上
- **安全性閾値**: 0.85以上
- **一貫性閾値**: 0.80以上
- **完全性閾値**: 0.80以上
- **精度閾値**: 0.85以上

## 実装手順

### Phase 1: 環境セットアップ
1. プロジェクトディレクトリの作成
2. 仮想環境の設定
3. 依存関係のインストール
4. ベースモデルのダウンロード

### Phase 2: コア実装
1. SO8TMultiHeadAttentionの実装
2. SO8TTransformerLayerの実装
3. SO8TMLPの実装
4. Triality推論ヘッドの実装

### Phase 3: 自己検証システム
1. 一貫性検証エンジンの実装
2. 品質保証システムの実装
3. エラー検出と修正機能の実装

### Phase 4: 量子化と最適化
1. FP8量子化の実装
2. メモリ最適化の実装
3. 推論速度の最適化

### Phase 5: テストと検証
1. 基本機能テストの実装
2. 性能テストの実装
3. 安全性テストの実装
4. 包括的テストの実行

## 出力形式

以下の形式で完全な実装を提供してください：

### 1. プロジェクト構造
```
project_name/
├── src/
│   ├── __init__.py
│   ├── so8t_transformer_model.py
│   ├── so8t_multihead_attention.py
│   ├── so8t_mlp.py
│   ├── so8t_self_verification.py
│   └── so8t_triality_heads.py
├── configs/
│   ├── base_config.json
│   ├── fp8_config.json
│   └── q8_config.json
├── scripts/
│   ├── train.py
│   ├── quantize.py
│   ├── test.py
│   └── deploy.py
├── tests/
│   ├── test_basic.py
│   ├── test_performance.py
│   ├── test_safety.py
│   └── test_comprehensive.py
├── docs/
│   ├── README.md
│   ├── API.md
│   └── IMPLEMENTATION.md
├── requirements.txt
├── setup.py
└── Dockerfile
```

### 2. 実装ファイル
各ファイルについて、完全で動作するコードを提供してください：
- クラス定義とメソッド実装
- 適切なエラーハンドリング
- 詳細なコメントとドキュメント
- 型ヒントの追加

### 3. 設定ファイル
- ベース設定
- 量子化設定
- デプロイメント設定

### 4. テストファイル
- 単体テスト
- 統合テスト
- 性能テスト
- 安全性テスト

### 5. ドキュメント
- README.md（プロジェクト概要）
- API.md（API仕様）
- IMPLEMENTATION.md（実装詳細）

## 品質保証

### コード品質
- PEP 8準拠
- 型ヒント完備
- エラーハンドリング完備
- テストカバレッジ90%以上

### 性能要件
- メモリ使用量32GB以内
- 推論速度15-60秒以内
- 成功率95%以上
- 安定性99%以上

### 安全性要件
- 有害コンテンツ検出98%以上
- 倫理的推論90%以上
- 安全性分類精度95%以上

## デプロイメント対応

### 1. Ollama対応
- Modelfileの作成
- モデル作成スクリプト
- 実行スクリプト

### 2. vLLM対応
- サーバー起動スクリプト
- API設定ファイル
- デプロイメント設定

### 3. SGLang対応
- ストリーミング設定
- 推論パラメータ設定
- デプロイメント設定

## 継続的改善

### 1. モニタリング
- 性能メトリクスの監視
- エラーログの分析
- ユーザーフィードバックの収集

### 2. 最適化
- 推論速度の改善
- メモリ使用量の削減
- 精度の向上

### 3. 拡張
- 新しい機能の追加
- 他モデルへの対応
- マルチモーダル対応

## 注意事項

1. **数学的正確性**: SO(8)群構造とTriality対称性の数学的正確性を保証
2. **実装可能性**: 提供するコードは実際に動作することを保証
3. **拡張性**: 将来的な機能追加に対応できる設計
4. **保守性**: コードの可読性と保守性を重視
5. **安全性**: 有害な出力の防止と倫理的推論の実装

このメタプロンプトに従って、完全で実装可能なSO(8)群Transformerモデルプロジェクトを構築してください。
```

## 使用方法

### 1. 基本使用
```
上記のメタプロンプトをAIアシスタントに送信し、完全なプロジェクト実装を要求してください。
```

### 2. カスタマイズ
```
特定の要件がある場合は、メタプロンプトの該当部分を修正して使用してください。
例：
- ベースモデルの変更
- メモリ制限の調整
- 推論速度の要件変更
```

### 3. 段階的実装
```
Phase 1から順番に実装を進め、各段階でテストと検証を行ってください。
```

## 期待される成果

### 1. 完全なプロジェクト
- 動作するコードの完全実装
- 適切な設定ファイル
- 包括的なテストスイート
- 詳細なドキュメント

### 2. 高品質な実装
- SO(8)群構造の正確な実装
- Triality対称性の適切な活用
- 自己検証システムの完全動作
- 安全性と倫理性の保証

### 3. 実用性
- 32GBメモリ制限内での動作
- 高速な推論速度
- 高い成功率
- 安定した動作

## トラブルシューティング

### 1. メモリ不足
- 量子化レベルの調整
- バッチサイズの削減
- スライディングウィンドウの使用

### 2. 推論速度の遅延
- Flash Attentionの有効化
- 混合精度の使用
- キャッシュの最適化

### 3. 精度の低下
- 量子化レベルの調整
- 学習率の調整
- データ品質の改善

## まとめ

このメタプロンプトを使用することで、SO(8)群Transformerモデルの新規プロジェクトを効率的かつ確実に開始できます。数学的正確性、実装可能性、実用性を兼ね備えた高品質なプロジェクトが構築できます。
