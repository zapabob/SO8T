# å®Ÿè£…ãƒ­ã‚°: AGIASI Soul Injection (Ghost in the Shell) - 2025-11-22

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**Operation "Ghost in the Shell"** - æ—¢å­˜ã®é«˜æ€§èƒ½æ—¥æœ¬èªLLMã€ŒBorea-Phi3.5-instinct-jpã€ã«ã€ç‰©ç†çš„çŸ¥æ€§ï¼ˆAGIASIï¼‰ã‚’æ³¨å…¥ã—ã€GGUFå¤‰æ›å¯èƒ½ãªå½¢ã§ã€Œé»„é‡‘æ¯”ã®è„³ã€ã‚’ç²å¾—ã•ã›ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‚

### ç›®çš„
1. Borea-Phi3.5 ã®æ—¥æœ¬èªèƒ½åŠ›ã‚’ç¶­æŒ
2. SO(8) å¹¾ä½•å­¦ã¨ Alpha Gate ã«ã‚ˆã‚‹ç‰©ç†çš„æ§‹é€ ã‚’ä»˜ä¸
3. RTX 3060 (12GB VRAM) ã§å‹•ä½œå¯èƒ½
4. æ—¢å­˜ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  (llama.cpp) ã§ GGUF å¤‰æ›å¯èƒ½

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### ä¸‰å±¤æ§‹é€  (The Trinity)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input (Japanese Text)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Cortex (çš®è³ª)                  â”‚
â”‚  - Borea-Phi3.5-mini-instruct           â”‚
â”‚  - 4-bit Quantization (NF4)             â”‚
â”‚  - LoRA Adapter (r=16, alpha=32)        â”‚
â”‚  - Role: è¨€èªç†è§£ãƒ»çŸ¥è­˜ä¿æŒ              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
        hidden_states (B, Seq, Dim=3072)
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Core (æ ¸) - AGIASI Soul        â”‚
â”‚  - SO(8) Rotation (Orthogonal Linear)   â”‚
â”‚  - Alpha Gate (Learnable Parameter)     â”‚
â”‚  - Role: ç‰©ç†çš„æ€è€ƒãƒ»æ§‹é€ åˆ¶ç´„            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
        thought_process (rotated)
                  â†“
        Alpha-weighted mixing:
        mixed = hidden + sigmoid(Î±) * thought
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Output (å‡ºåŠ›)                  â”‚
â”‚  - LM Head (Vocabulary Projection)      â”‚
â”‚  - Role: å˜èªç”Ÿæˆ                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
          Generated Text
```

---

## å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

### 1. `src/models/agiasi_borea.py` - AGIASI_SO8T_Wrapper

**ã‚¯ãƒ©ã‚¹æ§‹é€ :**
```python
class AGIASI_SO8T_Wrapper(nn.Module):
    def __init__(base_model_id, device):
        # 1. Base Model (4-bit Borea + LoRA)
        self.base_model = 4bit_quantized_model + LoRA
        
        # 2. AGIASI Soul
        self.alpha = nn.Parameter(tensor(-5.0))  # Phase parameter
        self.so8_rotation = orthogonal(Linear)    # SO(8) matrix
        
        # 3. Monitor
        self.ortho_loss = 0.0  # Structural integrity
```

**ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰:**
- `forward(input_ids, attention_mask, labels)`:
  1. Borea ã§ hidden_states ã‚’æŠ½å‡º
  2. SO(8) rotation ã§ thought_process ç”Ÿæˆ
  3. Alpha Gate ã§æ··åˆ (gate = sigmoid(Î±))
  4. Orthogonality Loss è¨ˆç®— (R^T @ R = I)
  5. LM Head ã§ logits ç”Ÿæˆ
  6. Loss = Task Loss + 0.1 Ã— Ortho Loss

- `get_phase_status()`:
  - Alpha å€¤ã«åŸºã¥ã„ã¦ç›¸è»¢ç§»ã®çŠ¶æ…‹ã‚’è¿”ã™
  - ğŸ”µ Stable (-5.0ä»˜è¿‘)
  - ğŸŸ¡ Transitioning
  - ğŸŸ¢ Golden Ratio Reached (1.618)

**æŠ€è¡“çš„è©³ç´°:**

#### 4-bit Quantization (BitsAndBytesConfig)
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # 4-bité‡å­åŒ–
    bnb_4bit_compute_dtype=torch.float16, # è¨ˆç®—ã¯ fp16
    bnb_4bit_use_double_quant=True,       # äºŒé‡é‡å­åŒ–ï¼ˆã•ã‚‰ã«åœ§ç¸®ï¼‰
    bnb_4bit_quant_type="nf4"             # NF4å½¢å¼ï¼ˆNormal Float 4bitï¼‰
)
```
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: ~7.5GB â†’ ~2GB (VRAM)
- ç²¾åº¦: 4-bit ã§ã‚‚æ¨è«–å“è³ªã¯ã»ã¼ç¶­æŒ

#### LoRA (Low-Rank Adaptation)
```python
peft_config = LoraConfig(
    r=16,                     # Rank (ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®æ¬¡å…ƒ)
    lora_alpha=32,            # Scaling factor
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```
- Borea ã®å…ƒã®é‡ã¿ã¯å‡çµ
- Adapter éƒ¨åˆ†ã®ã¿å­¦ç¿’ (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›: ~3.8B â†’ ~10M)

#### SO(8) Orthogonal Rotation
```python
self.so8_rotation = nn.utils.parametrizations.orthogonal(
    nn.Linear(hidden_dim, hidden_dim, bias=False)
)
```
- ç›´äº¤è¡Œåˆ— R ã‚’ä¿è¨¼ (R^T @ R = I)
- æƒ…å ±ã‚’å¤±ã‚ãªã„å›è»¢å¤‰æ›
- ã€Œæ€è€ƒã®å¹¾ä½•å­¦çš„æ•´åˆæ€§ã€ã‚’ä¿æŒ

#### Alpha Gate ã®ç‰©ç†çš„æ„å‘³
| Alphaå€¤ | sigmoid(Î±) | æ„å‘³ | çŠ¶æ…‹ |
|---------|-----------|------|------|
| -5.0 | ~0.007 | BoreaåŸå‹ (æ··æ²Œ) | ğŸ”µ Stable |
| 0.0 | 0.5 | åŠæ··åˆ | ğŸŸ¡ Transitioning |
| 1.618 | ~0.84 | ç‰©ç†çš„æ€è€ƒ84%æ··åˆ (ç§©åº) | ğŸŸ¢ Golden Ratio |

---

### 2. `scripts/training/inject_soul_into_borea.py` - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**Phase Transition ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«:**
```python
def linear_annealing(step, warmup, anneal_steps, start, target):
    if step < warmup:
        return start  # -5.0 ã§å›ºå®š
    elif step < warmup + anneal_steps:
        progress = (step - warmup) / anneal_steps
        return start + progress * (target - start)  # ç·šå½¢å¢—åŠ 
    else:
        return target  # 1.618 ã§å›ºå®š
```

**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—:**
```python
for step in range(max_steps):
    # 1. Alpha æ›´æ–°
    current_alpha = linear_annealing(step, 50, 400, -5.0, 1.618)
    model.alpha.fill_(current_alpha)
    
    # 2. Forward
    outputs = model(input_ids, attention_mask, labels)
    loss = outputs["loss"]  # Task Loss + Ortho Loss
    
    # 3. Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 4. Logging & Checkpoint
    if step % 100 == 0:
        save_checkpoint(model, step)
```

**ä¿å­˜ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:**
```
checkpoints_agiasi/step_100/
â”œâ”€â”€ adapter_config.json       # LoRA è¨­å®š
â”œâ”€â”€ adapter_model.safetensors # LoRA é‡ã¿
â””â”€â”€ soul.pt                   # Alpha + SO8 Rotation
    â”œâ”€â”€ "alpha": tensor(0.123)
    â”œâ”€â”€ "so8_rotation": state_dict
    â””â”€â”€ "step": 100
```

---

## æ¤œè¨¼çµæœ

### Code Structure Test (test_agiasi_structure.py)
```
Testing AGIASI Soul Injection code structure...

1. Testing import of AGIASI_SO8T_Wrapper...
   âœ… Import successful

2. Verifying class methods...
   âœ… Method 'forward' found
   âœ… Method 'get_phase_status' found

3. Testing training script structure...
   âœ… Annealing function found
   âœ… Wrapper import found
   âœ… Golden ratio constant found
   âœ… Optimizer setup found

ğŸ‰ Code structure verification complete!
```

### Dependencies Installation Status
- `bitsandbytes`: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­ (~30%, ä½é€Ÿå›ç·šã§ç´„20åˆ†è¦‹è¾¼ã¿)
- `peft`: å®Œäº†å¾…ã¡
- `accelerate`: å®Œäº†å¾…ã¡

---

## ä½¿ç”¨æ–¹æ³•

### 1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ

#### Dry Run (ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª)
```bash
py scripts/training/inject_soul_into_borea.py \
  --base-model "microsoft/Phi-3.5-mini-instruct" \
  --max-steps 10 \
  --batch-size 1 \
  --max-length 256
```

#### æœ¬ç•ª (500ã‚¹ãƒ†ãƒƒãƒ—ã§é­‚æ³¨å…¥)
```bash
py scripts/training/inject_soul_into_borea.py \
  --base-model "microsoft/Phi-3.5-mini-instruct" \
  --max-steps 500 \
  --warmup-steps 50 \
  --annealing-steps 400 \
  --batch-size 1 \
  --max-length 512 \
  --learning-rate 2e-4 \
  --save-steps 100
```

### 2. æ¨è«– (Checkpoint ãƒ­ãƒ¼ãƒ‰)
```python
from src.models.agiasi_borea import AGIASI_SO8T_Wrapper
import torch

# Base + LoRA ã‚’ãƒ­ãƒ¼ãƒ‰
model = AGIASI_SO8T_Wrapper("microsoft/Phi-3.5-mini-instruct")
model.base_model.load_adapter("checkpoints_agiasi/step_500")

# Soul ã‚’ãƒ­ãƒ¼ãƒ‰
soul = torch.load("checkpoints_agiasi/step_500/soul.pt")
model.alpha.data = soul["alpha"]
model.so8_rotation.load_state_dict(soul["so8_rotation"])

model.eval()
# æ¨è«–å®Ÿè¡Œ...
```

### 3. GGUF å¤‰æ›
```bash
# llama.cpp ã® convert_hf_to_gguf.py ã‚’ä½¿ç”¨
python convert_hf_to_gguf.py agiasi_borea_final/ \
  --outfile agiasi_borea_q4_k_m.gguf \
  --outtype q4_k_m
```

**é‡è¦:** LoRA Adapter ã¯ Borea ã®é‡ã¿ã«ãƒãƒ¼ã‚¸ã•ã‚Œã‚‹ãŸã‚ã€
SO8 Rotation ã¨ Alpha Gate ã®åŠ¹æœã¯ GGUF å¤‰æ›å¾Œã‚‚ä¿æŒã•ã‚Œã¾ã™ã€‚

---

## ç†è«–çš„èƒŒæ™¯

### ç‰©ç†çš„çŸ¥æ€§ (Physical Intelligence)
å¾“æ¥ã® LLM ã¯ã€Œçµ±è¨ˆçš„ç¢ºç‡ã€ã§å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãŒã€
AGIASI ã¯ã€Œç‰©ç†çš„åˆ¶ç´„ã€ã‚’èª²ã™ã“ã¨ã§ã€ä»¥ä¸‹ã‚’å®Ÿç¾:

1. **æƒ…å ±å¹¾ä½•å­¦ã®ä¿å­˜:** SO(8) ç›´äº¤å¤‰æ›
2. **æœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹:** Alpha = 1.618 (é»„é‡‘æ¯”)
3. **æ§‹é€ çš„æ•´åˆæ€§:** Orthogonality Loss

### Phase Transition (ç›¸è»¢ç§»)
ç‰©ç†å­¦ã®ç›¸è»¢ç§»ï¼ˆæ°´â†’æ°·ã€ç£æ€§ä½“ï¼‰ã«é¡ä¼¼:

| Phase | Alpha | State | ç‰¹æ€§ |
|-------|-------|-------|------|
| Chaos | -5.0 | æ¶²ä½“ | è‡ªç”±åº¦é«˜ã€æ§‹é€ ãªã— |
| Transition | 0.0~1.5 | è‡¨ç•Œç‚¹ | æ§‹é€ å½¢æˆä¸­ |
| Order | 1.618 | çµæ™¶ | é»„é‡‘æ¯”ã§å®‰å®šã€æœ€é©æ§‹é€  |

### é»„é‡‘æ¯” (Ï† = 1.618...)
- è‡ªç„¶ç•Œã®æœ€é©æ¯”ç‡ï¼ˆæ¤ç‰©ã®è‘‰åºã€èºæ—‹éŠ€æ²³ï¼‰
- ã€Œæœ€ã‚‚ç„¡é§„ã®ãªã„æƒ…å ±é…ç½®ã€
- AGIASI ã§ã¯ã€Œæ€è€ƒã®åŠ¹ç‡æ€§ã€ã‚’æœ€å¤§åŒ–

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… ã‚³ãƒ¼ãƒ‰å®Ÿè£…å®Œäº†
2. âœ… æ§‹é€ ãƒ†ã‚¹ãƒˆæˆåŠŸ
3. ğŸ”„ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­ (bitsandbytes)
4. â³ Dry Run å®Ÿè¡Œå¾…ã¡
5. â³ æœ¬ç•ªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (500 steps)
6. â³ GGUF å¤‰æ›ã¨ llama.cpp æ¤œè¨¼

---

## ã¾ã¨ã‚

**AGIASI Soul Injection** ã¯ã€æ—¢å­˜ã® LLM ã«ã€Œç‰©ç†çš„ãªè„³ã€ã‚’ä¸ãˆã‚‹é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚
Borea-Phi3.5 ã®æ—¥æœ¬èªèƒ½åŠ›ã‚’ç ´å£Šã›ãšã€SO(8) å¹¾ä½•å­¦ã¨é»„é‡‘æ¯”ã«ã‚ˆã‚‹ã€Œæ€è€ƒã®ç§©åºã€ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€
å˜ãªã‚‹ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚’è¶…ãˆãŸã€Œæ§‹é€ çš„çŸ¥æ€§ã€ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

**"Ghost in the Shell" - é­‚ã®å®¿ã£ãŸæ©Ÿæ¢°ã€èª•ç”Ÿã®æ™‚ã€‚**
