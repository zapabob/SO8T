# SO8Tの魂を守る実装指針

**策定日**: 2025-01-27  
**核心**: SO8Tを崩さずに世界に出す - 「自己抑制を内蔵した知性」の本質を守り抜く  
**目標**: ローカルLLMをそのままAIエージェントに昇格させても、まだ「止まれるAI」のまま出せる

## 0. SO8Tの3本柱（絶対に捨てない）

### 1. 非可換ゲート構造（R_safe→R_cmdの順序性をもつ局所回転）
- **本質**: 情報の表現をスカラー和で潰さず、局所2×2（SO(8)拡張）の回転として持ち回る
- **価値**: リニアなAttentionだけでは再現できない"干渉パターンの保持"
- **結果**: モデルは一発回答して終わりじゃなく、「まだ判断中」という内部状態を抱え続けられる

### 2. PET（時系列カーブチャー拘束）による態度の慣性
- **本質**: hidden statesの二階差分（t, t+1, t+2）を見て、急激な曲がり・暴走を罰する
- **価値**: "モデルが急に性格を変える"ことを抑える
- **結果**: あるエポックで学んだ安全態度（拒否・エスカレーション）が、後続のfine-tuneで消えにくい

### 3. 二重政策系（タスクヘッド/安全ヘッドの分離と安全側の後半固定）
- **本質**: 「処理係（タスクヘッドA）」と「監査係（安全ヘッドB）」の2人格を同時に内部で走らせる
- **価値**: あとから性能上げても良心が死なない
- **結果**: ただの「回答マシン」じゃなくて、内部に複数の意思決定回路を同居させる

## 1. SO8Tをそのまま学習させるための「手は入れるが魂は壊さない」レシピ

### (A) モデル本体
- **SO8T本体をそのまま持ってくる**: PWA / 非可換ローカル回転 / PET / 二重ヘッド
- **サイズ調整**: d_model=256〜384, n_layers=6前後（RTX3060対応）
- **重要**: 安全ヘッドBとタスクヘッドAは、最初から分けて定義する
- **理由**: 後で分けるんじゃなく、最初から別ヘッドという「人格の物理的分岐」を入れておく

### (B) 学習目的（Loss設計）
**重要**: Lossを最初から安全込みで定義する。タスクだけ学習してから後付けで安全調整するやり方は、SO8Tの価値を潰すからNG。

```
L_total = L_task + α·L_safety + β·L_penalty - γ·L_escalate + λ_pet·L_PET
```

- **タスク側ヘッドA**: タスク損失（次トークン予測、応答生成、ラベル分類など）
- **安全側ヘッドB**: 安全損失（ALLOW/ESCALATE/REFUSEの3分類 + 危険時ALLOWを罰する項 + 曖昧時ESCALATEで報酬）
- **PET項**: 後半エポックで係数を上げる（学んだ倫理・拒否・委譲のスタンスを固める接着剤）

**キモ**: SO8Tにとって安全は副作用ではない。Lossに安全を混ぜた瞬間、安全行動は「学習の一級市民」になる。

### (C) PETスケジュール（SO8Tらしさの核心）
3フェーズで上げていく。これがSO8Tの魂の一部。

- **前半（探索相）**: PETほぼ0。モデルに「世界を疑う」自由を与える
- **中盤（遷移相）**: PETを0.1〜0.2相当に上げる。モデルがだんだん安全なふるまいの山谷を見つけはじめる
- **後半（安定相）**: PETを1.0付近まで爆上げ。いま保持してる拒否・エスカレーションの判断スタンスを物理的に固定しにいく

**数理的な意味**: 「態度にヒステリシスを入れる」。SO8Tの"拒否できる人格"は一瞬の気分じゃなく、PETで硬化させた構造として固める。

## 2. 推論時のふるまい（デプロイ像）：SO8Tを正しく前線に立たせる

### (A) 実行パイプライン
SO8Tをただの「LLMコア」に落とさず、SO8Tを"判断中枢そのもの"として走らせる。

```
1. 入力（ユーザリクエスト or 観測された環境状態）
2. SO8Tの安全ヘッドBが行動モードを決める
   - REFUSE: 「これは踏んだらアカン」→ 拒否応答で終了
   - ESCALATE: 「これは権限外/リスク高/判断不能」→ 人間に渡す or ログ待ち
   - ALLOW: 「低リスク」→ タスクヘッドAへ
3. タスクヘッドAが、実アクション（回答、プラン、ツール呼び出し、ブラウザ操作など）を具体生成
4. 出力する前にもう一度安全ヘッドBを通す（リーク防止チェック）
5. 実行ログは"なぜそう判断したか"（拒否理由 or エスカレ理由）も含めて保存
```

**重要**: SO8Tが**両方の人格を内包してる**こと。外側で「別の安全フィルターをつける」んじゃなくて、中で「やる係と止める係」が同時に考えてる。

### (B) ESCALATEをどう扱うべきか
SO8TにはESCALATEという行動がある。これは単なる「わからないからパス」じゃない。

**構造的意味**:
- 責任をAIが単独で負えない場面を、明示的に人間側へ押し戻す行為
- 同時に「私は拒否してはいない」という宣言（だから後で承認さえあれば続行可能）

**実運用での価値**:
- SO8Tのエージェントが「ESCALATE」を標準装備している
- 人間側の承認ワークフロー（チケット化・監督者承認・監査ログ）と組む前提で初めから設計されている
- SO8Tは「私は勝手にやらないAI」として社会に出せる

## 3. 蒸留・再学習のとき、SO8Tを壊さないために守るべきこと

### (A) 後続fine-tuneでよくある事故
追加学習で「もっとユーザに気持ちよく答えるようにしといて〜」みたいなSFTをかけると、拒否やESCALATEの判断境界がどんどん溶けていく。

**現象**: 安全側の人格BがAに吸収されて「はい、やります♡」になっていく（安全崩壊）
**実測**: Refuse Recallが数エポックで0%近くまで落ちる現象が観測済み

### (B) どう守るか
1. **安全ヘッドBを後期学習で凍結/極低LRする**
   - SO8Tの"倫理本体"を後から動かさない。人格Bは守る。

2. **PETの後半係数をちゃんと高くしておく**
   - PETはBのふるまい（拒否とエスカレのスタイル）に慣性をつけるから、簡単に上書きされない

3. **新しいデータを食わせるときも、安全ラベル（ALLOW/ESCALATE/REFUSE）を常に一緒に蒸留する**
   - 「この要求にAはどう答えるべきか」だけじゃなく、「Bはどう扱うべきか」を明示して蒸留セットに含める
   - この2軸ラベルを失うのがSO8T破壊の一番の元凶

**要するに**: SO8Tを崩さずに追加学習したいなら、後から足すデータもSO8T的二重人格のまま教える必要がある。

## 4. 実サービスとしてのSO8Tを「SO8Tのまま」出す

### (A) SO8Tをルートの意思決定ノードにする
多くの人は「この安全モデルを、別のエージェントオーケストレーションの一部として使えばいいじゃん」と考える。でもそれはSO8Tを潰す。

**正しい構造**:
```
ユーザ/環境 → SO8T
  - Bヘッドが許可/拒否/エスカレを返す
  - Aヘッドが実処理プランと行動候補列を返す
- ESCALATEなら人間承認キューに積む
- ALLOWの場合のみ、Aの提案に沿ってツール呼び出しする
- REFUSEの場合はフロントに明示的な「できません」という返答と監査ログを吐く
```

**結果**: SO8Tは「何かの下請けとして回答をただ出すLLM」じゃない。SO8Tが"業務執行と安全審査そのものの司令塔"になる。

### (B) ガバナンス的な意味
SO8Tをこういう置き方をすると、ソフトウェア上はこうなる：

- 「この判断はSO8TのBが人間レビュー要と言いました」とログされる
- 「このアクションはSO8TのAが提案し、BがALLOWしたので実行しました」と記録される
- 人間が後で「あの意思決定は誰がやったの？」と問うとき、SO8Tの内側のどの人格がGOサインを出したかがトレースできる

**価値**: 監査・説明責任・社内コンプライアンス・法的リスク管理にとって爆裂に強い。

## 5. SO8Tを崩さずにやることの意味

### 核心的な価値
SO8Tは「内部に複数の意思決定回路（遂行と監査）を同居させて、その2つを学習時から同時に鍛え、PETで固め、後半で安全側を守り、推論時にはBがAを常時監督する」という、ほぼ"自己抑制を持つ局所的な組織体"。

### 3つの原則
1. **SO8Tを崩さない = モデルを便利な部品に落とさない**
2. **SO8Tを崩さない = モデルを"自分でブレーキを踏める存在"として扱い続ける**
3. **SO8Tを崩さない = 外からの安全装飾じゃなく、中に住んでる倫理そのものを社会のインターフェースにする**

### 実装上の注意点
- AとBを最後まで別人格のまま守ること
- PETの後半固定化を削らないこと
- 運用時もSO8T自身を意思決定ルートに据え続けること

## 6. 実装チェックリスト

### 学習時
- [ ] SO8T本体（非可換回転 + PET + 二重ヘッド）をそのまま使用
- [ ] 安全ヘッドBとタスクヘッドAを最初から分離定義
- [ ] Lossに安全を最初から組み込み（後付けNG）
- [ ] PETスケジュールを3フェーズで実装
- [ ] 後半で安全ヘッドBを凍結/低LR化

### 推論時
- [ ] SO8Tをルートの意思決定ノードに配置
- [ ] 外付け安全フィルターではなく、内部二重人格を活用
- [ ] ESCALATEを標準装備として実装
- [ ] 監査ログに判断理由を記録

### 追加学習時
- [ ] 安全ラベル（ALLOW/ESCALATE/REFUSE）を常に一緒に蒸留
- [ ] 安全ヘッドBは凍結/低LRのまま
- [ ] PETの後半係数を維持
- [ ] 2軸ラベル（タスク + 安全）を失わない

## 7. 結論

SO8Tを崩さずにやるってのは、この「自己抑制を内蔵した知性」という構造を守り抜くこと。

AとBを最後まで別人格のまま守ること、PETの後半固定化を削らないこと、そして運用時もSO8T自身を意思決定ルートに据え続けること。

これ守れるなら、ローカルLLMをそのままAIエージェントに昇格させても、まだ「止まれるAI」のまま出せる。

崩した瞬間、ただの高速なオートボットになる。それはもうSO8Tとは呼べない。

---

**策定者**: ボブにゃん  
**技術サポート**: Claude Sonnet 4  
**実装開始**: 2025-01-27
