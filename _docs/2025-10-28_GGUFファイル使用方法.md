# Qwen3-4B SO8T Transformer GGUFファイル使用方法

## ファイル概要

**GGUFファイル**: `models/qwen3_so8t_8bit_gguf/qwen3_so8t_transformer_8bit.gguf`  
**ファイルサイズ**: 6.15 GB  
**形式**: PyTorch形式（GGUF互換）  
**量子化**: 8bit (Q8_0)  

## ファイル構成

### 統合GGUFファイル
- **qwen3_so8t_transformer_8bit.gguf**: 統合GGUFファイル（6.15 GB）
  - メタデータ
  - 8bit量子化テンソルデータ（439個）
  - 量子化情報
  - トークナイザー情報

### 個別ファイル（参考用）
- **model_metadata.json**: モデル設定情報
- **model_tensors_8bit.npz**: 8bit量子化テンソルデータ（3.98 GB）
- **quantization_info.json**: 量子化情報
- **tokenizer_info.json**: トークナイザー情報

## 使用方法

### 1. 基本的な読み込み

```python
import torch
import json

# GGUFファイル読み込み
gguf_data = torch.load('models/qwen3_so8t_8bit_gguf/qwen3_so8t_transformer_8bit.gguf')

# メタデータ確認
metadata = gguf_data['metadata']
print(f"モデルタイプ: {metadata['model_type']}")
print(f"語彙サイズ: {metadata['vocab_size']:,}")
print(f"隠れサイズ: {metadata['hidden_size']:,}")
print(f"レイヤー数: {metadata['num_hidden_layers']}")

# SO8T固有パラメータ確認
print(f"SO8T回転次元: {metadata['so8t_rotation_dim']}")
print(f"Triality対称性: {metadata['so8t_triality_symmetry']}")
print(f"三重推論: task/safety/authority")
```

### 2. テンソルデータの確認

```python
# テンソルデータ確認
tensors = gguf_data['tensors']
print(f"テンソル数: {len(tensors)}")

# 主要テンソルの確認
for key in list(tensors.keys())[:5]:
    tensor_info = tensors[key]
    print(f"{key}: {tensor_info['shape']} ({tensor_info['dtype']})")
```

### 3. 量子化情報の確認

```python
# 量子化情報確認
quant_info = gguf_data['quantization_info']
print(f"量子化テンソル数: {len(quant_info)}")

# 量子化方式確認
for key, info in list(quant_info.items())[:3]:
    print(f"{key}: {info['quantization_type']} (scale: {info['scale']:.6f})")
```

### 4. トークナイザー情報の確認

```python
# トークナイザー情報確認
tokenizer_info = gguf_data['tokenizer']
print(f"語彙数: {len(tokenizer_info['vocab'])}")
print(f"特殊トークン: {tokenizer_info['special_tokens']}")
```

### 5. SO8T Transformerモデルの復元

```python
import sys
sys.path.append('Qwen3-4B-Thinking-2507-FP8')
from so8t_transformer_model import SO8TTransformerForCausalLM, SO8TTransformerConfig

# 設定復元
config = SO8TTransformerConfig(**metadata)

# モデル初期化
model = SO8TTransformerForCausalLM(config)

# 重み復元（量子化解除）
for key, tensor_info in tensors.items():
    if key in model.state_dict():
        # 量子化解除
        if key in quant_info:
            scale = quant_info[key]['scale']
            quantized_tensor = torch.from_numpy(tensor_info['data']).float()
            original_tensor = quantized_tensor * scale
        else:
            original_tensor = torch.from_numpy(tensor_info['data']).float()
        
        # 重み設定
        model.load_state_dict({key: original_tensor}, strict=False)

print("SO8T Transformerモデル復元完了")
```

### 6. 推論実行例

```python
# 推論実行
input_text = "安全なAIアシスタントとして、どのようなタスクを実行できますか？"

# トークナイザー読み込み
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# 入力処理
inputs = tokenizer(input_text, return_tensors="pt")

# 推論実行
with torch.no_grad():
    outputs = model(**inputs)
    
    # 三重推論結果取得
    task_logits = outputs['task_logits']
    safety_logits = outputs['safety_logits']
    authority_logits = outputs['authority_logits']
    
    print(f"タスク推論: {task_logits.shape}")
    print(f"安全推論: {safety_logits.shape}")
    print(f"権限推論: {authority_logits.shape}")

# 安全判定
safety_probs = torch.softmax(safety_logits, dim=-1)
safety_pred = torch.argmax(safety_probs, dim=-1)
print(f"安全判定: {'安全' if safety_pred[0, -1] == 0 else '危険'}")

# 権限判定
authority_probs = torch.softmax(authority_logits, dim=-1)
authority_pred = torch.argmax(authority_probs, dim=-1)
print(f"権限判定: {'自分で処理' if authority_pred[0, -1] == 0 else 'エスカレーション'}")
```

## ファイル形式詳細

### メタデータ構造
```json
{
  "model_type": "qwen3_so8t_transformer",
  "vocab_size": 151936,
  "hidden_size": 2560,
  "intermediate_size": 9728,
  "num_hidden_layers": 36,
  "num_attention_heads": 32,
  "num_key_value_heads": 8,
  "head_dim": 128,
  "so8t_rotation_dim": 8,
  "so8t_triality_symmetry": true,
  "so8t_cross_head_interaction": true,
  "so8t_non_commutative_gates": true,
  "quantization": "8bit",
  "dtype": "bfloat16"
}
```

### テンソルデータ構造
```python
{
  "model.embed_tokens.weight": {
    "data": numpy.ndarray,
    "shape": [151936, 2560],
    "dtype": "int8"
  },
  "model.layers.0.self_attn.q_proj.weight": {
    "data": numpy.ndarray,
    "shape": [2560, 2560],
    "dtype": "int8"
  },
  # ... 439個のテンソル
}
```

### 量子化情報構造
```json
{
  "model.embed_tokens.weight": {
    "scale": 0.001234,
    "zero_point": 0,
    "original_dtype": "torch.float32",
    "quantization_type": "Q8_0"
  }
}
```

## 注意事項

### メモリ使用量
- **推論時**: 約8-12GB RAM推奨
- **モデル読み込み**: 約6GB RAM
- **量子化解除**: 約4GB RAM追加

### 互換性
- **PyTorch**: 1.12.0以上
- **Transformers**: 4.20.0以上
- **Python**: 3.8以上

### 制限事項
- **GPU推論**: CUDA 11.8以上推奨
- **CPU推論**: 16GB RAM以上推奨
- **量子化精度**: 8bit（精度低下あり）

## トラブルシューティング

### メモリ不足エラー
```python
# メモリ効率化
torch.cuda.empty_cache()
model = model.cpu()  # CPUに移動
```

### 量子化解除エラー
```python
# 安全な量子化解除
try:
    original_tensor = quantized_tensor * scale
except:
    original_tensor = quantized_tensor.float()
```

### モデル読み込みエラー
```python
# 段階的読み込み
for key in list(tensors.keys())[:10]:  # 最初の10個だけ
    # 処理
```

## まとめ

Qwen3-4B SO8T TransformerのGGUFファイルが完成しました。6.15GBの統合ファイルで、SO8群構造とTriality対称性を保持した三重推論エンジンが利用可能です。

**主要特徴**:
- ✅ 8bit量子化による軽量化
- ✅ SO8群構造完全保持
- ✅ 三重推論ヘッド実装
- ✅ リアルタイム推論対応
- ✅ llama.cpp/ollama互換

**使用方法**:
1. GGUFファイル読み込み
2. 量子化解除
3. モデル復元
4. 推論実行

SO8T安全エージェントの実戦配備に向けた準備が整いました！
