# 人間を模倣したWebスクレイピング 実装ログ

## 実装情報
- **日付**: 2025-11-08
- **Worktree**: main
- **機能名**: 人間を模倣したWebスクレイピング（自動ページ遷移対応）
- **実装者**: AI Agent

## 実装内容

### 1. 人間を模倣したWebスクレイピングスクリプト作成

**ファイル**: `scripts/data/human_like_web_scraping.py` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 19:40:00  
**備考**: 人間を模倣した動作と自動ページ遷移を実装したWebスクレイピングスクリプト

#### 主な機能
1. **人間を模倣したマウス移動**
   - ランダムな位置へのマウス移動
   - 滑らかな移動（複数ステップ）
   - ランダムな移動パターン

2. **人間を模倣したスクロール動作**
   - 段階的なスクロール（一度に全部スクロールしない）
   - ランダムなスクロール速度
   - スクロール後の待機時間

3. **人間を模倣した待機時間**
   - ランダムな待機時間（1.0-3.0秒）
   - ページ読み込み後の自然な待機
   - アクション間の待機

4. **人間を模倣したホバー動作**
   - リンクやボタンへのホバー
   - ランダムに1-3個の要素にホバー
   - ホバー後の待機時間

5. **自動ページ遷移**
   - リンクの自動抽出
   - キューを使った幅優先探索
   - 最大深度の制限
   - 同じドメイン内のリンクのみ追跡

### 2. バッチスクリプト作成

**ファイル**: `scripts/data/run_human_like_scraping.bat` (新規作成)

**実装状況**: [実装済み]  
**動作確認**: [OK]  
**確認日時**: 2025-11-08 19:40:00  
**備考**: 人間を模倣したWebスクレイピングを簡単に実行するためのバッチスクリプト

- URL引数対応
- 音声通知機能
- エラーハンドリング

### 3. 人間を模倣した動作の実装詳細

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 19:40:00  
**備考**: 人間の自然な動作を模倣する機能を実装

#### マウス移動
- **実装**: `human_like_mouse_move()`
- **動作**: ランダムな位置への滑らかな移動（3-8ステップ）
- **待機時間**: 各ステップで0.05-0.15秒

#### スクロール動作
- **実装**: `human_like_scroll()`
- **動作**: 段階的なスクロール（200-600pxずつ）
- **待機時間**: スクロール後に0.5-1.5秒
- **追加動作**: 50%の確率でトップに戻る

#### 待機時間
- **実装**: `human_like_wait()`
- **動作**: ランダムな待機時間（デフォルト1.0-3.0秒）
- **用途**: ページ読み込み後、アクション間

#### ホバー動作
- **実装**: `human_like_hover()`
- **動作**: リンクやボタンへのランダムなホバー（1-3個）
- **待機時間**: ホバー後に0.3-0.8秒

### 4. 自動ページ遷移機能

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 19:40:00  
**備考**: リンクを自動的に追跡してページ遷移を実行

- **リンク抽出**: JavaScriptでページ内のリンクを抽出
- **URL正規化**: 相対URLを絶対URLに変換
- **ドメインフィルタ**: 同じドメイン内のリンクのみ追跡
- **キュー管理**: 幅優先探索でURLキューを管理
- **深度制限**: 最大深度を設定可能（デフォルト3）

### 5. Cursorブラウザ統合

**実装状況**: [実装済み]  
**動作確認**: [実行中]  
**確認日時**: 2025-11-08 19:40:00  
**備考**: Cursorブラウザ（Chrome DevTools MCP）を統合

- CDP経由でCursorブラウザに接続
- Playwrightを使用したページ操作
- 人間を模倣した動作の実行

## 作成・変更ファイル
- `scripts/data/human_like_web_scraping.py` (新規作成)
- `scripts/data/run_human_like_scraping.bat` (新規作成)
- `_docs/2025-11-08_main_人間を模倣したWebスクレイピング実装.md` (新規作成)

## 設計判断

1. **人間を模倣した動作**: ボット検知を回避するため、人間の自然な動作を模倣
2. **自動ページ遷移**: リンクを自動的に追跡して効率的にデータ収集
3. **深度制限**: 無限ループを防ぐため、最大深度を設定
4. **ドメインフィルタ**: 同じドメイン内のリンクのみ追跡して、データの一貫性を保つ

## テスト結果

### 実行結果
- **実行時刻**: 2025-11-08 19:40:00
- **実行状態**: バックグラウンドで実行中
- **設定**: 
  - 開始URL: https://ja.wikipedia.org/wiki/メインページ
  - 最大ページ数: 10
  - リンク追跡: 有効
  - 最大深度: 2

### 動作確認項目
- [実行中] マウス移動の動作
- [実行中] スクロール動作の動作
- [実行中] ホバー動作の動作
- [実行中] 自動ページ遷移の動作
- [実行中] データ抽出の動作

## 使用方法

### 基本的な使用方法
```bash
# デフォルト設定で実行
py -3 scripts\data\human_like_web_scraping.py --urls https://ja.wikipedia.org/wiki/メインページ

# カスタム設定で実行
py -3 scripts\data\human_like_web_scraping.py \
    --urls https://ja.wikipedia.org/wiki/メインページ \
    --output D:\webdataset\processed \
    --max-pages 100 \
    --follow-links \
    --max-depth 3 \
    --delay 2.0
```

### バッチスクリプト使用
```bash
# デフォルトURLで実行
scripts\data\run_human_like_scraping.bat

# カスタムURLで実行
scripts\data\run_human_like_scraping.bat https://ja.wikipedia.org/wiki/メインページ https://www.e-gov.go.jp/
```

## パラメータ説明

- `--urls`: 開始URLリスト（必須）
- `--output`: 出力ディレクトリ（デフォルト: D:\webdataset\processed）
- `--use-cursor-browser`: Cursorブラウザを使用（デフォルト: true）
- `--remote-debugging-port`: リモートデバッグポート（デフォルト: 9222）
- `--delay`: リクエスト間の遅延（秒、デフォルト: 2.0）
- `--timeout`: ページ読み込みタイムアウト（ミリ秒、デフォルト: 30000）
- `--max-pages`: 最大ページ数（デフォルト: 100）
- `--follow-links`: リンクを自動追跡（デフォルト: true）
- `--max-depth`: 最大深度（デフォルト: 3）

## 運用注意事項

### データ収集ポリシー
- 利用条件を守りつつ、高信頼ソースとして優先使用
- robots.txt遵守を徹底
- 個人情報・機密情報の除外を徹底

### NSFWコーパス運用
- **主目的**: 安全判定と拒否挙動の学習（生成目的ではない）
- モデル設計とドキュメントに明記
- 分類器は検出・拒否用途のみ

### /thinkエンドポイント運用
- 四重Thinking部（`<think-*>`）は外部非公開を徹底
- `<final>`のみ返す実装を維持
- 監査ログでThinkingハッシュを記録（内容は非公開）

## 次のステップ

1. **動作確認**: 実行中のスクレイピングの動作を確認
2. **パフォーマンス最適化**: スクレイピング速度の最適化
3. **エラーハンドリング強化**: 失敗時のリトライ機能
4. **データ品質チェック**: 抽出データの品質検証

