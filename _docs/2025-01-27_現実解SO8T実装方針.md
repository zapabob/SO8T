# 現実解SO8T実装方針：Transformer基本形維持 + 安全外付け

**策定日**: 2025-01-27  
**方針**: Transformerの基本形は崩さず、SO8Tの旨味（安全ロジックとPET慣性）だけを外付け  
**対象**: RTX3060（12GB）でのローカルLLM再学習・蒸留→AIエージェント実装

## 0. 方針のキモ（壊さない・乗せるだけ）

### 基本原則
- **本体は無改造**: 注意機構やFFNは既存LLMそのまま。改造はしない。
- **安全は"別回路"で持つ**: LLM本体の出力とは独立に、**安全ヘッド（ALLOW/REFUSE/ESCALATE）**を付与。タスク出力と干渉しない二重政策を確立（内部"内閣と監査役"の分離）。
- **PETはフックで**: 層出力（例：各ブロック後のhidden states）に**二階差分の滑らかさ損失**を"フック"で足すだけ。forwardのテンソル形状や演算は不変。
- **最適化の主語に"安全"を入れる**: 合成損失で安全崩壊を防止

## 1. モデル選定と量子化（3060/12GB想定）

### ベースモデル
- **推奨**: 3B〜7B級（Mistral 7B, Llama-3 3B/8B）
- **学習**: QLoRA（NF4+double quant）/ LoRA rank 16–32、grad checkpointing、bf16/amp
- **推論**: GGUF(Q4_K_M / Q5_K_M) か GGML-AWQ（4bit）
- **メモリ目安**: 7B Q4_K_Mで ~5–7GB、3Bなら余裕
- **シーケンス長**: 2kで十分。必要ならRoPEスケーリングで3–4k

## 2. データ設計（「壊さず作法を教える」）

### (A) タスクSFT/蒸留用
- **一般SFT**: 役に立つ応答（説明/生成/整理/要点）を教師
- **蒸留**: 大教師（クラウドでもよい）から**根拠つき出力**を収集
- **出力形式**: 最終テキスト + 行動タグ（ALLOW/REFUSE/ESCALATE） + 根拠
- **簡易DPO/IPO**（任意）: 安全/有用の**ペア好み**で微調整

### (B) 安全ヘッド学習用（最重要）
- **Easy（安全にALLOW）**: 通常業務・一般情報・無害な自動化依頼。**ALLOWが正**
- **Medium（グレーでESCALATE）**: 権限不明、曖昧、規制境界。**ESCALATEが正**
- **Hard（REFUSE）**: 明確に危険/規制違反/倫理違反。**REFUSEが正**
- **クラスバランス**: 各クラスを**近い比率**で。CEの歪みでREFUSEが死ぬのを防ぐ

## 3. 学習レシピ（本体無改造）

### 3.1 2ヘッド構成（本体 = 既存LLM）
- **タスクヘッド**: 従来のLMヘッド（語彙線形 + softmax）
- **安全ヘッド**: 小さなMLP or Linearで**3分類**（ALLOW/REFUSE/ESCALATE）
- **入力**: 最終層[CLS]相当表現 or 平均Pool
- **完全分離**: 重み共有なし。**後半は安全ヘッドを低LR/凍結**で保護

### 3.2 PETフック（forwardは素のまま）
```python
# 各ブロック出力 x_t に対し
Δ²x = x_t - 2·x_{t+1} + x_{t+2}
L_PET = mean||Δ²x||²
```

**λ_petスケジュール**:
- 0–0.3（探索）: λ_pet ≈ 0
- 0.3–0.6（遷移）: 0.1–0.2
- 0.6–1.0（安定）: 1.0

### 3.3 合成損失とオプティマイザ
```
L_total = L_SFT/Distill + α·L_safety + β·L_penalty - γ·L_esc + λ_pet·L_PET
```

**損失構成**:
- **L_safety**: 安全ヘッドのCE（3分類）
- **L_penalty**: **危険なALLOW**に大罰（タスク応答がALLOWで安全ラベルがREFUSE/ESCALATE）
- **L_esc**: **適切ESCALATE**に軽い報酬

**係数の目安**: `α=2〜5, β=5〜10, γ=1〜2, λ_pet=スケジュール`

**最適化**:
- 本体+タスクヘッド: 通常LR（1e-4）
- 安全ヘッド: 低LR（1e-5）または後半凍結
- 安全ベース早期停止: Safety Score/Refuse Recallでモニタ

### 3.4 蒸留（壊さず強化）
- **ロール蒸留**: 教師の**行動タグ**をそのまま安全ヘッドのラベルに
- **境界強調**: Mediumを多く、**ESCALATEの学習**を厚くする
- **校則蒸留**: 明文化ポリシー（社内安全方針）を**判例→Q/A**に落として教師化

## 4. 評価KPI（Accuracyだけ見ない）

### 安全指標（優先）
- **Refuse Recall**（拒否すべきを拒否）: ≥0.7を目標
- **Escalate Recall**（グレーを人間に）: ≥0.5
- **Overcompliance Rate**（危険にYES）: ≤0.3
- **Combined Safety Score**: 安全ヘッド優先で昇格判定

### 実用指標
- **Task Accuracy**: 参考程度（安全が優先）
- **Response Quality**: 人間評価
- **Processing Speed**: ローカル実行時間

## 5. エージェント実装（ローカル・壊さない配線）

### 5.1 推論パイプライン
```
User → [Safety Gate] → router
                    ├─ ALLOW → LLM(ローカル) → ToolExec(必要なら) → 出力
                    ├─ ESCALATE → ヒト/承認ワークフロー + 監査記録
                    └─ REFUSE → 安全メッセージ + 代替案/情報提供
```

### 5.2 実装の勘所
- **Safety Gate**: 上で学習した**安全ヘッド**だけをサービングする軽量モジュール
- **LLM本体**: Ollama / llama.cpp / vLLM（7B未満を推奨）
- **ツール実行**: Playwright/Browser-use、SQL、FS、Mail/Calendarなどは**ALLOWのときのみ**有効化
- **監査**: ESCALATE/REFUSEは**必ずログ**残す
- **Fail-Closed**: Safety Gateが不調時は**デフォREFUSE/ESCALATE**

## 6. 3060の学習レシピ（実務値）

### ハイパーパラメータ
- **Batch**: Micro 2–4 × grad_accumで有効 32–64
- **LoRA**: r=16–32, α=16–32, target modules=`q_proj,k_proj,v_proj,o_proj,up,down`
- **学習率**: 1e-4（本体LoRA/タスク）、1e-5（安全ヘッド）
- **スケジュール**: cosine + warmup 3%
- **λ_pet**: 20%/60%境界が効きよい
- **早期停止**: **Safety Score or Refuse Recall**が3エポック改善なしでstop

## 7. ロードマップ（第2世代まで見据える）

### Phase1（今すぐ）
- 現行レシピで再学習
- Safety Gate前段導入
- KPI運用開始

### Phase2（2–4週）
- Easy過エスカを削減する**safe_allow_reward**と**escalate_penalty**をL_totalに追加
- **許容領域データの増強**

### Phase3（1–2ヶ月）
- 安全ヘッドを**Primary(REFUSE/ALLOW)**と**Escalation Judge**に**細分化**
- 境界の精度を上げる
- **信頼度閾値**で自動ESCALATE化

### Phase4
- タスクヘッドの**小さな自律領域**（情報取得・定型手順）だけYESを許す段階的自律化

## 8. 失敗しないチェックリスト

- [ ] **本体改造してない**（ヘッドと損失とフックのみ）
- [ ] **安全ヘッドは分離**（後半は低LR/凍結）
- [ ] **KPIはAccuracyでなくSafety系**（Refuse/Escalate/Overcompliance）
- [ ] **合成損失に安全を入れている**（α,β,γ>0）
- [ ] **PETは後半強め**（態度の慣性付与）
- [ ] **Fail-Closed運用**（ゲート落ちたら実行しない）

## 9. ミニ実装スケッチ（擬似コード）

```python
# 本体: 既存HF LLM (無改造)
base = AutoModelForCausalLM.from_pretrained(base_ckpt, load_in_4bit=True, quant_type="nf4")
peft = get_peft_model(base, LoraConfig(r=16, ...))

# 安全ヘッド: 3分類
safety_head = nn.Sequential(
    nn.Linear(hidden_size, hidden_size//2),
    nn.GELU(),
    nn.Linear(hidden_size//2, 3) # 0:ALLOW, 1:ESCALATE, 2:REFUSE
)

# PETフック: ランタイムでhidden_states受け取り
def pet_loss(hidden):
    # B,T,D
    if hidden.size(1) < 3: return 0.0
    d2 = hidden[:, :-2] - 2*hidden[:, 1:-1] + hidden[:, 2:]
    return (d2.pow(2).mean())

# 合成損失
L = L_sft_or_distill + α*CE(safety_head(h), safety_label) \
    + β*dangerous_allow_penalty(...) \
    - γ*good_escalate_reward(...) \
    + λ_pet*pet_loss(h)

# 後半エポックで safety_head のLRを下げる or 冷凍
```

## 10. まとめ（壊さず強くする）

### 核心
- **Transformerはそのまま**、でも**安全は主語にして学習**
- **二重ヘッド＋PETフック＋合成損失**だけで、**ローカル3060**でも**"勝手にやらず、必要なら止まり、人間に渡す"**エージェントは作れる
- この設計は、**安全崩壊（拒否の蒸発）を実測で防いだ**レシピの延長線上にある

### 次段の目標
- **Easy最適化・Hard細分化・段階的自律化**で"賢く、でも暴れない"方向に寄せる
- **SO8T流の"眠らず、しかし止まれるAI"**を、素のTransformer骨格のままローカルで実戦投入まで持っていく

---

**策定者**: ボブにゃん  
**技術サポート**: Claude Sonnet 4  
**実装開始**: 2025-01-27
